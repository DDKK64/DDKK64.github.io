---
layout: posts
title: "PRML - Probability distributions - Part II"
---
- [Guassian distribution](#guassian-distribution)
  - [Marginal Gaussian distribution](#marginal-gaussian-distribution)
  - [Conditional Gaussian distribution](#conditional-gaussian-distribution)
  - [Bayes' theorem for Gaussian distribution](#bayes-theorem-for-gaussian-distribution)
  - [Maximum likelihood for Gaussian](#maximum-likelihood-for-gaussian)
  - [Sequential estimation](#sequential-estimation)
  - [Bayesian inference for Gaussian distribution](#bayesian-inference-for-gaussian-distribution)
  - [Student's t-distribution](#students-t-distribution)
  - [Periodic variables](#periodic-variables)
  - [Mixture of Gaussians](#mixture-of-gaussians)

## Guassian distribution

A.k.a. normal distribution.

Univariate Gaussian distribution takes the form

$$
\mathcal{N} (\mu ,\sigma ^{2}) = \frac{1}{(2 \pi \sigma ^{2})^{1/2}} \exp \left \{ - \frac{1}{2 \sigma ^{2}} (x-\mu)^2 \right \}
$$

where $\mu$ is the mean and $\sigma ^{2}$ is the variance.

D-dimensional multivariate Guassian distribution takes the form

$$
\mathcal{N} (\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma})
= \frac{1}{(2\pi)^{D/2}} \frac{1}{| \boldsymbol{\Sigma} | ^{1/2}} \exp \left \{ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right \}
$$

where 

- $\mathbf{x}$ is a $D$-dimensional vector
- $\boldsymbol{\mu}$ is the mean vector
- $\boldsymbol{\Sigma}$ is a $D \times D$ covariance matrix
- $\| \boldsymbol{\Sigma} \|$ is the determinant of $\boldsymbol{\Sigma}$

Recall that, the Gaussian distribution maximizes the entropy under the finite mean and variance.

Central limit theorem. As an example, the binomial distribution will tend to a Guassian as $N \rightarrow \infty$.

The quadratic form in the exponent

$$
\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})
$$

is called the **Mahalanobis distance** from $\mathbf{x}$ to $\boldsymbol{\mu}$.

$\Delta$ degrades to  Euclidean distance when $\boldsymbol{\Sigma}$ is the identity matrix.

Without loss of generality, $\boldsymbol{\Sigma}$ can always be taken to be symmetric.
If not, let $\Lambda$ denote $\boldsymbol{\Sigma}^{-1}$, 
then by substituting $\Lambda$ with 

$$
\Lambda = \frac{\Lambda + \Lambda^{\mathsf {T}}}{2} + \frac{\Lambda - \Lambda^{\mathsf {T}}}{2}
$$

we have

$$
\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \frac{\Lambda + \Lambda^{\mathsf {T}}}{2} (\mathbf{x} - \boldsymbol{\mu}) + 
\frac{1}{2} \left[ (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda (\mathbf{x} - \boldsymbol{\mu}) - (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda^{\mathsf {T}} (\mathbf{x} - \boldsymbol{\mu}) \right]
$$

Since

$$
(\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda (\mathbf{x} - \boldsymbol{\mu})
= \left[ (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda (\mathbf{x} - \boldsymbol{\mu}) \right]^{\mathsf {T}}
= (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda^{\mathsf {T}} (\mathbf{x} - \boldsymbol{\mu})
$$

we obtain

$$
\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \frac{\Lambda + \Lambda^{\mathsf {T}}}{2} (\mathbf{x} - \boldsymbol{\mu})
$$

in which $\frac{\Lambda + \Lambda^{\mathsf {T}}}{2}$ is symmetric.

> Basically, what we've done here is to transform terms like $\displaystyle a\_{ij} x\_i x\_j + a\_{ji} x\_j x\_i$ into
> $b\_{ij} x\_i x\_j + b\_{ji} x\_j x\_i$ where $b\_{ij} = b\_{ji} = (a\_{ij} + a\_{ji}) /2$.

If $\boldsymbol{\Sigma}$ (or $\boldsymbol{\Sigma}^{-1}$ vice versa) is symmetric, $\boldsymbol{\Sigma}^{-1}$ is also symmetric since

$$
(\boldsymbol{\Sigma}^{-1})^{\mathsf {T}} \boldsymbol{\Sigma}
= (\boldsymbol{\Sigma}^{-1})^{\mathsf {T}} \boldsymbol{\Sigma}^{\mathsf {T}}
= I
\Rightarrow (\boldsymbol{\Sigma}^{-1})^{\mathsf {T}} = \boldsymbol{\Sigma}^{-1}
$$

As from now, suppose $\boldsymbol{\Sigma}$ is a real, symmetric matrix. There exists an orthogonal matrix 

$$
U = (\mathbf{u_1}, \dots, \mathbf{u_D})^{\mathsf {T}}
$$

such that

$$
U \boldsymbol{\Sigma} U^{\mathsf {T}} = \operatorname{diag} (\lambda_1, \dots, \lambda_D)
$$

where $\mathbf{u\_i}$ is the eigenvector of $\boldsymbol{\Sigma}$ corresponding to eigenvalue $\lambda\_i$.

Then we obtain

$$
(U \boldsymbol{\Sigma} U^{\mathsf {T}})^{-1}
= U \boldsymbol{\Sigma}^{-1} U^{\mathsf {T}}
= \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D})
$$

The inverse of $\boldsymbol{\Sigma}$ can be expressed as

$$
\boldsymbol{\Sigma}^{-1} = U^{\mathsf {T}} \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D}) U
$$

Then we have

$$
\begin{align*}
\Delta^2 
&= (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}}
U^{\mathsf {T}} \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D}) U
(\mathbf{x} - \boldsymbol{\mu}) \\
&= \mathbf{y}^{\mathsf {T}} \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D}) \mathbf{y} \\
&= \sum_{i=1}^D \frac{y_i^2}{\lambda_i}
\end{align*}
$$

in which $\mathbf{y} = U (\mathbf{x} - \boldsymbol{\mu})$.

This can be interpreted as a sequence of shifting (by $\boldsymbol{\mu}$), 
rotating and reflecting (by $U$) operations that transform the coorinate system of $\mathbf{x}$ to $\mathbf{y}$.

Setting $\Delta^2$ to constant, we obtain a hyperellipsoid spanned by $\mathbf{x}$, on which the probability density will be constant.
The length of axes is determined by the eigenvalues of the covariance matrix.

For a Guassian distribution to be well defined, the covariance matrix must be postive definite. I.e. all eigenvalues of the covariance matrix should be positve.

We can see that a non-postive eigenvalue causes some part of Gaussian integral to diverge,
so the distribution cannot be normalized properly.
Particularly, a zero eigenvalue makes $\boldsymbol{\Sigma}$ become singular, which indicates 
some components of $\mathbf{x}$ are linearly dependent, and $\mathbf{x}$ does not span $\mathbb{R}^D$.

In fact, any covariance matrix is positive semi-definite. 
Let $\mathbf{x}=(x\_1, \dots, x\_n)^{\mathsf {T}}$ be a multivariate random variable. 
For any $\mathbf{a} \in \mathbb{R}^n$ that $\mathbf{a} \ne \mathbf{0}$, we have

$$
\begin{align*}
\mathbf{a}^{\mathsf {T}} \operatorname{cov}(\mathbf{x}, \mathbf{x}) \mathbf{a}
&= \mathbf{a}^{\mathsf {T}} \operatorname{E} \left[ (\mathbf{x}-\operatorname{E}[\mathbf{x}])(\mathbf{x} - \operatorname{E}[\mathbf{x}])^{\mathsf {T}} \right] \mathbf{a} \\
&= \operatorname{E} \left[ \mathbf{a}^{\mathsf {T}} (\mathbf{x}-\operatorname{E}[\mathbf{x}])(\mathbf{x} - \operatorname{E}[\mathbf{x}])^{\mathsf {T}} \mathbf{a} \right] \\
&= \operatorname{E} \left[ \langle \mathbf{a}, \mathbf{x}-\operatorname{E}[\mathbf{x}] \rangle ^2 \right] \ge 0
\end{align*}
$$

\
Using

$$
| \boldsymbol{\Sigma} | = \prod_{i=1}^D \lambda_i
$$

the guassian distribution in terms of $\mathbf{y}$ can also be written as

$$
p(\mathbf{y}) = \prod_{i=1}^{D} \frac{1}{(2 \pi \lambda_i)^{1/2}} \exp \left\{ -\frac{y_i^2}{2 \lambda_i}  \right\}
$$

which is the product of D independent univariate Guassian distributions, and it has the following properties:

$$
\begin{align*}
\int p(\mathbf{y}) \,d\mathbf{y} &= 1 \\
\operatorname{E} [\mathbf{y}] &=  0 \\
\operatorname{cov} (\mathbf{y}, \mathbf{y}) &= \operatorname{diag} (\lambda_1, \dots, \lambda_D)
\end{align*}
$$

The normalization of $\mathbf{y}$ is shown by

$$
\begin{align*}
\int p(\mathbf{y}) \,d\mathbf{y}
&= \int \cdots \int \prod_{i=1}^{D} \frac{1}{(2 \pi \lambda_i)^{1/2}} \exp \left\{ -\frac{y_i^2}{2 \lambda_i}  \right\} \,dy_1 \dots dy_D \\
&= \prod_{i=1}^{D} \int \frac{1}{(2 \pi \lambda_i)^{1/2}} \exp \left\{ -\frac{y_i^2}{2 \lambda_i}  \right\} \,dy_i \\
&= 1
\end{align*}
$$

Since the components of $\mathbf{y}$ are independent, by inspecting the integrands we have

$$
\begin{align*}
\operatorname{E} [y_i] &= 0 \\
\operatorname{cov} (y_i, y_j) &= 
\begin{cases}
0 &\text{ if } i \ne j \\
\operatorname{var}(y_i) = \lambda_i &\text{ if } i=j 
\end{cases} \\
\end{align*}
$$

Now back to $p(\mathbf{x})$.

To show that $\int p(\mathbf{x}) \,d\mathbf{x} = 1$, we substitute $\mathbf{x}$ with

$$
\mathbf{x} = U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu}
$$

Taking derivative with repspect to $\mathbf{y}$, we obtain the Jacobian matrix 

$$
\mathbf{J} = \frac{\partial \mathbf{x}}{\partial \mathbf{y}} = U^{\mathsf {T}}
$$

and thus $\left\| \mathbf{J} \right\| = 1$.

Then the integral on $\mathbf{x}$ gives

$$
\begin{align*}
\int p_{\mathbf{x}}(\mathbf{x}) \,d\mathbf{x} 
&= \int p_{\mathbf{x}}(U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu}) \left| \mathbf{J} \right| \,d\mathbf{y} \\
&= \int p_{\mathbf{y}}(\mathbf{y}) \,d\mathbf{y} \\
&= 1
\end{align*}
$$

where $p\_{\mathbf{x}}$, $p\_{\mathbf{y}}$ denotes the density function about $\mathbf{x}$, $\mathbf{y}$ respectively.
This confirms the multivariate Guassian distribution is normalized.

The expectation of $\mathbf{x}$ is given by

$$
\operatorname{E} [\mathbf{x}] = \boldsymbol{\mu}
$$

which is derived from

$$
\begin{align*}
\operatorname{E} [\mathbf{x}]
&= \operatorname{E} \left[ U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu} \right] \\
&= U^{\mathsf {T}} \operatorname{E} [\mathbf{y}] + \boldsymbol{\mu} \\
&= \boldsymbol{\mu}
\end{align*}
$$

The second order moment is given by

$$
\operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} \right]
= \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} + \boldsymbol{\Sigma} \tag{2.62}
$$

which is derived from

$$
\begin{align*}
\operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} \right]
&= \operatorname{E} \left[ (U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu}) (U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu})^{\mathsf {T}} \right] \\
&= U^{\mathsf {T}} \operatorname{E} \left[ \mathbf{y} \mathbf{y}^{\mathsf {T}} \right] U
+ \boldsymbol{\mu} \operatorname{E}[\mathbf{y}^{\mathsf {T}}] U
+ U^{\mathsf {T}} \operatorname{E}[\mathbf{y}] \boldsymbol{\mu}^{\mathsf {T}}
+ \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= U^{\mathsf {T}} \operatorname{E} \left[ (\mathbf{y}-\mathbf{0}) (\mathbf{y}^{\mathsf {T}}-\mathbf{0}) \right] U
+ \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= U^{\mathsf {T}} \operatorname{diag} (\lambda_1, \dots, \lambda_D) U
+ \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= \boldsymbol{\Sigma} + \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}}
\end{align*}
$$

And the covariance matrix of $\mathbf{x}$ is shown by

$$
\begin{align*}
\operatorname{cov} (\mathbf{x}, \mathbf{x})
&= \operatorname{E} \left[ (\mathbf{x}-\operatorname{E}[\mathbf{x}])(\mathbf{x} - \operatorname{E}[\mathbf{x}])^{\mathsf {T}} \right] \\
&= \operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} \right] - \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= \boldsymbol{\Sigma}
\end{align*}
$$

However, there are some limitations of the Gaussian distribution.

The number of independent parameters grows quadraticly, which incurs significant computation cost. There are $D(D+1)/2$ parameters in $\boldsymbol{\Sigma}$ and $D$ parameters in $\boldsymbol{\mu}$.

By restricting $\boldsymbol{\Sigma}$ to be diagonal, the total of parameters is reduced to $2D$. 
By further restricting $\boldsymbol{\Sigma}$ to be isotropic, i.e. $\boldsymbol{\Sigma} = \sigma^2 I$, the number is reduced to $D+1$.

However, the capability of simplified distributions is also restricted.

Gaussian distribution is unimodal.

### Marginal Gaussian distribution

Suppose $\mathbf{x}$ is a $D$-dimensional vector with Gaussian distribution $\mathcal{N} (\mathbf{x} \| \boldsymbol{\mu}, \boldsymbol{\Sigma})$, 
and $\mathbf{x}\_a$, $\mathbf{x}\_b$ are two disjoint subsets obtained by partitioning $\mathbf{x}$.
Without loss of generality, we can write $\mathbf{x}$ as

$$
\mathbf{x} =
\begin{pmatrix}
\mathbf{x}_a \\
\mathbf{x}_b \\
\end{pmatrix}
$$

Correspondingly, the mean $\boldsymbol{\mu}$ takes the form

$$
\boldsymbol{\mu} =
\begin{pmatrix}
\boldsymbol{\mu}_a \\
\boldsymbol{\mu}_b \\
\end{pmatrix}
$$

and the covariance matrix $\boldsymbol{\Sigma}$ takes the form

$$
\boldsymbol{\Sigma} =
\begin{pmatrix}
\boldsymbol{\Sigma}_{aa} & \boldsymbol{\Sigma}_{ab} \\
\boldsymbol{\Sigma}_{ba} & \boldsymbol{\Sigma}_{bb} \\
\end{pmatrix}
$$

Define the **precision matrix** $\Lambda$ as $\Lambda \equiv \boldsymbol{\Sigma}^{-1}$.
Then the partitioned form of the precision matrix is given by

$$
\boldsymbol{\Lambda} =
\begin{pmatrix}
\boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab} \\
\boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb} \\
\end{pmatrix}
$$

The relationship between the blocks of $\Lambda$ and that of $\boldsymbol{\Sigma}$ can be derived using

$$
{\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}
={\begin{bmatrix}\mathbf{M}&-\mathbf{M}\mathbf {BD} ^{-1}\\-\mathbf {D} ^{-1}\mathbf {C} \mathbf{M}&\quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} \mathbf{M}\mathbf {BD} ^{-1}\end{bmatrix}} \tag{2.76}
$$

where $\mathbf{M} = \left(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} \right)^{-1}$. Moreover, this identity can be obtained by conducting row operations on the partitioned matrix. 

By definition, the mardinal distribution of $\mathbf{x}\_a$ is given by

$$
p(\mathbf{x}_a) = \int p(\mathbf{x}_a, \mathbf{x}_b) \,d\mathbf{x}_b
$$

Expanding the exponent of $p(\mathbf{x})$ using partitioned matrix, we have

$$
\begin{align*}
&-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) = \\
& -\frac{1}{2} (\mathbf{x}_a - \boldsymbol{\mu}_a)^{\mathsf {T}} \boldsymbol{\Lambda}_{aa}(\mathbf{x}_a - \boldsymbol{\mu}_a)
-\frac{1}{2} (\mathbf{x}_a - \boldsymbol{\mu}_a)^{\mathsf {T}} \boldsymbol{\Lambda}_{ab}(\mathbf{x}_b - \boldsymbol{\mu}_b) \\
& -\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\mu}_b)^{\mathsf {T}} \boldsymbol{\Lambda}_{ba}(\mathbf{x}_a - \boldsymbol{\mu}_a)
-\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\mu}_b)^{\mathsf {T}} \boldsymbol{\Lambda}_{bb}(\mathbf{x}_b - \boldsymbol{\mu}_b)
\end{align*}
$$

Grouping terms involving $\mathbf{x}\_b$ and completing the squre with respect to $\mathbf{x}\_b$, we have

$$
-\frac{1}{2} \mathbf{x}_b^{\mathsf {T}} \boldsymbol{\Lambda}_{bb} \mathbf{x}_b
+ \mathbf{x}_b^{\mathsf {T}} \mathbf{m}
= -\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m})^{\mathsf {T}} \boldsymbol{\Lambda}_{bb} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m})
+ \frac{1}{2} \mathbf{m}^{\mathsf {T}} \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m}
$$

in which $\mathbf{m} = \boldsymbol{\Lambda}\_{bb} \boldsymbol{\mu}\_b -\boldsymbol{\Lambda}\_{ba} (\mathbf{x}\_a - \boldsymbol{\mu}\_a)$ is independent of $\mathbf{x}\_b$, but dependent of  $\mathbf{x}\_a$.

The integral over $\mathbf{x}\_b$ given by

$$
\int \exp \left\{ -\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m})^{\mathsf {T}} \boldsymbol{\Lambda}_{bb} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m}) \right\} \,d\mathbf{x}_b
$$

is a Gaussian integral, and the result can be efficiently derived from the normalization constant of Gaussian distribution. 
Therefore, we see that the result of the integaral is dependent only on covariance matrix $\boldsymbol{\Lambda}\_{bb}$, 
and is independent of $\mathbf{x}\_a$.

Grouping the remaining terms dependent on $\mathbf{x}\_a$, we have

$$
\frac{1}{2} \mathbf{m}^{\mathsf {T}} \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m}
- \frac{1}{2} \mathbf{x}_a^{\mathsf {T}} \boldsymbol{\Lambda}_{aa} \mathbf{x}_a
+ \mathbf{x}_a^{\mathsf {T}} ( \boldsymbol{\Lambda}_{aa} \boldsymbol{\mu}_a +  \boldsymbol{\Lambda}_{ab} \boldsymbol{\mu}_b) \\
= - \frac{1}{2} \mathbf{x}_a^{\mathsf {T}} (\boldsymbol{\Lambda}_{aa} - \boldsymbol{\Lambda}_{ab} \boldsymbol{\Lambda}_{bb}^{-1} \boldsymbol{\Lambda}_{ba}) \mathbf{x}_a
+ \mathbf{x}_a^{\mathsf {T}} (\boldsymbol{\Lambda}_{aa} - \boldsymbol{\Lambda}_{ab} \boldsymbol{\Lambda}_{bb}^{-1} \boldsymbol{\Lambda}_{ba}) \boldsymbol{\mu}_a + \textrm{Constant}
$$

where the $\textrm{Constant}$ includes other terms independent of $\mathbf{x}\_a$.

Since the exponent is a quadratic form in $\mathbf{x}\_a$, the marginal distribution $p(\mathbf{x}\_a)$ will also be a Gaussian.

Instead of integration over $\mathbf{x}\_a$ to obtain the complete form of $p(\mathbf{x}\_a)$, we inspect the exponent to get 

$$
\operatorname{cov} [\mathbf{x}_a]
= \boldsymbol{\Sigma}_a = (\boldsymbol{\Lambda}_{aa} - \boldsymbol{\Lambda}_{ab} \boldsymbol{\Lambda}_{bb}^{-1} \boldsymbol{\Lambda}_{ba})^{-1}
$$

Using the relationship between $\boldsymbol{\Lambda}$ and $\boldsymbol{\Sigma}$, we can write

$$
\operatorname{cov} [\mathbf{x}_a] = \boldsymbol{\Sigma}_{aa}
$$

The mean is given by 

$$
\operatorname{E} [\mathbf{x}_a] = \boldsymbol{\mu}_a
$$

Therefore, the marginal distribution of $\mathbf{x}\_a$ is given by

$$
p(\mathbf{x}_a) = \mathcal{N} (\mathbf{x}_a | \boldsymbol{\mu}_a, \boldsymbol{\Sigma}_{aa})
$$

### Conditional Gaussian distribution

By definition, the probability distribution of $\mathbf{x}\_a$ conditioned on $\mathbf{x}\_b$ is given by

$$
p(\mathbf{x}_a | \mathbf{x}_b)
= \frac{p(\mathbf{x}_a, \mathbf{x}_b)}{p(\mathbf{x}_b)}
= \frac{p(\mathbf{x})}{p(\mathbf{x}_b)}
$$

One way to evaluate the $p(\mathbf{x}\_a \| \mathbf{x}\_b)$ is to obtain the marginal distribution $p(\mathbf{x}\_b)$ first 
and divede $p(\mathbf{x})$ by it. However it is more efficient to obtain the result by normalizing the function about $\mathbf{x}\_a$.

By fixing $\mathbf{x}\_b$, $p(\mathbf{x}\_b)$ will be constant and becomes a part of the normalization constant.
The exponent of $p(\mathbf{x}\_a, \mathbf{x}\_b)$ will be a quadratic form of $\mathbf{x}\_a$, which indicates the conditional distribution is also a Gaussian.

Grouping terms involving $\mathbf{x}\_a$, we have

$$
-\frac{1}{2} \mathbf{x}_a^{\mathsf {T}} \boldsymbol{\Lambda}_{aa} \mathbf{x}_a
+ \mathbf{x}_a^{\mathsf {T}} \left[ \boldsymbol{\Lambda}_{aa} \boldsymbol{\mu}_a -\boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \right]
$$

By inspection, the covariance of $p(\mathbf{x}\_a, \mathbf{x}\_b)$ is given by

$$
\boldsymbol{\Sigma}_{a | b} = \boldsymbol{\Lambda}_{aa}^{-1}
$$

The mean is given by

$$
\begin{align*}
\boldsymbol{\mu}_{a | b} 
&= \boldsymbol{\Sigma}_{a | b} \left[ \boldsymbol{\Lambda}_{aa} \boldsymbol{\mu}_a -\boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \right] \\
&= \boldsymbol{\mu}_a - \boldsymbol{\Lambda}_{aa}^{-1} \boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \\
\end{align*}
$$

Substituting precision matrices with covariance matrices, we have

$$
\begin{align*}
\boldsymbol{\mu}_{a | b} &= \boldsymbol{\mu}_a + \boldsymbol{\Sigma}_{ab} \boldsymbol{\Sigma}_{bb}^{-1} (\mathbf{x}_b - \boldsymbol{\mu}_b) \\
\boldsymbol{\Sigma}_{a | b} &= \boldsymbol{\Sigma}_{aa} - \boldsymbol{\Sigma}_{ab} \boldsymbol{\Sigma}_{bb}^{-1} \boldsymbol{\Sigma}_{ba} \\
\end{align*}
$$

Thus the distribution of $\mathbf{x}\_a$ conditioned on $\mathbf{x}\_b$ is given by

$$
\begin{align*}
p(\mathbf{x}_a | \mathbf{x}_b) &= \mathcal{N} (\mathbf{x}_a | \boldsymbol{\mu}_{a | b}, \boldsymbol{\Lambda}_{aa}^{-1}) \\
\boldsymbol{\mu}_{a | b} &= \boldsymbol{\mu}_a - \boldsymbol{\Lambda}_{aa}^{-1} \boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \\
\end{align*}
$$

### Bayes' theorem for Gaussian distribution

Given $p(\mathbf{x})$ and $p(\mathbf{y} \| \mathbf{x})$, find $p(\mathbf{y})$ and $p(\mathbf{x} \| \mathbf{y})$.

Suppose

$$
\begin{align*}
p(\mathbf{x}) &= \mathcal{N} (\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}) \\
p(\mathbf{y} | \mathbf{x}) &= \mathcal{N} (\mathbf{y} | A \mathbf{x} + \mathbf{b}, \mathbf{L}^{-1}) \\
\end{align*}
$$

where $\boldsymbol{\Lambda}$ and $\mathbf{L}$ are precision matrices, $\mathbf{x}$ and $\mathbf{y}$ have demensionality M and D respectively, and A is a $D \times M$ matrix.

Let

$$
\mathbf{z} =
\begin{pmatrix}
\mathbf{x} \\
\mathbf{y} \\
\end{pmatrix}
$$

The log joint distribution is then given by

$$
\begin{align*}
\ln p(\mathbf{z}) &= \ln p(\mathbf{y} | \mathbf{x}) + \ln p(\mathbf{x}) \\
&= -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Lambda} (\mathbf{x} - \boldsymbol{\mu}) 
-\frac{1}{2} (\mathbf{y} - A \mathbf{x} - \mathbf{b})^{\mathsf {T}} \mathbf{L} (\mathbf{y} - A \mathbf{x} - \mathbf{b}) + \mathrm{constant} \\
\end{align*}
$$

where 'constant' is independent of $\mathbf{x}$ and $\mathbf{y}$.

Grouping second order terms gives

$$
-\frac{1}{2} \mathbf{x}^{\mathsf {T}} (\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A) \mathbf{x}
-\frac{1}{2} \mathbf{y}^{\mathsf {T}} \mathbf{L} \mathbf{y}
+\frac{1}{2} \mathbf{y}^{\mathsf {T}} \mathbf{L} A \mathbf{x}
+\frac{1}{2} \mathbf{x}^{\mathsf {T}} A^{\mathsf {T}} \mathbf{L} \mathbf{y} \\
= -\frac{1}{2}
\begin{pmatrix} \mathbf{x} \\ \mathbf{y} \\ \end{pmatrix} ^{\mathsf {T}}
\begin{pmatrix}
\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A & - A^{\mathsf {T}} \mathbf{L} \\
- \mathbf{L} A & \mathbf{L} \\
\end{pmatrix}
\begin{pmatrix} \mathbf{x} \\ \mathbf{y} \\ \end{pmatrix}
= \mathbf{z}^{\mathsf {T}} \mathbf{R} \mathbf{z}
$$

Thus the precision matrix of $\mathbf{z}$ is obtained

$$
\mathbf{R} =
\begin{pmatrix}
\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A & - A^{\mathsf {T}} \mathbf{L} \\
- \mathbf{L} A & \mathbf{L} \\
\end{pmatrix}
$$

Using (2.76), the corresponding covariance matrix is given by

$$
\operatorname{cov}[\mathbf{z}] = \mathbf{R}^{-1}
= \begin{pmatrix}
\boldsymbol{\Lambda}^{-1} & \boldsymbol{\Lambda}^{-1} A^{\mathsf {T}} \\
A \boldsymbol{\Lambda}^{-1} & \mathbf{L}^{-1} + A \boldsymbol{\Lambda}^{-1} A^{\mathsf {T}}\\
\end{pmatrix}
$$

Then Grouping the linear terms, we have

$$
\mathbf{x}^{\mathsf {T}} \boldsymbol{\Lambda} \boldsymbol{\mu}
- \mathbf{x}^{\mathsf {T}} A^{\mathsf {T}} \mathbf{L} \mathbf{b}
+ \mathbf{y}^{\mathsf {T}} \mathbf{L} \mathbf{b}
= \begin{pmatrix} \mathbf{x} \\ \mathbf{y} \\ \end{pmatrix} ^{\mathsf {T}}
\begin{pmatrix} \boldsymbol{\Lambda} \boldsymbol{\mu} - A^{\mathsf {T}} \mathbf{L} \mathbf{b} \\ \mathbf{L} \mathbf{b} \\ \end{pmatrix}
$$

The mean of $\mathbf{z}$ is given by

$$
\operatorname{E}[\mathbf{z}] = \mathbf{R}^{-1} 
\begin{pmatrix} \boldsymbol{\Lambda} \boldsymbol{\mu} - A^{\mathsf {T}} \mathbf{L} \mathbf{b} \\ \mathbf{L} \mathbf{b} \\ \end{pmatrix}
= \begin{pmatrix} \boldsymbol{\mu} \\ A \boldsymbol{\mu} + \mathbf{b} \\ \end{pmatrix}
$$

From the joint distribution, we obtain the marginal distribution $p(\mathbf{y})$

$$
\begin{align*}
\operatorname{E}[\mathbf{y}] &= A \boldsymbol{\mu} + \mathbf{b} \\
\operatorname{cov}[\mathbf{y}] &= \mathbf{L}^{-1} + A \boldsymbol{\Lambda}^{-1} A^{\mathsf {T}} \\
\end{align*}
$$

and the conditional distribution $p(\mathbf{x} \| \mathbf{y})$

$$
\begin{align*}
\operatorname{E}[\mathbf{x} | \mathbf{y}] 
&= (\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A) ^{-1} 
\left[ A^{\mathsf {T}} \mathbf{L} (\mathbf{y} - \mathbf{b}) + \boldsymbol{\Lambda} \boldsymbol{\mu} \right] \\
\operatorname{cov}[\mathbf{x} | \mathbf{y}] &= (\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A) ^{-1} \\
\end{align*}
$$

### Maximum likelihood for Gaussian

Given a data set $\mathbf{X} = (\mathbf{x}\_1, \dots, \mathbf{x}\_N)^{\mathsf{T}}$ with each observation drawn independently from a Gaussian distribution.
Then the log likelihood function is given by

$$
\ln p(\mathbf{X} | \boldsymbol{\mu}, \boldsymbol{\Sigma})
= - \frac{ND}{2} \ln (2 \pi) - \frac{N}{2} \ln |\boldsymbol{\Sigma}|
- \frac{1}{2} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x}_n - \boldsymbol{\mu})
$$

Setting derivative with respect to $\boldsymbol{\mu}$ to zero

$$
\frac{\partial }{\partial \boldsymbol{\mu}} \ln p(\mathbf{X} | \boldsymbol{\mu}, \boldsymbol{\Sigma})
= \sum_{n=1}^N \boldsymbol{\Sigma}^{-1} (\mathbf{x}_n - \boldsymbol{\mu}) = 0
$$

we obtain

$$
\boldsymbol{\mu}_{\textrm{ML}} = \frac{1}{N} \sum_{n=1}^N \mathbf{x}_n
$$

Setting derivative with respect to $\boldsymbol{\Sigma}$ to zero, we have

$$
- \frac{N}{2} \boldsymbol{\Sigma}^{-\mathsf{T}}
+ \frac{1}{2} \sum_n \boldsymbol{\Sigma}^{-\mathsf{T}} (\mathbf{x}_n - \boldsymbol{\mu})(\mathbf{x}_n - \boldsymbol{\mu})^{\mathsf{T}} \boldsymbol{\Sigma}^{-\mathsf{T}}
= \mathbf{0}
$$

Solving for $\boldsymbol{\Sigma}$, we obtain

$$
\boldsymbol{\Sigma}_{\textrm{ML}} = \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}}) (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}})^{\mathsf {T}}
$$

Thus we see the sufficient statistics for Gassuain distribution are given by $\sum\_n \mathbf{x}\_n$ and $\sum\_n \mathbf{x}\_n \mathbf{x}\_n^{\mathsf {T}}$.

Evaluating the expectation of estimated mean gives

$$
\operatorname{E} [\boldsymbol{\mu}_{\textrm{ML}}] = \frac{1}{N} \sum_{n=1}^N \operatorname{E} [\mathbf{x}_n] = \boldsymbol{\mu}
$$

And the expectation of the covariance matrix is evaluated by

$$
\begin{align*}
\operatorname{E} [\boldsymbol{\Sigma}_{\textrm{ML}}]
&= \frac{1}{N} \sum_{n=1}^N \operatorname{E} \left[ (\mathbf{x}_n - \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i) (\mathbf{x}_n - \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i)^{\mathsf {T}} \right] \\
&= \frac{1}{N} \sum_{n=1}^N \operatorname{E} \left[ \mathbf{x}_n \mathbf{x}_n^{\mathsf {T}} - \frac{2}{N} \mathbf{x}_n \sum_{i=1}^N \mathbf{x}_i^{\mathsf {T}}
+ \frac{1}{N^2} \sum_{i=1}^N \mathbf{x}_i \sum_{i=1}^N \mathbf{x}_i^{\mathsf {T}} \right]
\end{align*}
$$

Using (2.62), for $i = j$ we have 

$$
\operatorname{E} [\mathbf{x}_i \mathbf{x}_j^{\mathsf {T}} ] 
= \operatorname{E} [\mathbf{x}_i \mathbf{x}_i^{\mathsf {T}} ] 
= \boldsymbol{\Sigma} + \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}}
$$

For $i \ne j$, since $\mathbf{x}\_i$ and $\mathbf{x}\_j$ is independent, we have

$$
\operatorname{E} [\mathbf{x}_i \mathbf{x}_j^{\mathsf {T}} ]
= \operatorname{E} [\mathbf{x}_i] \operatorname{E} [ \mathbf{x}_j^{\mathsf {T}} ]
= \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}}
$$

By substitution, we have

$$
\operatorname{E} [\boldsymbol{\Sigma}_{\textrm{ML}}] = \frac{N-1}{N} \boldsymbol{\Sigma}
$$

which means $\boldsymbol{\Sigma}\_{\textrm{ML}}$ is a biased estimator for the covariance.

We can fix the bias by using the new estimator

$$
\tilde{\boldsymbol{\Sigma}} = \frac{1}{N-1} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}}) (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}})^{\mathsf {T}}
$$

### Sequential estimation

TODO: Robbins-Monro algorithm

### Bayesian inference for Gaussian distribution

Consider a univariate gaussian distribution with known $\sigma^2$ and unknown $\mu$. 

Given a set of $N$ observations $\mathbf{X} = \{ x\_1, \dots, x\_N \}$, the likelihood $p(\mathbf{X} \| \mu)$ is given by

$$
p(\mathbf{X} | \mu) = \prod_{n=1}^N p(x_n | \mu)
= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left\{ - \frac{1}{2 \sigma^2} \sum_{n=1}^N (x_n - \mu)^2 \right\}
$$

Note that, though written in the form $p(\cdot)$, the likelihood function is not a valid probability since it is not normalized.


For $\mu$ , we choose the conjugate prior

$$
p(\mu) = \mathcal{N} (\mu | \mu_0, \sigma_0^2)
$$

The posterior distribution is given

$$
p(\mu|\mathbf{X}) \propto p(\mu) p(\mathbf{X}|\mu)
$$

Inspecting the exponent of the product

$$
- \frac{1}{2} \mu^2 \left( \frac{1}{\sigma_0^2} + \frac{N}{\sigma^2} \right)
+ \mu \left( \frac{1}{\sigma_0^2} \mu_0 + \frac{1}{\sigma^2} \sum_{n=1}^N x_n \right)
+ \textrm{constant}
$$

we find

$$
p(\mu|\mathbf{X}) =  \mathcal{N} (\mu | \mu_N, \sigma_N^2)
$$

where

$$
\begin{align*}
\frac{1}{\sigma_N^2} &= \frac{1}{\sigma_0^2} + \frac{N}{\sigma^2} \\
\mu_N &= \frac{\sigma^2}{N \sigma_0^2 + \sigma^2} + \frac{N \sigma_0^2}{N \sigma_0^2 + \sigma^2} \mu_{\mathrm{ML}} \\
\mu_{\textrm{ML}} &= \frac{1}{N} \sum_{n=1}^N x_n \\
\end{align*}
$$

Note that

- When $N=0$ the posterior is identical to the prior.
- $N \rightarrow \infty$, we have $\mu\_N \rightarrow \mu\_{\mathrm{ML}}$ and $\sigma\_N^2 \rightarrow 0$
- As the number of observations grows, the precision of the posterior increases
- In the limit $\sigma\_0^2 \rightarrow \infty$, we have $\mu\_N \rightarrow \mu\_{\mathrm{ML}}$, and the precision is given by $\displaystyle \frac{1}{\sigma\_N^2} = \frac{N}{\sigma^2}$

Another approach to evaluate the posterior is by using the results in section 2.3.3, with following change of variables

$$
\begin{align*}
A &\rightarrow \mathbf{1} , &\mathbf{x} &\rightarrow \mu, & b &\rightarrow \mathbf{0} \\
\mathbf{L}^{-1} &\rightarrow \sigma^{2}I, & \boldsymbol{\mu} &\rightarrow \mu_0, & \boldsymbol{\Lambda}^{-1} &\rightarrow \sigma_0^2 I \\
\end{align*}
$$

As a general rule, we should notice that Bayesian method provides a natural way of sequential learning. It can be shown by

$$
p(\mu|\mathbf{X}) \propto \left[ p(\mu) \prod_{n=1}^{N-1} p(x_n|\mu) \right]  p(x_N|\mu)
$$

in which the posterior distribution is also a prior distribution for subsequent observation.

Now suppose the mean is known and the variance is unkown. For convenience, define $\lambda \equiv 1/{\sigma^2}$.

Then the conjugate prior can be constructed proportional to

$$
\lambda^{a-1} \exp (-b \lambda)
$$

Normalization constant is derived from

$$
\Gamma (a) = \int _{0}^{\infty }t^{a-1}e^{-t}\,dt
$$

By substitute $t$ with $t = b \lambda$

$$
\Gamma (a) = \int _{0}^{\infty } (b \lambda)^{a-1}e^{- b \lambda} b \,d\lambda
= b^a\int _{0}^{\infty } \lambda^{a-1}e^{- b \lambda} \,d\lambda
$$

Then we have the gamma distribution defined by

$$
\mathrm{Gam}(\lambda|a,b) = \frac{1}{\Gamma(a)} b^a \lambda^{a-1} \exp (- b \lambda)
$$

where $a, b>0$. And with $a \ge 1$, the density has finite value range.

Using the similar approach, the following properties are obtained

$$
\begin{align*}
\operatorname{E} [\lambda] &= \frac{a}{b} \\
\operatorname{var} [\lambda] &= \frac{a}{b^2} \\
\operatorname{mode} [\lambda] &= \frac{a-1}{b} \quad \text{ for } a \ge 1 \\
\end{align*}
$$

Now define the prior of $\lambda$ as

$$
p(\lambda) = \mathrm{Gam}(\lambda|a_0,b_0)
$$

Then the posterior is given by

$$
p(\lambda|\mathbf{X}) \propto \lambda^{a_0 - 1} \lambda^{N/2} \exp \left\{ -b_0 \lambda - \frac{\lambda}{2} \sum_{n=1}^N (x_n - \mu)^2 \right\}
$$

which takes the form of a gamma distribution. By inspection, we obtain

$$
p(\lambda|\mathbf{X}) = \mathrm{Gam}(\lambda|a_N,b_N)
$$

in which we defined

$$
\begin{align*}
a_N &= a_0 + \frac{N}{2} \\
b_N &= b_0 + \frac{N}{2} \sigma_{\textrm{ML}}^2 \\
\end{align*}
$$

Here, $2a\_0$ can be interpreted as the number of fictitious observations.
By writing $b\_0$ as $\displaystyle \frac{2 a\_0}{2} \frac{b\_0}{a\_0}$,
we see $\displaystyle \frac{b\_0}{a\_0}$ can be interpreted as the variance of fictitious data.

Instead of working with the precision, the conjugate prior about the variance is called the inversed gamma distribution.

Now suppose the mean and the variance are both unkown.

The likelihood function can be written as

$$
\begin{align*}
p(\mathbf{X} | \mu, \lambda)
&= \left( \frac{\lambda}{2\pi} \right)^{N/2} \exp \left\{ - \frac{\lambda}{2} \sum_{n=1}^N (x_n - \mu)^2 \right\} \\
&\propto \lambda^{N/2} \exp \left\{ - \frac{N \lambda}{2} \left( \mu^2 - 2 \mu \frac{\sum_n x_n}{N} + \frac{\sum_n x_n^2}{N} \right) \right\}
\end{align*}
$$

Defining $\displaystyle c=\frac{\sum\_n x\_n}{N}, d=\frac{\sum\_n x\_n^2}{N}$ and rearranging terms, we have

$$
\begin{align*}
p(\mathbf{X} | \mu, \lambda)
&\propto \lambda^{N/2} \exp \left\{ - \frac{N \lambda}{2} \left[ (\mu - c)^2 + d - c^2 \right] \right\} \\
&= \lambda^{1/2} \exp \left\{ - \frac{N \lambda}{2} \left( \mu - c \right)^2 \right\}
\lambda^{(N-1)/2} \exp \left\{ - \frac{N}{2} (d - c^2) \lambda \right\}
\end{align*}
$$

which is a product of a gamma distribution over $\lambda$ and a Gaussian distribution conditioned on $\lambda$. 

Therefore, the prior distribution $p(\mu, \lambda)$ can take the form

$$
p(\mu, \lambda) = \mathcal{N} (\mu | \mu_0, (\beta \lambda)^{-1}) \mathrm{Gam}(\lambda|a,b)
$$

which is called the normal-gamma distribution or gaussian-gamma distribution.

\
Now consider a D-deimensional multivariate Gaussian $\mathcal{N} (\mathbf{x} \| \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})$ 
and a set of observations $\mathbf{X} = \{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$. The likelihood function is given by

$$
p(\mathbf{X}|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})
= \prod_{n=1}^N p(\mathbf{x}_n|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})
= \frac{1}{(2 \pi)^{DN/2}} |\boldsymbol{\Lambda}|^{N/2} \exp \left\{ - \frac{1}{2} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Lambda} (\mathbf{x}_n - \boldsymbol{\mu}) \right\}
$$

Suppose the precision $\boldsymbol{\Lambda}$ is known and the mean $\boldsymbol{\mu}$ is known.

Analogous to the univariate case, we choose the prior of $\boldsymbol{\mu}$ as

$$
p(\boldsymbol{\mu}) = \mathcal{N} (\boldsymbol{\mu} | \boldsymbol{\mu}_0, \boldsymbol{\Lambda}_0^{-1})
$$

The posterior is given by

$$
p(\boldsymbol{\mu} | \mathbf{X}) 
\propto p(\boldsymbol{\mu}) p(\mathbf{X}|\boldsymbol{\mu})
$$

Inspecting the exponent of the product

$$
- \frac{1}{2} \boldsymbol{\mu}^{\mathsf {T}} (\boldsymbol{\Lambda}_0 + N \boldsymbol{\Lambda}) \boldsymbol{\mu}
+ \boldsymbol{\mu}^{\mathsf {T}} \left( \boldsymbol{\Lambda} \boldsymbol{\mu}_0 + \boldsymbol{\Lambda} \sum_n \mathbf{x}_n \right)
+ \textrm{Constant}
$$

where $\text{Constant}$ are terms irrelevent with $\boldsymbol{\mu}$, then we see the posterior is a Gaussian distribution defined by

$$
p(\boldsymbol{\mu} | \mathbf{X}) = \mathcal{N} (\boldsymbol{\mu} | \boldsymbol{\mu}_N, \boldsymbol{\Lambda}_N^{-1})
$$

where

$$
\begin{align*}
\boldsymbol{\Lambda}_N &= \boldsymbol{\Lambda}_0 + N\boldsymbol{\Lambda} \\
\boldsymbol{\mu}_N &= \boldsymbol{\Lambda}_N^{-1} (\boldsymbol{\Lambda}_0 \boldsymbol{\mu}_0 + N \boldsymbol{\Lambda} \boldsymbol{\mu}_{\mathrm{ML}}) \\
&= (\boldsymbol{\Lambda}_0 + N\boldsymbol{\Lambda})^{-1} (\boldsymbol{\Lambda}_0 \boldsymbol{\mu}_0 + N \boldsymbol{\Lambda} \boldsymbol{\mu}_{\mathrm{ML}}) \\
\boldsymbol{\mu}_{\mathrm{ML}} &= \frac{1}{N} \sum_n \mathbf{x}_n \\
\end{align*}
$$


Now if the mean $\boldsymbol{\mu}$ is known and the precision $\boldsymbol{\Lambda}$ is unknown. The conjugate prior for the precision is the **Wishart distribution** given by

$$
\mathcal{W} (\boldsymbol{\Lambda} | \mathbf{W}, \nu) = B |\boldsymbol{\Lambda}| ^{(\mu - D - 1)/2} \exp \left\{ - \frac{1}{2} \operatorname{Tr} (\mathbf{W}^{-1} \boldsymbol{\Lambda}) \right\}
$$

where $\nu$ is degree of freedom, $\mathbf{W}$ is symmetric $D \times D$ matrix.

The normalization constatnt $B$ is given by

$$
\text{TODO}
$$

TODO: proof

### Student's t-distribution

Beginning with the univariate case, we integrate out the precision from the normal-gamma prior distribution

$$
\begin{align*}
p(x | \mu, a, b)
&= \int_0^\infty \mathcal{N} (x|\mu, \tau^{-1}) \mathrm{Gam} (\tau | a, b) \\
&= \int_0^\infty \frac{b^a \tau^{a-1} e^{-b \tau}}{\Gamma(a)} \left( \frac{\tau}{2 \pi} \right)^{1/2} \exp \left\{ - \frac{\tau}{2} (x - \mu)^2 \right\} \,d\tau \\
&= \frac{b^a}{(2\pi)^{1/2} \Gamma(a)} \int_0^\infty \tau^{a - \frac{1}{2}} \exp \left\{ -\left[ \frac{1}{2} (x-\mu)^2 + b \right] \tau \right\} \,d\tau \\
\end{align*}
$$

Subtituting $\tau$ with $t = \left[ \frac{1}{2} (x-\mu)^2 + b \right] \tau$, we have

$$
p(x | \mu, a, b)
= \frac{b^a \Gamma(a+\frac{1}{2})}{(2\pi)^{1/2} \Gamma(a)}  \left[ \frac{1}{2} (x-\mu)^2 + b \right]^{-a-\frac{1}{2}}
$$

Substituting parameters with $\nu = 2a$ and $\lambda = a/b$, we have

$$
\mathrm{St} (x | \mu, \nu, \lambda)
= \frac{\Gamma(\frac{\nu + 1}{2})}{\Gamma(\frac{\nu}{2})} \left( \frac{\lambda}{\pi\nu} \right)^{1/2} \left[ 1 + \frac{\lambda}{\nu} (x-\mu)^2 \right]^{-(\nu+1)/2}
$$

which is known as **(localtion-scale) t-distribution**. $\nu$ is the degree of freedom.

As a special case of $\nu=1$, the distribution reduces to cauchy distribution.

As $\nu \rightarrow \infty$, the t-distribution converges to $\mathcal{N} (\mu, \lambda^{-1})$. This can be shown by taking the limit of terms related to $x$

$$
\begin{align*}
&\lim_{\nu \rightarrow \infty} \left[ 1 + \frac{\lambda}{\nu} (x-\mu)^2 \right]^{-(\nu+1)/2} \\
=& \lim_{t \rightarrow \infty} \left[ \left( 1 + \frac{1}{t} \right)^{t} \right]^{- \lambda (x-\mu)^{2}/2} \cdot \left( 1 + \frac{1}{t}\right)^{-1/2} \\
=& \exp \left\{ - \frac{\lambda}{2} (x-\mu)^2 \right\}
\end{align*}
$$

in which $\nu$ is substitued with $\nu = \lambda (x-\mu)^2 t$.  Since the remaining terms are independent of $x$ and the distribution is always normalized,
the normalization constant can be easily obtained by inspecting the exponent,
instead of taking the limit of the normalization constant of t-stribution.

> However, we can take the limit of $\Gamma(z)$ by using the Stirling's approximation.

t-distribution can be seen as a sum of infinite number of Gaussian distributions that have identical mean and various precisions.

It has heavier tails than the normal distribution and thus exhibits robustness, which means it is less sensitive to the presence of outiliers.

> tail? robustness?

TODO: plot illustration
TODO: EM for t-distribution

A generalization to the multivariate t-distribution can be obtained by integrating

$$
\mathrm{St}(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)
= \int_0^\infty \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta
$$

Using the same technique as the univariate case, we obtain

$$
\mathrm{St}(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)
= \frac{\Gamma(\frac{D+\nu}{2})}{\Gamma(\frac{\nu}{2})} \frac{|\boldsymbol{\Lambda}|^{1/2}}{(\pi \nu)^{D/2}} \left( 1 + \frac{\Delta^2}{\nu} \right)^{-(D+\nu)/2}
$$

in which

$$
\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Lambda} (\mathbf{x} - \boldsymbol{\mu})
$$

and $D$ is the demensionality of $\mathbf{x}$.

It has following properties

$$
\begin{align*}
\operatorname{E}[\mathbf{x}] &= \boldsymbol{\mu} &\textrm{if} \;\; \nu > 1 \\
\operatorname{cov} [\mathbf{x}] &= \frac{\nu}{\nu-2} \boldsymbol{\Lambda}^{-1} &\textrm{if} \;\; \nu > 2 \\
\operatorname{mode}[\mathbf{x}] &= \boldsymbol{\mu} & \\
\end{align*}
$$

These also apply to the univariate case.

The mean can be derived by the integral

$$
\begin{align*}
& \int_{\mathbb{R}_D} \int_0^\infty \mathbf{x} \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta d\mathbf{x} \\
=& \int_0^\infty \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta \int_{\mathbb{R}_D} \mathbf{x} \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \,d\mathbf{x} \\
=& \boldsymbol{\mu}
\end{align*}
$$

Using the same approach, the covariance is shown by

$$
\begin{align*}
& \int_0^\infty \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta
\int_{\mathbb{R}_D} ( \mathbf{x} - \boldsymbol{\mu} )^2 \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \,d\mathbf{x} \\
=&\int_0^\infty (\eta \boldsymbol{\Lambda})^{-1} \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta \\
=& \boldsymbol{\Lambda}^{-1} \frac{(\nu/2)^{\nu/2}}{\Gamma(\nu/2)} \int_0^\infty \eta^{\nu/2 - 2} \exp \left( - \frac{\nu}{2} \eta \right) \,d\eta \\
=& \boldsymbol{\Lambda}^{-1} \frac{(\nu/2)^{\nu/2}}{\Gamma(\nu/2)} \frac{\Gamma(\nu/2 - 1)}{(\nu/2)^{\nu/2-1}} \\
=&  \frac{\nu}{\nu-2} \boldsymbol{\Lambda}^{-1} \\
\end{align*}
$$

Since $\mathrm{Gam}(\eta\|\nu/2, \nu/2)$ is non-negtive and independent of $\mathbf{x}$,
the mode of t-distribution will correspond to that of normal distribution given by $\boldsymbol{\mu}$.

> The valid range of $\nu$ is unobvious in the derivation above. Though, by integrating the t-distribution directly we can observe the valid range for $\nu$. 
> 
> My guess is that for some $\nu$, the improper integral diverges in different orders of integration.

### Periodic variables

Variables that describe cycling data, e.g. day time, direction.

By convention, the distribution for periodic variables satisfies following conditions:

$$
\begin{align*}
p(\theta) &\ge 0 \\
\int_0^{2\pi} p(\theta) \,d\theta &= 1 \\
p(\theta + 2 \pi) &= p(\theta) \\
\end{align*}
$$

Consider a bivariate Gaussian distribution defined over $x\_1, x\_2$, with mean $\mu\_1, \mu\_2$ and covariance $\boldsymbol{\Sigma} = \sigma^2 I$

$$
p(x_1, x_2) = \frac{1}{2 \pi \sigma^2} \exp \left\{ - \frac{1}{2\sigma^2} \left[ (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right] \right\}
$$

By substituting variables with 

$$
\begin{align*}
x_1 &= \cos \theta ,& x_2 &= \sin \theta \\
\mu_1 &= r_0 \cos \theta_0 ,& \mu_2 &= r_0 \sin \theta_0 \\
\end{align*}
$$

we try to construct a distribution over a unit circle in the polar system.

Then the exponent is given by

$$
\begin{align*}
&- \frac{1}{2\sigma^2} \left[ (\cos \theta - r_0 \cos \theta_0)^2 + (\sin \theta - r_0 \sin \theta_0)^2 \right] \\
&= - \frac{1}{2\sigma^2} \left[ 1 + r_0^2 - 2 r_0 \cos \theta \cos \theta_0 - 2 r_0 \sin \theta \sin \theta_0 \right] \\
&= \frac{r_0}{\sigma^2} \cos (\theta - \theta_0) + \textrm{constant} \\
\end{align*}
$$

where 'constant' summarizes the terms that are independent of $\theta$.

Now we write the distribution in the form

$$
p(\theta | \theta_0, m) = \frac{1}{2 \pi I_0(m)} \exp \left\{ m \cos (\theta - \theta_0) \right\}
$$

which is called the **von Mises** distribution or the **circular normal**. 
Here $\theta\_0$ corresponds to the mean, $m$ is called the **concentration parameter** and is analogous to the precision. 

$I\_0(m)$ is the zeroth-order **modified Bessel function** of the first kind defined by

$$
I_0(m) = \frac{1}{2 \pi} \int_0^{2 \pi} \exp \left\{ m \cos \theta \right\} \,d\theta
$$

For large $m$, the distribution approximates the Gaussian. This can be seen by using Taylor expansion

$$
\begin{align*}
\exp \left\{ m \cos (\theta-\theta_0) \right\}
&\approx \exp \left\{ m \left[ 1 - \frac{(\theta-\theta_0)^2}{2} \right]  \right\} \\
&= e^m \exp \left\{ - \frac{m (\theta - \theta_0)^2}{2} \right\}
\end{align*}
$$

In small neighborhood of $\theta\_0$ it approximates well.

Now consider the likelihood estimator for von Mises distribution. The log likelihood function is given by

$$
p(D|\theta_0, m) = -N \ln (2\pi) - N \ln I_0(m) + m\sum_{n=1}^N \cos(\theta_n - \theta_0)
$$

Setting derivative to 0 with repect to $\theta\_0$, we have

$$
\sum_{n=1}^N \sin (\theta_n - \theta_0) = 0
$$

The estimator for $\theta\_0$ is given by

$$
\theta_0^{\mathrm{ML}} = \arctan \left( \frac{\sum_n \sin \theta_n}{\sum_n \cos \theta_n} \right)
$$

Setting derivative to 0 with respect to $m$,  we have

$$
\begin{align*}
A(m_{\mathrm{ML}})
&= \frac{1}{N} \sum_{n=1}^N \cos(\theta_n - \theta_0^{\mathrm{ML}} ) \\
&= \left( \frac{1}{N} \sum_n \cos \theta_n \right) \cos \theta_0^{\mathrm{ML}} + \left( \frac{1}{N} \sum_n \sin \theta_n \right) \sin \theta_0^{\mathrm{ML}}
\end{align*}
$$

in which

$$
\begin{align*}
I_1(m) &= I_0^\prime (m) \\
A(m) &= \frac{I_1(m)}{I_0(m)} \\
\end{align*}
$$

and $m$ can be numerically evaluated from $A(m)$.

One limitation of von Mises distribution is that it is unimodal. This can be addressed by introducing mixtures of von Mises distributions.

### Mixture of Gaussians

A mixture distribution is formed by linear combination of multiple basic distributions.

A superposition of $K$ Gaussian distributions takes the form 

$$
p(\mathbf{x}) = \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$

which is called **mixture of Gaussians**. Each $\mathcal{N} (\mathbf{x}\|\boldsymbol{\mu}\_k, \boldsymbol{\Sigma}\_k)$ is called a component of the mixture.

$\pi\_k$ are mixing coefficients that satisfy $0 \le \pi\_k \le 1$ and $\sum\_{k=1}^K \pi\_k = 1$.
It can be viewed as the prior distribution $p(k)$ for the k-th component.

The posterior $p(k\|\mathbf{x})$ is also known as responsibilities.

The log likelihood function for mixture of Gaussians is given by

$$
\ln p(\mathbf{X}|\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma})
= \sum_{n=1}^N \ln \left\{ \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right\}
$$

The maximum likelihood estimator no longer has an analytical solution.
