# Recurrent Neural Network

Learning sequential data.

## Unfolding computational graph

Many RNNs define the state of a system at time $t$ as

$$
\boldsymbol{h}^{(t)} = f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)}, \boldsymbol{\theta})
$$

It can be represented by circuit diagram on the left

![Unfolding](/assets/images/deeplearning/fig-10.2.png)

where the black box represent a delay of a single time step. By unfolding it we obtain the corresponding computational graph.

## Recurrent nerual networks

There are three design patterns of RNN.

The first is characterized by hidden-to-hidden connections:

![1st design pattern](/assets/images/deeplearning/fig-10.3.png)

where $\boldsymbol{W}$ is the hidden-to-hidden weight matrix, $\boldsymbol{V}$ is the hidden-to-output weight matrix and $\boldsymbol{U}$ is the intput-to-hidden weight matrix.

Assume that $\boldsymbol{o}$ is unnormalized probability. We can formulate the pattern as

$$
\begin{align*}
\boldsymbol{a}^{(t)} &= \boldsymbol{b} + \boldsymbol{W} \boldsymbol{h}^{(t-1)} + \boldsymbol{U} \boldsymbol{x}^{(t)} \\
\boldsymbol{h}^{(t)} &= \tanh (\boldsymbol{a}^{(t)}) \\
\boldsymbol{o}^{(t)} &= \boldsymbol{c} + \boldsymbol{V} \boldsymbol{h}^{(t)} \\
\hat{\boldsymbol{y}}^{(t)} &= \operatorname{softmax} (\boldsymbol{o}^{(t)}) \\
\end{align*}
$$

Forward propagation starts with $\boldsymbol{h}^{(0)}$ with a initial value. The loss ${L}^{(t)}$ is the negative log likelihood

$$
{L}^{(t)} = - \log p_{\text{model}} \left( t^{(t)} | \boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(t)} \right)
$$

which can be obtained from the corresponding entry of $\hat{\boldsymbol{y}}^{(t)}$.

The total loss is additive so that

$$
L(\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(t)}, \boldsymbol{y}^{(1)}, \dots, \boldsymbol{y}^{(t)}) = \sum_t {L}^{(t)}
$$

The back-propagation algorithm requires $O(t)$ steps through the sequence and cannot be parallelized. It is sometimes called back-propagation through time or BPTT.

The second pattern features output-to-hidden connections:

![Alt text](/assets/images/deeplearning/fig-10.4.png)

It is strictly less powerful than the first pattern, since all of the information of the past is captured by the output which is trained to fit the target and thus unlikely to capture the necesssary information.
The advantage of removing hidden-to-hidden links is that each time step can be trained concurrently as we will see.

The third pattern has hidden-to-hidden connections but only a single output:

![Alt text](/assets/images/deeplearning/fig-10.5.png)

### Teacher forcing and networks with output recurrence

Teacher forcing is a training technique applicable to RNNs with output-to-hidden connections.


### Recurrent networks as directed graphical models


