# Convolutional networks

## The concolution operation

Concolution is an operation defined on two functions that produces a third function.
In general it can be defined as

$$
s(t) = (x \ast w) (t) = \int_{-\infty}^{\infty} x(a) w(t-a) \,d a
$$

A particular interpretation of the convolution is the weighted average. In this case, $w(a)$ is constrained to be a valid probability density function.

The discrete convolution is defined as

$$
s(t) = (x \ast w) (t) = \sum_{a=-\infty}^\infty x(a) w(t-a) \,d a
$$

In machine learning context, the function $x(t)$ is often refered to as the **input**, the function $w(a)$ as the **kernel** and the output is sometimes refered to as the **feature map**.
Usually the input and the output are multi-demensional arrays called tensors. And we assume that these functions take zero value everywhere but a finite set of points where we specified values.

For a 2-D image $I$ and a 2-D kernel $K$, the convolution is given by

$$
S(i, j) = (I \ast K) (i, j) = \sum_m \sum_n I(m, n) K(i-m, j-n)
$$

Convolution is commutative, so that

$$
S(i, j) = (K \ast I) (i, j) = \sum_m \sum_n I(i-m, j-n) K(m, n)
$$

In both forms, we say the kernel is flipped in a sense that as the index into the input increases, the index into the kernel decreases.

A convolution operation without flipping the kernel is called the **cross-correlation**, which is given by

$$
S(i, j) = (K \ast I) (i, j) = \sum_m \sum_n I(i+m, j+n) K(m, n)
$$

Many machine learning implementations use cross-correlation but call it convolution. It is not commutative.

Discrete convolution can be viewed as matrix multiplication. In the case of univariate discrete convolution, the kernel is given by a **Toeplitz matrix**. 
A kernel consisting of 3 elements corresponds to

$$
\begin{pmatrix}
K_1 & 0 & \cdots & 0 & 0 \\
K_2 & K_1 & \cdots & 0 & 0 \\
K_3 & K_2 & \cdots & 0 & 0 \\
0 & K_3 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & K_1 & 0 \\
0 & 0 & \cdots & K_2 & K_1 \\
0 & 0 & \cdots & K_3 & K_2 \\
0 & 0 & \cdots & 0 & K_3 \\
\end{pmatrix}
$$

In two dimensions, the kernel corresponds to a doubly block circulant matrix.

Since the size of kernel is usually much smaller than the input, these matrices are very sparse.

## Motivation

Three ideas: sparse interactions, parameter sharing and equivariat representations.

Convolutional networks have sparse interactions (a.k.a sparse connectivity or sparse weights) because a output unit only interacts with a small sets of the input (and vice versa).
This reduces the number of parameters and consequently the memory and computational cost.
Consider a netowrk with single-layer weights, $m$ inputs and $n$ outputs. If the network is fully connected, the number of weights is $O(m \times n)$. If the connection to each output units is limited to k, then the number of weights is cut down to $O(k \times n)$.
Units of deeper layers indirectly interact with a larger number of the input than the shallow layer.

Concolutional networks share parameters by applying the same kernel at every position of the input, so that each parameter in the kernel has effect on output units.

Equivariance is a property that if the input changes, the output changes in the same way.
Specifically, a function $f(x)$ is equivariant if $f(g(x)) = g(f(x))$. The convolutional network is equivariant to translation.

Concolution is not equivariant to other transformations like scale or rotation.

## Pooling

A typical convolutional network consists of three stages:

1. Convolution state which generates multiple linear activations.
2. Detector stage which applies non-linear activation function.
3. Optional pooling stage which uses a pooling function to modify the output.

A pooling function replace an output unit with a summary statistic of its neighbor. For example:

- Max pooling selects the maximum within a rectangular neighborhood.
- Average pooling

Pooling makes the network invariant to small translations of the input.
Invariance means when the input changes, the output stays unchanged.
Invariance to other kinds of transformations can also be built through multi-channel pooling.

Pooling with a stride large than 1 reduces the number of output units which improves computational efficiency for the next layer.

Pooling can produce fixed-size output even if the input is has variable size. This is usually done by first dividing detector units into groups with equal size and then applying pooling function on each of them.

Dynamic pooling

## Convolution and pooling as an infinitely strong prior

A convolutional network can be regarded as a fully connected network with an infinitely strong prior over the weights.

Underfitting can occur if we do not make careful assumption about the data.

## Variants of the basic convolution function

It is common to use multi-channel convolution. 

Consider a 4-D kernel tensor $\mathsf{K}$ with elements $K\_{i, j, k, l}$ where $i$ denotes the output channel, $j$ denotes the input channel, $k$ denotes the row and $l$ denotes the column.
Assume input tensor $\mathsf{V}$ is defined by $V\_{i, j, k}$ where $i$ is the input channel, and $j$ and $k$ are row/column index respectively.
The output $\mathsf{Z}$ takes the same format as $\mathsf{V}$.
If $\mathsf{Z}$ is produced by convolving $\mathsf{K}$ over $\mathsf{V}$ without kernel flipping, then

$$
Z_{i, j, k} = \sum_{l, m, n} V_{i, j+m-1, k+n-1} K_{i, l, m, n}
$$

We may skip some positions by putting a **stride** to obtain a smaller feature map. This is equivalent to downsampling the output of a full convolution.

$$
Z_{i, j, k} = c(\mathsf{K}, \mathsf{V}, s)_{i,j,k} = \sum_{l, m, n} V_{i, (j-1)s+m, (k-1)s+n} K_{i, l, m, n}
$$

Different zero-padding schemes give different sizes of convolution outputs.
In MATLAB terminology, a **valid** convolution only performs convolution within the region of the input. If the intput has width $m$, the kernel has width $k$, the output width will shrink to $m - k + 1$. This will severly limit the depth of a network.
A **same** convolution keeps the size of the output equal to the size of the input. 
A **full** concolution evaluation each pixel at $k$ positions of the kernel, resulting an output of size $m + 2(k - 1)$.

Usually the optimal zero-padding lies between 'valid' and 'same'.

A locally connected layer, sometimes called **unshared convolution**, has an intermediate complexity between the convolution and a fully connected layer.
Each position of the input uses different kernels with seperate parameters.
Let $\mathsf{W}$ be a 6-D tensor with elements $W\_{i,j,k,l,m,n}$ where

- $i$ is the output channel
- $j$ is the row index of the output
- $k$ is the column index of the output
- $l$ is the input channel
- $m$ is the row index of the input
- $n$ is the column index of the input

Then we have

$$
Z_{i, j, k} = \sum_{l, m, n} V_{i, j+m-1, k+n-1} W_{i,j,k,l,m,n}
$$

A convolution is illustrated

![Convolution](/assets/images/deeplearning/fig-9.14-2.png)

A locally connected layer is illustrated

![Locally connected](/assets/images/deeplearning/fig-9.14-1.png)

A fully connected layer

![Fully connected](/assets/images/deeplearning/fig-9.14-3.png)

Complexity may also be redueced by limiting interactions between channels. For example, each output channel is constrained to connect a subset of input channels.

Reduced connectivity gives fewer parameters and lowers computational cost and memory comsumption without compromising the number of hidden units.

**Tiled convolution** has a higher complexity than locally connected layer. Instead of using distinct kernels at each position, it applies a fixed set of kernels in a round-robin manner.
Consider a 6-D kernel tensor $\mathsf{K}$ with two additional dimensions specifying the choice of the kernel. If $t$ is the number of different kernels, we have

$$
Z_{i, j, k} = \sum_{l, m, n} V_{i, j+m-1, k+n-1} K_{i,l,m,n,j\%t+1,k\%t-1}
$$

Tiled convolution

![Tiled convolution](/assets/images/deeplearning/fig-9.16-2.png)


