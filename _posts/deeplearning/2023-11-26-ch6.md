# Ch6 Deep feedforward networks

## Gradient-based learning

Most loss functions in neural networking traning are non-convex, which disallows convex optimization methods.

### Cost functions

In most cases, we estimate the conditional probability $p(\boldsymbol{y}\|\boldsymbol{x}; \boldsymbol{\theta})$ using the maximum likelihood.
The cost function is given by the negative log-likelihood, a.k.a the cross-entropy error function

$$
J(\boldsymbol{\theta}) = - \operatorname{E}_{\mathbf{x, y} \sim \hat{p}_{\text{data}}} \log p_{\text{model}} (\boldsymbol{y} | \boldsymbol{x})
$$

The log operation gives two advantages.  For independent observations, it decomposes the product into additive terms.
For distributions of exponential family it further cancels the exponential operation, which may avoid the saturation problem.

Another form of cost function is the functional, which gives an estimator $f(\boldsymbol{x}; \boldsymbol{\theta})$ of some statistics of $\boldsymbol{y}$.
Given a specific cost function, using variational calculus, we may obtain an optimal solution of $f$.

For example, the qudratic cost function

$$
J(f) = \operatorname{E}_{\mathbf{x, y} \sim p_{\text{data}}} \| \boldsymbol{y} - f(\boldsymbol{x}) \|^2
$$

yeilds a solution

$$
f^\ast (\boldsymbol{x}) = \operatorname{E}_{\mathbf{y} \sim p_{\text{data}}(\boldsymbol{y}|\boldsymbol{x})} [\boldsymbol{y}]
$$

which is the conditional mean of $\boldsymbol{y}$ given $\boldsymbol{x}$.

The **mean absolute error** function

$$
J(f) = \operatorname{E}_{\mathbf{x, y} \sim p_{\text{data}}} \| \boldsymbol{y} - f(\boldsymbol{x}) \|_1
$$

yeilds a function that predicts the median of $\boldsymbol{y}$ given $\boldsymbol{x}$.

The functional approach may suffer the problem of saturate for some choices of output units.

### Output units

Units used as outputs may also be used as hidden units.

#### Linear units for Gaussian distribution

Given hidden features defined by $\boldsymbol{h} = f(\boldsymbol{x}; \boldsymbol{\theta})$,
a layer of lienar output units is given by $\hat{\boldsymbol{y}} = \boldsymbol{W}^{\mathsf{T}} \boldsymbol{h} + \boldsymbol{b}$.

The linear output layer is often used as the mean of the conditional Gaussian distribution

$$
p(\boldsymbol{y}|\boldsymbol{x}) = \mathcal{N} (\boldsymbol{y}; \hat{\boldsymbol{y}}, \boldsymbol{I})
$$

Which is equivalent to minimizing the mean squared error.

Linear units do not saturate.

#### Sigmoid units for Bernouli distribution

Let $y \in \{0, 1\}$ be a binary random variable. The goal is to predict $P(y=1\|\boldsymbol{x})$.

First we produce a linear layer of activation $z = \boldsymbol{w}^{\mathsf{T}} \boldsymbol{h} + b$.
For the probability to be valid, then apply a linear activation function

$$
\hat{y} = P(y=1|\boldsymbol{x}) = \max\{0, \min\{1, z \}\}
$$

However, if $z$ lies outside the interval $[0, 1]$, the gradient will be zero and the optimization cannot precede properly.

Assume that the log distribution is linear in $y$ and $z$, which is often the case for exponential family. We have

$$
\log P(y) = zy + \text{const}
$$

Normalizing $P(y)$ gives

$$
P(y) = \sigma((2y - 1) z)
$$

Here $z$ is called a **logit**.

Based on the maximum likelihood, the cost function is then given by

$$
\begin{align*}
J(\boldsymbol{\theta}) &= - \log P(y | \boldsymbol{x}) \\
&= - \log \sigma((2y - 1) z) \\
&= \zeta ((2y - 1) z) \\
\end{align*}
$$

The use of softplus instead of the sigmoid avoids the saturation.

#### Softmax units for categorical distributions

Let $y$ be a discrete variables with $n$ states. The goal is to predict $\hat{y}\_i = P(y=i\|\boldsymbol{x})$.

Let $\boldsymbol{z} = \boldsymbol{W}^{\mathsf{T}} \boldsymbol{h} + \boldsymbol{b}$.
Similar to the sigmoid units, we construct the distribution as

$$
\log P(y=i|\boldsymbol{x}) = z_i
$$

Normalizing the distribution gives

$$
\hat{y}_i = \operatorname{softmax} (\boldsymbol{z})_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
$$

The softmax can saturate when the difference between values are large.
Taking the negative logarithm gives

$$
\log \operatorname{softmax} (\boldsymbol{z})_i = \log \sum_j \exp(z_j) - z_i
$$

which cancels the effect of $\exp$ and avoids the saturation.

A numerically stable version of softmax is given by

$$
\operatorname{softmax} (\boldsymbol{z}) = \operatorname{softmax} (\boldsymbol{z} - \max_i z_i)
$$

An alternative way to motivate the softmax is using the fact that the $\sum\_i \hat{y}\_i = 1$.
So the degree of freedom is $n-1$. We can define a fixed activation $z\_0 = 0$. When $n=2$, this is exactly the sigmoid unit.
In practice, they rarely make a difference. The overparameterized version is simpler to implement.

From the neuroscientific view, the competition between arguments of the softmax is analogous to the lateral inhibition between neurons.

The name 'softmax' is somewhat confusing. It is better to be called 'softargmax'.
The soft version of the max function is $\operatorname{softmax}({\boldsymbol{z}})^{\mathsf{T}} \boldsymbol{z}$.

#### Other output types

## Hidden units

### Rectified linear units and their generalizations

Rectified linear units (ReLU) use the activation function

$$
g(z) = \max \{0, z\}
$$

It has the advantage of a large and consistent gradient. 

When used on top of an affine transformation

$$
\boldsymbol{h} = g(\boldsymbol{W}^{\mathsf{T}} \boldsymbol{x} + \boldsymbol{b})
$$

it is good to intialize values of $\boldsymbol{b}$ to some small postive number, so the ReLU is likely to be be active initially.

A disadcantage of ReLU is that all negtive activations become zero immediately, from which the algorithm cannot learn via gradient-based methods.

Three generalizations of ReLU are based on using a non-zero slope $\alpha\_i$ when $z\_i < 0$ so that $g(\boldsymbol{z}, \boldsymbol{\alpha})\_i = \max(0, z\_i) + \alpha\_i \min (0, z\_i)$.
The **absolute value rectification** takes $g(z) = \|z\|$.
The **leaky ReLU** fixes $\alpha\_i$ to some small value like $0.01$.
The **Parametric ReLU** or **PReLU** makes $\alpha\_i$ an adaptive parameter.

**Maxout units** devides $\boldsymbol{z}$ into groups of $k$. Each maxout unit choose the maximum element of a group:

$$
g(\boldsymbol{z})_i = \max_{j \in \mathbb{G}^{(i)}} z_j
$$

where $\mathbb{G}^{(i)}$ is the i-th group.

A maxout unit can learn a piece wise linear, convex function with up to $k$ pieces.
It typically requries more regularization.
It helps to resist the catastrophic forgetting.

The ReLU and its generalizations are based on the principle that models are easier to optimiza if they are close to linear.

### Logistic sigmoid and hyperbolic tangent

The logistic sigmoid activation function

$$
g(z) = \sigma(z)
$$

The hyperbolic tangent acticvation function

$$
g(z) = \tanh (z)
$$

They are related by $\tanh (z) = 2 \sigma (2 z) - 1$.

Both functions saturate across most of their domain, which discourage the use as activation function of hidden units. As the output units, the saturation can be undone by choosing a approapriate cost function.

Hyperbolic tangent typically performs better than the logistical sigmoid as an activations function.

### Other hidden units

Most hidden units perform comparably well. New hidden unit stands out only when it has significant advantages.

## Architecture design

### Universal approximation properties and depth

**Universal approximation theorem** states that given enough hidden units,
a feedforward network with lienar outputs can approximate any Borel measurable function.
The theorem applies for a wide class of activation including the ReLU, sigmoid and softmax.

Note that, the theorem states the ability of MLP to represent the function but does not guarantee that the algorithm can actually learn the function.
A learning algorithm may fail to find the correct parameters which consequently cause either under-fitting or over-fitting.

In the worst case, an exponential number of hidden units may be required to achieve certain degree of accuracy. 
Consider a function $f: \{0, 1\}^n \rightarrow \{0, 1\}$. The total number of possible functions is $2^{2^n}$. A specific choice of function takes $2^n$ bits, which require $O(2^n)$ degrees of freedom.

A single-layer feedforward that is suffiecient powerful may require infeably large number of hidden units.
In many circumstances, using deeper models can reduce the number of units required to achieve the same level of accuracy and generalization.

### Other achitectural considerations

Besides bipartite 
The strategies of conencting layers are diverse and usually application-specific.

## Back-propagation and other differentiation algorithms

### Computational graph

Each node in the graph represents a variable.

A operation is define on one or more nodes to compute some other node.

### Chain rule of calculus

Define mapping $g: \mathbb{R}^m \rightarrow \mathbb{R}^n$ and $f: \mathbb{R}^n \rightarrow \mathbb{R}$.
If $\boldsymbol{y} = g(\boldsymbol{x})$ and $z = f(\boldsymbol{y})$, by the chain rule we have

$$
\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}
$$

It can be written in the vector notation

$$
\nabla_{\boldsymbol{x}} z = \left( \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \right)^{\mathsf{T}} \nabla_{\boldsymbol{y}} z
$$

where $\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}$ denotes the Jacobian matrix.

### Recursively applying the chain rule to obtain backprorp

Consider a computational graph with $n$ nodes where each node $u^{(i)}$ is associated with an operation $f^{(i)}$ such that

$$
u^{(i)} = f^{(i)} (\mathbb{A}^{(i)})
$$

where $\mathbb{A}^{(i)}$ is the set of parent nodes of $u^{(i)}$.

Without loss of generality, we assume that nodes are indexed by their topological order so that $u^{(1)}, \dots, u^{(n\_i)}$ are input nodes and $u^{(n)}$ are the output node.

Then $u^{(n)}$ can be evaluate using the following procedure

<img alt="algorithm 6.1" src="/assets/images/deeplearning/alg-6.1.png" width="256"/>

This gives the procedure of forward propagation.

To evaluate the direvatives of $u^{(n)}$ with respect to variables in the graph, we use the chain rule

$$
\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i : j \in Pa(u^{(i)})} \frac{\partial u^{(n)}}{\partial u^{(i)}} \frac{\partial u^{(i)}}{\partial u^{(j)}}
$$

where $i : j \in Pa(u^{(i)})$ denotes the children of $u^{(j)}$.

To avoid evaluations of repeated subexpressions, we use the table-filling dynamic programming.
The back-propagation algorithm is given as follows:

<img src="/assets/images/deeplearning/alg-6.2.png"/>

### Back-propagation computation in fully-connected MLP

The forward procedure

<img src="/assets/images/deeplearning/alg-6.3.png" width="70%"/>

The backward procedure

<img src="/assets/images/deeplearning/alg-6.4.png" width="80%"/>

### Symbol-to-symbol derivatives

Algebraic expressions and computational graphs both operate on symbols rather than specific values. Such representations are called the **symbolic** representations.

The symbol-to-number approach to back-propagation takes a computational graph and a set of input values to the graph, and then returns the results. This approach is applied by libraries such as Torch and Caffe.

The symbol-to-symbol approach takes a computational graph and add additional nodes to represent derivatives. E.g.

<img src="/assets/images/deeplearning/fig-6.10.png" width="50%"/>

This approach is used by Theano and Tensorflow. An advantage of this approach is that we can run back-propagation again to obtain higher derivatives.
Moreover, the evaluation of a node can begin once the values of its parents are available, which may enable concurrent evaluation.

Both approaches perform the exact same computations. The key difference is that the first approach does not expose a graph.

### General back-propagation

TODO


