# Optimization for Training Deep Models

## Chanllenges in neural network optimization

### Ill-conditioning

As discussed in ch4, ill-conditioned Hessian matrix can cause difficulty in choosing step size.

A method to determine wether ill-conditioning is detrimental is to monitor the terms $\boldsymbol{g}^{\mathsf{T}} \boldsymbol{g}$ and $\boldsymbol{g}^{\mathsf{T}} \boldsymbol{H} \boldsymbol{g}$.
In the case of ill-conditioning, it usually appears that the squared gradient norm does not shrink significantly but the $\boldsymbol{g}^{\mathsf{T}} \boldsymbol{H} \boldsymbol{g}$ grows by more than an order of maginitude.

### Local Minima

Deep models generally have a large number of local minima.

A model is said to be identifiable if a sufficiently large data set can rule out all but one set of parameters.
Models with latent variables are often not identifiable. For instance, we can exchange the components along with the latent variable of a mixture model without comprising the equivalence.
For neural network, a common source of non-identifiability comes from the weight space symmetry.

Local minima with high cost can be problematic for gradient-based algorithms. 
A test that can rule out the local minima as the problem is to plot the norm of the gradient over time.

### Plateaus, saddle points and other flat regions



## Basic algorithms

### Stochastic gradient descent

The SGD is given as follows

![](/assets/images/deeplearning/alg-8.1.png)

An advantage of SGD is that it does not require evaluation of the whole dataset to begin learning.
For large datasets, the convergence criterion may be met before all training data points are processed.



