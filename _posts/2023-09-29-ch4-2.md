- [Probabilistic discriminative models](#probabilistic-discriminative-models)
  - [Fixed basis functions](#fixed-basis-functions)
  - [Logistic regression](#logistic-regression)
  - [Iterative reweighted least squares](#iterative-reweighted-least-squares)
  - [Multiclass logistic regression](#multiclass-logistic-regression)
  - [Probit pregression](#probit-pregression)
  - [Canonical link functions](#canonical-link-functions)
- [The Laplace approximation](#the-laplace-approximation)
  - [Model comparison and BIC](#model-comparison-and-bic)
- [Bayesian logistic regression](#bayesian-logistic-regression)
  - [Laplace approximation](#laplace-approximation)
  - [Predictive distribution](#predictive-distribution)

## Probabilistic discriminative models

### Fixed basis functions

By making a fixed nonlinear transformation on $\mathbf{x}$, the decion boundaries may become linear in feature space of $\boldsymbol{\phi}$.

However, such transformation cannot remove overlap among classes.

### Logistic regression

Consider a two-class classification problem. We write the posterior distributions as

$$
\begin{align*}
p(C_1|\boldsymbol{\phi}) &= y (\boldsymbol{\phi}) = \sigma (\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) \tag{4.87} \\
p(C_2|\boldsymbol{\phi}) &= 1 - p(C_1|\boldsymbol{\phi}) \\
\end{align*}
$$

where $\boldsymbol{\phi}$ is the feature vector and $\sigma$ is the logistic sigmoid function.

In statistics, this model is called the logistic regression, however it's for classification not for regression.

For an M-dimensional $\boldsymbol{\phi}$, if we model the class distribution with Gaussians,
the number of parameters will be quadratic in $M$.
In logistic regression, only $M$ parameters needs to estimate, which shows significan advantage when $M$ is large.

We now use maximum likelihood to estimate $\mathbf{w}$. Before we go, the derivative of signoid function is given by

$$
\frac{d \sigma}{d a} = \sigma \cdot (1 - \sigma) \tag{4.88}
$$

Given a data set $\{\boldsymbol{\phi}\_n, t\_n\}$, where $N=1, \dots, N$ and $\boldsymbol{\phi}\_n = \boldsymbol{\phi} (\mathbf{x}\_n)$,
the likelihood function takes the form

$$
p(\mathbf{t}|\mathbf{w}) = \prod_{n=1}^N y_n^{t_n} (1-y_n)^{1-t_n} \tag{4.89}
$$

where $\mathbf{t} = (t\_1, \dots, t\_N)^{\mathsf{T}}$ and $y\_n = p(C\_k\|\boldsymbol{\phi}\_n) = \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n)$.

Taking the negtive logrithm, we obtain the **cross-entropy** error function 

$$
E(\mathbf{w}) = -\ln p(\mathbf{t}|\mathbf{w})
= - \sum_{n=1}^N \left\{ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right\} \tag{4.90}
$$

Using (4.88), we have the derivative

$$
\frac{\partial y_n}{\partial \mathbf{w}} = y_n (1 - y_n) \boldsymbol{\phi}_n
$$

Taking the gradient of the error function with respect to $\mathbf{w}$, we have

$$
\nabla E(\mathbf{w}) = \sum_{n=1}^N (y_n - t_n) \boldsymbol{\phi}_n
$$

With the gradient, we can give a sequential algorithm.

However, this error function will cause severe over-fitting for linearly separable data sets. 
For such data sets, we will first find a $\mathbf{w}$ such that $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n > 0$ for $n \in C\_1$ and $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n < 0$ for $n \in C\_2$,
and from (4.90) we see $E(\mathbf{w}) \ge 0$. Therefore to achieve the minimum of $0$ we want $y\_n \rightarrow t\_n$,
which will drive the magnitude of $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n$ to infinity and thus $y\_n \rightarrow 0 \text{ or } 1$.

The singularity can be avoided by conducting a MAP method or introducing a regularization term.

### Iterative reweighted least squares

The Newton-Raphson method for optimization the error function takes the form

$$
\mathbf{w}^{(\mathrm{new})} = \mathbf{w}^{(\mathrm{old})} - \mathbf{H}^{-1} \nabla E(\mathbf{w})
$$

where $\mathbf{H}$ is the Hessian matrix.

First we try this method on the linear regression model with the sum-of-squares error function defined by (3.12).
The gradient and the Hessian are given by

$$
\begin{align*}
\nabla E(\mathbf{w}) &= \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{w} - \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
\mathbf{H} &= \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \\
\end{align*}
$$

It can be seen that $\mathbf{H}$ is postive semi-definite,
the sum-of-squares error function is convex. We can use the Newton-Raphson to get the iteration

$$
\begin{align*}
\mathbf{w}^{(\mathrm{new})} &= \mathbf{w}^{(\mathrm{old})}
- (\boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1}
\left\{ \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{w}^{(\mathrm{old})} - \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \right\} \\
&= (\boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
\end{align*}
$$

which cancels the $\mathbf{w}^{\mathrm{old}}$ and is the exact solution for least-sqaures.

For logistic regression model with two classes, we optimize the cross-entropy error function defined by (4.90). The gradient and the Hessian gives

$$
\begin{align*}
\nabla E(\mathbf{w}) &= \sum_{n=1}^N (y_n - t_n) \boldsymbol{\phi}_n
= \boldsymbol{\Phi}^{\mathsf{T}} (\mathbf{y} - \mathbf{t}) \\
\mathbf{H} &= \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi}
\end{align*}
$$

where $\mathbf{R}$ is an $N \times N$ diagonal matrix with elements

$$
R_{nn} = y_n (1 - y_n)
$$

Let $\mathbf{u} \ne \mathbf{0}$ be a vector in $\mathbb{R}^D$. Then we have

$$
\mathbf{u}^{\mathsf{T}} \mathbf{H} \mathbf{u}
= (\boldsymbol{\Phi} \mathbf{u})^{\mathsf{T}} \mathbf{R} (\boldsymbol{\Phi} \mathbf{u})
$$

Since $\mathbf{R}$ is diagonal and $0 < R\_{nn} < 1$, $\mathbf{R}$ is postive definite.
Provided that columns of $\boldsymbol{\Phi}$ are linearly independent,
we have $\boldsymbol{\Phi} \mathbf{u} \ne \mathbf{0}$ and hence $\mathbf{H}$ is postive definite.
The error function is strictly convex and we can conduct Newton-Raphson iteration

$$
\begin{align*}
\mathbf{w}^{(\mathrm{new})} &= \mathbf{w}^{(\mathrm{old})}
- (\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf{T}} (\mathbf{y} - \mathbf{t}) \\
&= (\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi})^{-1} 
\left\{ \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi} \mathbf{w}^{(\mathrm{old})} - \boldsymbol{\Phi}^{\mathsf{T}} (\mathbf{y} - \mathbf{t}) \right\} \\
&= (\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi})^{-1} 
\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \mathbf{z}
\end{align*}
$$

where

$$
\mathbf{z} = \boldsymbol{\Phi} \mathbf{w}^{(\mathrm{old})} - \mathbf{R}^{-1} (\mathbf{y} - \mathbf{t})
$$

TODO: approximation interpretation

### Multiclass logistic regression

Suppose the posterior distributions are defined in the form of softmax function, which are given by

$$
p(C_k|\boldsymbol{\phi}) = y_k (\boldsymbol{\phi}) 
= \frac{\exp(a_k)}{\sum_j \exp(a_j)}
$$

where activations $a\_k$ are defined by

$$
a_k = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}
$$

We now use the maximum likelihood to estimate the parameters. First of all, we show a derivative

$$
\frac{\partial y_k}{\partial y_j} = y_k (I_{kj} - a_j)
$$

Suppose target vector $\mathbf{t}\_n$ uses 1-of-K coding scheme and corresponds to feature vector $\boldsymbol{\phi}\_n$.
The likelihood function is given by

$$
p(\mathbf{T}|\mathbf{w}_1, \dots, \mathbf{w}_K)
= \prod_{n=1}^N \prod_{k=1}^K p(C_k|\boldsymbol{\phi}_n)^{t_{nk}}
= \prod_{n=1}^N \prod_{k=1}^K y_{nk}^{t_{nk}}
$$

where we defined $y\_{nk} = p(C\_k\|\boldsymbol{\phi}\_n)$, and $\mathbf{T}$ is an $N \times K$ matrix with elements $t\_{nk}$.

Taking the negtive logarithm, we obtain the cross-entropy error function

$$
E(\mathbf{w}_1, \dots, \mathbf{w}_K) = - \ln p(\mathbf{T}|\mathbf{w}_1, \dots, \mathbf{w}_K)
= - \sum_{n=1}^N \sum_{k=1}^K t_{nk} \ln y_{nk}
$$

Taking the radient with repsect to $\mathbf{w}\_j$, we have

$$
\begin{align*}
\nabla_{\mathbf{w}_j} E(\mathbf{w}_1, \dots, \mathbf{w}_K)
&= - \sum_n \sum_k t_{nk} \frac{1}{y_{nk}} y_{nk} (I_{kj} - y_{nj}) \boldsymbol{\phi}_n \\
&= -\sum_n \sum_k I_{kj} t_{nk} \boldsymbol{\phi}_n
+ \sum_n \sum_k t_{nk} y_{nj} \boldsymbol{\phi}_n \\
&= -\sum_n t_{nj} \boldsymbol{\phi}_n + \sum_n y_{nj} \boldsymbol{\phi}_n \\
&= \sum_n (y_{nj} - t_{nj}) \boldsymbol{\phi}_n \\
\end{align*}
$$

from which we can derive a sequential algorithm using stochastic gradient descent.

Taking the gradient again with respect to $\mathbf{w}\_k$, we obtain

$$
\mathbf{H}_{jk} = \nabla_{\mathbf{w}_k} \nabla_{\mathbf{w}_j} E(\mathbf{w}_1, \dots, \mathbf{w}_K)
= - \sum_{n=1}^N y_{nk} (I_{kj} - y_{nj}) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \tag{4.110}
$$

which is block $(j, k)$ of the Hessian matrix. Note that, the Hessian now comprises of $K \times K$ blocks each of which has a size of $M \times M$.

To show that the Hessian is postive semi-definite. Consider a non-zero vector $\mathbf{u}$ of size $MK$, which is concatenated vertically by vectors $\mathbf{u}\_i$ of size $M$.

Then we have

$$
\begin{align*}
\mathbf{u}^{\mathsf{T}} \mathbf{H} \mathbf{u}
&= \sum_{i=1}^K \sum_{j=1}^K \mathbf{u}_i^{\mathsf{T}} \mathbf{H}_{ij} \mathbf{u}_j \\
&= \sum_{i=1}^K \sum_{j=1}^K \mathbf{u}_i^{\mathsf{T}}
\left( \sum_{n=1}^N y_{nj} (I_{ij} - y_{ni}) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \right) \mathbf{u}_j \\
&= \sum_i \sum_j \sum_n \mathbf{u}_i^{\mathsf{T}} y_{nj} I_{ij} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_j 
- \sum_i \sum_j \sum_n \mathbf{u}_i^{\mathsf{T}} y_{nj} y_{ni} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_j \\
&= \sum_n \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_i
- \sum_n \left( \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \right) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \left( \sum_i y_{ni} \mathbf{u}_i \right) \\
&= \sum_n \left\{ \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_i 
- \left( \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \right) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \left( \sum_i y_{ni} \mathbf{u}_i \right) \right\} \\
\end{align*}
$$

in which $\mathbf{H}\_{ij}$ is block $(i, j)$ of the Hessian matrix given by (4.110).

Defining a function

$$
f(\mathbf{u}) = \mathbf{u}^{\mathsf{T}} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}
$$

we have

$$
f(\mathbf{u}) = \left\| \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u} \right\|^2 \ge 0
$$

Hence $\boldsymbol{\phi} \boldsymbol{\phi}^{\mathsf{T}}$ is postive semi-definite 
and $f(\mathbf{u})$ is convex.

Using Jesnsen's inequality we have:

$$
f \left( \frac{\sum_i y_{ni} \mathbf{u}_i}{\sum_i y_{ni}} \right)
\le \sum_i y_{ni} f(\mathbf{u}_i)
$$

Here we use $\sum\_i y\_{ni} = 1$ and the equality holds if and only if $\mathbf{u}\_1 = \cdots = \mathbf{u}\_K$.

Therefore, we have

$$
\mathbf{u}^{\mathsf{T}} \mathbf{H} \mathbf{u} \ge \mathbf{0}
$$

and $\mathbf{H}$ is postive semi-definite.

### Probit pregression

Althogh a broad range of distributions can be expressed as a sgimoid (or softmax) transformation on a linear function of input variables,
there's still some distributions for which we cannot apply the approach, e.g. mixture distributions.

Consider a two-class problem, and a generalized linear model defined by

$$
p(t=1|a) = f(a)
$$

where $a=\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}$ and $f(\cdot)$ is the activition function.

For each input $\boldsymbol{\phi}\_n$, we evaluate $a\_n = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n$ and select the target value according to

$$
t_n = 
\begin{cases}
1 & \text{ if } a_n \ge \theta \\
0 & \text{ otherwise} \\
\end{cases}
$$

in which $\theta$ is a noisy threshold draw from the distribution $p(\theta)$.

Then the activation function is given by the cumulative distribution function

$$
f(a) = \int_{- \infty}^a p(\theta) \,d\theta
$$

TODO: illustration of mixture of gaussian

When $p(\theta) = \mathcal{N} (\theta \| 0, 1)$, the function

$$
\Phi (a) = \int_{- \infty}^a \mathcal{N} (\theta|0, 1) \,d\theta
$$

is called the **probit** function.

TODO: plot

Many numerical programs provide methods to calculate the **erf** function or **error** function defined by

$$
\mathrm{erf} (a) = \frac{2}{\sqrt{\pi}} \int_0^a \exp \left( - \frac{\theta^2}{2} \right) \,d\theta
$$

It is related to the probit function by

$$
\Phi (a) = \frac{1}{2} \left\{ 1 + \frac{1}{\sqrt{2}} \mathrm{erf} (a) \right\}
$$

The generalized linear model based on the probit activation is called the probit regression.

The probit model is significantly more sensitive to outiliers than the logistic sigmoid.

Note that, both logistic sigmoid and the probit models assume that the target value is correctly labeled.
To deal with mislabelling, a probability $\epsilon$ is introduced to indicate the chance of mislabelling.
For a two-class classification problem, the target distribution is then given by

$$
\begin{align*}
p(t|\mathbf{x}) &= (1 - \epsilon) \sigma(\mathbf{x}) + \epsilon (1-\sigma(\mathbf{x})) \\
&= \epsilon + (1 - 2 \epsilon) \sigma(\mathbf{x})
\end{align*}
$$

where $\sigma$ is the activation function. $\epsilon$ can be set in advance or estimated from data.

### Canonical link functions

From previous results, we see that the derivative with respect to $\mathbf{w}$ takes the form of the error given by $y\_n - t\_n$ times the feature $\boldsymbol{\phi}\_n$.
Indeed, this is a general result related to the exponential family.

The corresponding choice for activation function is called the canonical link function.

Consider a distribution over target $t$, with the restricted form of expoenential family, which is given by

$$
p(t|\eta, s) = \frac{1}{s} h \left( \frac{t}{s} \right) g(\eta) \exp \left( \frac{\eta t}{s} \right)
$$

We define $y$ as the conditional mean of $t$, such that

$$
y \equiv \operatorname{E} [t|\eta] = - s \frac{d}{d \eta} \ln g(\eta)
$$

This is derived using (2.226), with following relationship

$$
g(\boldsymbol{\eta}) \rightarrow g(\eta),
\quad h(\mathbf{x}) \rightarrow \frac{1}{s} h(\frac{t}{s}),
\quad \boldsymbol{\eta} \rightarrow \eta,
\quad \mathbf{u}(\mathbf{x}) \rightarrow \frac{t}{s}
$$

Define a generalized linear model

$$
y = f(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi})
$$

where $f(\cdot)$ is the activation function, and its inverse $f^{-1}(\cdot)$ is known as the link function in statistics.

The likelihood function for this model is then given by

$$
\ln p(\mathbf{t}|\eta,s) = \sum_{n=1}^N \ln p(t_n|\eta, s)
= \sum_{n=1}^N \left\{ \ln g(\eta_n) + \frac{\eta_n t_n}{s} \right\} + \mathrm{const}
$$

where all observations share a common scale parameter $s$.

Taking derivative with repspect to $\mathbf{w}$, we have

$$
\begin{align*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t}|\eta,s) 
&= \sum_{n=1}^N \left\{ \frac{d}{d \eta_n} \ln g(\eta_n) + \frac{t_n}{s} \right\}
\frac{d \eta_n}{d a_n} \nabla a_n \\
&= \sum_n \frac{1}{s} (t_n - y_n) \frac{d \eta_n}{d a_n} \boldsymbol{\phi}_n \\
\end{align*}
$$

where $a\_n = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n$ and $y\_n = f(a\_n)$.

If we choose a particular link function $f^{-1}$ so that

$$
f^{-1} (y) = \eta
$$

then we have

$$
\begin{align*}
&f^{-1} (y_n) = f^{-1} (f(a_n)) = a_n = \eta_n \\
&\frac{d \eta_n}{d a_n} = 1 \\
\end{align*}
$$

Therefore, the gradient of the error function takes the form

$$
\begin{align*}
\nabla E(\mathbf{w}) &= - \nabla_{\mathbf{w}} \ln p(\mathbf{t}|\eta,s) \\
&= \frac{1}{s} \sum_{n=1}^N (y_n - t_n) \boldsymbol{\phi}_n
\end{align*}
$$

For the Gaussian $s = \beta^{-1}$; for logistic model $s=1$.

## The Laplace approximation

To handle the case when the integral over parameters cannot be easily solved, utilization of some approximation method is required.

Consider a univariate distribution given by

$$
p(z) = \frac{1}{Z} f(z)
$$

where $Z = \int f(z) \,dz$. The goal of the Laplace method is to find a Gaussian approximation $q(z)$ centered on the mode of $p(z)$.

Let $z\_0$ be a stationary point of $p(z)$, which satisfies $f^\prime (z\_0) = 0$.

Expanding $\ln f(z)$ at $z\_0$ using Taylor's theorem gives

$$
\begin{align*}
\ln f(z) &\simeq f(z_0) + \frac{f^\prime (z_0)}{f(z_0)} (z - z_0)
+ \frac{1}{2} \left( \frac{d^2}{d z^2} \ln f(z) \bigg|_{z = z_0} \right) (z-z_0)^2 \\
&= f(z_0) - \frac{1}{2} A (z-z_0)^2 \\
\end{align*}
$$

where we define

$$
A = - \frac{d^2}{d z^2} \ln f(z) \bigg|_{z = z_0} 
$$

Taking the exponential we have

$$
f(z) \simeq f(z_0) \exp \left\{ - \frac{A}{2} (z-z_0)^2 \right\}
$$

Normalizing $f(z)$ we obtain the distribution

$$
q(z) = \left( \frac{A}{2 \pi} \right)^{1/2} \exp \left\{ - \frac{A}{2} (z - z_0)^2 \right\}
$$

Note that a well defined $q(z)$ requires $A>0$, that is, the second order derivative is positive and hence $z\_0$ must be a local maximum.

For a random vector $\mathbf{z}$ with dimensionality $M$ and a stationary point $\mathbf{z}\_0$ of $f(\mathbf{z})$.
Using similar approach, we obtain

$$
q(\mathbf{z}) = \mathcal{N} (\mathbf{z} | \mathbf{z}_0, \mathbf{A}^{-1})
$$

where

$$
\mathbf{A} = -\nabla \nabla \ln f(\mathbf{z}) \big|_{\mathbf{z} = \mathbf{z}_0} 
= - \mathbf{H}(\ln f(\mathbf{z})) \big|_{\mathbf{z} = \mathbf{z}_0} \tag{4.132}
$$

is the $M \times M$ Hessian matrix.

$q(\mathbf{z})$ will be well defined if $\mathbf{A}$ is postive definite, which implies $\mathbf{z}\_0$ is a local maximum.

In practive, a mode can be found numerically.

Note that, the Laplace appximation only use the local information at specific point, which fails to capture the global information.

### Model comparison and BIC

TODO

## Bayesian logistic regression

Exact Bayesian inference is intractable for logistic regression, since the evaluation of the posterior and the predictive distribution involves integration of the product of logistic sigmoid functions and the prior probability density. 

### Laplace approximation

Suppose the prior distribution of parameter $\mathbf{w}$ follows Gaussian distribution

$$
p(\mathbf{w}) = \mathcal{N} (\mathbf{w} | \mathbf{m}_0, \mathbf{S}_0)
$$

By

$$
p(\mathbf{w}|\mathbf{t}) \propto p(\mathbf{t}|\mathbf{w}) p (\mathbf{w})
$$

we write the log posterior as

$$
\ln p(\mathbf{w}|\mathbf{t}) = -\frac{1}{2} (\mathbf{w} - \mathbf{m})^{\mathsf{T}} \mathbf{S}_0^{-1} (\mathbf{w} - \mathbf{m}_0)
+ \sum_{n=1}^N \left\{ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right\}
+ \text{constant}
$$

in which $y\_n = \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n)$. 

To conduct Laplace appximation on the posterior distribution, we first find the maximum posterior solution $\mathbf{w}\_{\mathrm{MAP}}$, which is the mean of the Gaussian approximation.
From (4.132), the covariance matrix is given by

$$
\mathbf{S}_N^{-1} = \mathbf{S}_0^{-1} + \sum_{n=1}^N y_n (1-y_n) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}}
$$

Thus the Gaussian approximation to the posterior distribution is given by

$$
q(\mathbf{w}) = \mathcal{N} (\mathbf{w}|\mathbf{w}_{\mathrm{MAP}}, \mathbf{S}_N)
$$

### Predictive distribution

With the posterior distribution, we can obtain the prediction distribution by integrating out the $\mathbf{w}$. Using the approximation we have

$$
p(C_1|\boldsymbol{\phi}, \mathbf{t}) 
= \int p(C_1 | \boldsymbol{\phi}, \mathbf{w}) p(\mathbf{w}|\mathbf{t}) \,d\mathbf{w}
\simeq \int \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \, d\mathbf{w}
$$

The predictive distribution for class $C\_2$ is given by $p(C\_2\|\boldsymbol{\phi}, \mathbf{t})=1 - p(C\_1\|\boldsymbol{\phi}, \mathbf{t})$.

To evaluate the integral, we first define

$$
\sigma (\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi})
= \int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) \sigma (a) \,da
$$

where $\delta(\dot)$ is the Dirac delta function. Then we have

$$
\begin{align*}
\int \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \, d\mathbf{w}
&= \int q(\mathbf{w}) \,d\mathbf{w}
\int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) \sigma (a) \,da \\
&= \int \sigma(a) \,da \int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \,d\mathbf{w} \\
&= \int \sigma(a) p(a) \,da \\
\end{align*}
$$

where we defined

$$
p(a) = \int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \,d\mathbf{w}
$$

To evaluate $p(a)$, without loss of generality, we define $\delta(\cdot)$ as a Gaussian distribution in the limit form such that

$$
\delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi})
= \lim_{\beta \rightarrow 0^+} \mathcal{N} (a | \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}, \beta)
$$

$p(a)$ is now the marginalized distribution of two Gaussians. Using the results of section 2.2.3, we have

$$
p(a) = \mathcal{N} (a| \mu_a, \sigma_a^2)
$$

where

$$
\begin{align*}
\mu_a &= \operatorname{E} [a] = \boldsymbol{\phi}^{\mathsf{T}} \mathbf{w}_{\mathrm{MAP}} \\
\sigma_a^2 &= \operatorname{var} [a] = \boldsymbol{\phi}^{\mathsf{T}} \mathbf{S}_n \boldsymbol{\phi} \\
\end{align*}
$$

Still, the integration of a Gaussian and a logistic sigmoid cannot be evaluated analytically. 
A good approximation for $\sigma(a)$ is $\Phi (\lambda a)$, in which the value of $\lambda$ is determined so that the have the same slope at the origin.
Therefore we have $\lambda = \frac{\pi}{2\sqrt{2}}$ and the integral becomes

$$
\int \sigma(a) \mathcal{N} (a|\mu_a, \sigma_a^2) \,da
\simeq \int \Phi(\lambda a) \mathcal{N} (a|\mu_a, \sigma_a^2) \,da
$$



To evalute the integration, define two random varianbles such that

$$
\begin{align*}
X &\sim \mathcal{N} (0, 1) \\
Y &\sim \mathcal{N} (\mu, \sigma^2) \\
\end{align*}
$$

Then we have

$$
\Phi (\lambda a) = P(X \le \lambda Y | Y=a )
$$

and

$$
P(X - \lambda Y \le 0) = P(X \le \lambda Y) = \int P(X \le \lambda Y | Y=a) p(a) \,da
$$

Since

$$
X - \lambda Y \sim \mathcal{N} (- \lambda \mu, 1 + \lambda^2 \sigma^2)
$$

we have

$$
P(X - \lambda Y \le 0) = \Phi \left( \frac{\lambda \mu}{(1 + \lambda^2 \sigma^2)^{1/2}} \right)
\simeq \sigma \left( k(\sigma^2) \right)
$$

where we used the approximation $\sigma(a) \simeq \Phi(\lambda a)$ again and defined

$$
k(\sigma^2) = (1 + \pi \sigma^2 /8)^{-1/2}
$$

Therefore, we have the approximated predictive distribution given by

$$
p(C_1|\boldsymbol{\phi}, \mathbf{t}) \simeq \sigma \left( k(\sigma^2) \mu_a \right)
$$

Note that the decision boundary corresponding to $p(C\_1 \| \boldsymbol{\phi}, \mathbf{t}) = 0.5$ is given by $\mu\_a = 0$,
which is the same decision boundary as using the $\mathbf{w}\_{\mathrm{MAP}}$ directly.
So if the decision criterion is the misclassification rate, the marginalization over $\mathbf{w}$ has no effect.
