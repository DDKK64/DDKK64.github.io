---
layout: posts
title: "PRML - Probability distributions - Part I"
---
- [Probability Distributions](#probability-distributions)
	- [Binary Variables](#binary-variables)
		- [Beta Distribution](#beta-distribution)
	- [Multinomial Variables](#multinomial-variables)
		- [Dirichlet distribution](#dirichlet-distribution)

# Probability Distributions

Desity estimation: given a set of observations, model the probability distribution of the random variable

## Binary Variables

Considering a random variable $x \in \{ 0, 1 \}$, whose probability distribution is given by

$$
\begin{align*}
p(x=1|\mu) &= \mu \\
p(x=0|\mu) &= 1 - \mu
\end{align*}
$$

where $0 \le \mu \le 1$.

Its equivalent form 

$$
\mathrm{Bern}(x|\mu) = \mu^x (1-\mu)^{1-x}
$$

is called the Bernoulli distribution.

It has following properties

$$
\begin{align*}
\operatorname{E}[x] &= \mu \\
\operatorname{var}[x] &= \mu (1-\mu) \\
\operatorname{mode}[x] &= \begin{cases}
1 & \text{if }	\mu \ge 0.5 \\
0 & \text{otherwise} \\
\end{cases}
\end{align*}
$$

which are straightforward.

Let $D=\{ x\_1, \dots, x\_N \}$ be a set of observed values of $x$. The likelihood function of Bernoulli distribution is then given by

$$
p(D|\mu) = \prod_{n=1}^N p(x_n|\mu) = \prod_{n=1}^N \mu^{x_n}(1-\mu)^{1-x_n}.
$$

Taking logrithm of both sides, we have

$$
\ln p(D|\mu) = \sum_{n=1}^N \left\{ x_n \ln \mu + (1-x_n) \ln (1-\mu)  \right\}.
$$

Setting the derivative with respect to $\mu$ to 0, we obtain the estimator

$$
\mu_{\mathrm{ML}} = \frac{1}{N} \sum_{n=1}^N x_n
$$

which is also known as the **sample mean**. 

> **Sufficient statistics** for a parameter summurize the information required to estimate the parameter.
> $\sum\_{n=1}^N x\_n$ is an example of such a statistic under Bernoulli distribution.

$m$ observations of $x=1$ out of a data set of size $N$ is distributed according to **binomial distribution** given by

$$
\mathrm{Bin}(m|N,\mu) = \binom{N}{m} \mu^m (1-\mu)^{N-m}
$$

where

$$
\binom{N}{m} \equiv \dfrac{N!}{(N-m)!m!}.
$$

Using the binomial theorem, we see that the Bernoulli distribution is normalized.

Its mean and variance is given by

$$
\begin{align*}
\operatorname{E}[m] &= N \mu \\
\operatorname{var}[m] &= N \mu (1-\mu)
\end{align*}
$$

Let $x\_1, \dots, x\_N$ be independent random variables distributed according to Bernoulli distribution, 
and then we see $m=x\_1 + \dots + x\_N$. Therefore

$$
\begin{align*}
\operatorname{E}[m] &= \operatorname{E}[x_1 + \dots + x_N] = \operatorname{E}[x_1] + \cdots+ \operatorname{E}[x_N] = N \mu
\\
\operatorname{var}[m] &= \operatorname{var}[x_1 + \dots + x_N] = \operatorname{var}[x_1] +\cdots + \operatorname{var}[x_N] = N \mu (1-\mu) 
\end{align*}
$$

Another method to derive the mean and variance is by calculus. We first use binomial theorem to obtain

$$
\sum_{m=0}^N \binom{N}{m} \mu^m (1-\mu)^{N-m} = 1,
$$

Taking derivatives with respect of $\mu$, we have

$$
\begin{align*}
\sum_m \binom{N}{m} \left\{ m \mu^{m-1} (1-\mu)^{N-m} - (N-m) \mu^m (1-\mu)^{N-m-1} \right\} &= 0
\\
\frac{\operatorname{E}[m]}{\mu} - N \frac{1}{1-\mu} + \frac{\operatorname{E}[m]}{1-\mu} &= 0 \\
\operatorname{E} [m] &= N\mu \\
\end{align*}
$$

Taking derivatives of $\operatorname{E}[m] = N \mu$ with respect to $\mu$, we have

$$
\begin{align*}
\sum_m m \binom{N}{m} \left\{ m \mu^{m-1} (1-\mu)^{N-m} - (N-m) \mu^m (1-\mu)^{N-m-1} \right\} &= N \\
\frac{\operatorname{E}[m^2]}{\mu} - N \frac{\operatorname{E}[m]}{1-\mu} + \frac{\operatorname{E}[m^2]}{1-\mu} &= N \\
\operatorname{E}[m^2] - N \mu \operatorname{E}[m] = \operatorname{E}[m^2] - (\operatorname{E}[m])^2 &= N \mu (1-\mu) \\
\operatorname{var}[m] &= N \mu (1-\mu) \\
\end{align*}
$$

### Beta Distribution

Bernoulli distribution can give severely over-fitted results on small data sets. E.g. a set of observations with 3 heads up ($x=1$) leads to an estimation of $\mu\_{\mathrm{ML}} = 1$.

A prior distribution on $\mu$ is introduced to treat the problem, which is the **beta distribution** given by

$$
\mathrm{Beta}(\mu|a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1} (1-\mu)^{b-1}
$$

where $\Gamma(x)$ is the gamma function. $a$ and $b$ are called hyperparameters,
since they control the distribution of the parameter $\mu$.

> Motivation: To drive Bayesian treatment, we want a prior distriubtion that has the same form as the likelihood function, and so the posterior will take the same form.
> That is, $\mathrm{C} \cdot \mu^{x} (1-\mu)^{y}$

To show that

$$
\int_{0}^{1} \mathrm{Beta}(\mu|a,b) \,d\mu = 1
$$

we instead prove

$$
\Gamma(a)\Gamma(b) = \Gamma(a+b) \int_{0}^{1} \mu^{a-1} (1-\mu)^{b-1} \,d\mu
$$

That is

$$
\begin{align*}
\Gamma(a)\Gamma(b) &= \int _{0}^{\infty }x^{a-1}e^{-x}\,dx \int _{0}^{\infty }y^{b-1}e^{-y}\,dy \\
&= \int _{0}^{\infty } \int _{0}^{\infty } x^{a-1} y^{b-1}e^{-x-y} \,dy \,dx \\
\end{align*}
$$

By setting $y=t-x$, we obtain

$$
\int _{0}^{\infty } x^{a-1} \,dx \int _{x}^{\infty } (t-x)^{b-1}e^{-t} \,dt
$$

By reordering the integral, we obtain

$$
\int _{0}^{\infty } e^{-t} \,dt \int _{0}^{t}(t-x)^{b-1} x^{a-1} \,dx
$$

Setting $x = t \mu$, we obtain

$$
\begin{align*}
&\int _{0}^{\infty } e^{-t} \,dt \int _{0}^{1} (t-t\mu)^{b-1} (t \mu)^{a-1} t \,d\mu \\
&\quad = \int _{0}^{\infty } t^{a+b-1} e^{-t} \,dt \int _{0}^{1} \mu^{a-1} (1-\mu)^{b-1} \,d\mu \\
&\quad = \Gamma(a+b) \int _{0}^{1} \mu^{a-1} (1-\mu)^{b-1} \,d\mu
\end{align*}
$$

Mean, variance and mode is given by

$$
\begin{align*}
\operatorname{E}[\mu] &= \frac{a}{a+b} \\
\operatorname{var}[\mu] &= \frac{ab}{(a+b)^2(a+b+1)} \\
\operatorname{mode}[\mu] &= \frac{a-1}{a+b-2}
\end{align*}
$$

Note that, when $a, b<1$ the $\operatorname{mode}[\mu]$ becomes the anti-mode at which the the density gets the smallest value.

The derivation is shown below.

Using the normalization integral, we have

$$
\Gamma(a+1)\Gamma(b) = \Gamma(a+b+1) \int _{0}^{1} \mu^{a} (1-\mu)^{b-1} \,d\mu
$$

Therefore

$$
\begin{align*}
\operatorname{E}[\mu] &= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \int _{0}^{1} \mu^{a} (1-\mu)^{b-1} \\
&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \cdot \frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)} \\
&= \frac{a}{a+b}
\end{align*}
$$

Since

$$
\Gamma(a+2)\Gamma(b) = \Gamma(a+b+2) \int _{0}^{1} \mu^{a+1} (1-\mu)^{b-1} \,d\mu
$$

we have

$$
\begin{align*}
\operatorname{var}[\mu] &= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \int _{0}^{1} \mu^{a+1} (1-\mu)^{b-1} \\
&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \cdot \frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)} \\
&= \frac{ab}{(a+b)^2(a+b+1)}
\end{align*}
$$

By setting $\displaystyle \frac{\mathrm{d}}{\mathrm{d} \mu} \mathrm{Beta}(\mu\|a,b) = 0$
, the mode is obtained. 

The posterior distribution over $\mu$ is obtained by nomalizing the product of the beta prior and the binomial likelihood function, which takes the form

$$
p(\mu|m,l,a,b) = \mathrm{Constant} \cdot \mu^{m+a-1} (1-\mu)^{l+b-1}
$$

where $l=N-m$.

Using the same technique for normalization as before, we obtain

$$
p(\mu|m,l,a,b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)} \mu^{m+a-1} (1-\mu)^{l+b-1}
$$

which is also a beta distribution.

We see that the effect of new observations is added onto the prior distribution.
Furthermore, the current posterior distribution can also act as the prior for subsequent observations, which leads to a sequential approach of learning.

Sequential approaches are popular for

- real-time learning
- large data sets that are difficult to be loaded into memory

For the purpose of prediction, we want to evaluate the predictive distribution of $x$ given by

$$
\begin{align*}
p(x=1|D) &= \int _{0}^{1} p(x=1|\mu) p(\mu|D) \,d\mu \\
&= \int _{0}^{1} \mu p(\mu|D) \,d\mu \\
&= \operatorname{E}[\mu|D]
\end{align*}
$$

which is the mean of a beta distribution. Therefore

$$
p(x=1|D) = \frac{m+a}{m+a+l+b}
$$

If the data set is infinitely large so that $m,l \rightarrow \infty$, the distribution reduces to the maximum likellihood result $m/N$.
Indeed, the posterior mean always lies between the prior mean and the maximum likelihood estimetor. This can be proven by contradiction.

Assume not. If $\mu\_{\mathrm{post}} < \mu\_{\mathrm{prior}}$ and $\mu\_{\mathrm{post}} < \mu\_{\mathrm{ML}}$, then we have

$$
\begin{align*}
\frac{m+a}{m+a+l+b} < \frac{a}{a+b} &\Rightarrow mb < al \\
\frac{m+a}{m+a+l+b} < \frac{m}{m+l} &\Rightarrow al < mb \\
\end{align*}
$$

Contradiction. Similarly we can show contradiction for $\mu\_{\mathrm{post}} > \mu\_{\mathrm{prior}}$ and $\mu\_{\mathrm{post}} > \mu\_{\mathrm{ML}}$.

> Another method is to show that $\mu\_{\mathrm{post}} = \lambda \mu\_{\mathrm{prior}} + (1-\lambda) \mu\_{\mathrm{ML}}$ and $0 \le \lambda \le 1$.

Moreover, as the number of observations gorws, the distribution of $\mu$ becomes more sharply peaked, since the variance converges to 0.

> The derivation that the variance decreases when more data are observed??

## Multinomial Variables

A random variable $\mathbf{x}$ that takes on one of K possbile mutually exclusive states, can be denoted by

$$
\mathbf{x} = (x_1, \dots, x_K)^{\mathsf {T}}
$$

where only one element of $\mathbf{x}$ eqals 1 and the remaining elements is equal to 0.

If we denote the probability of $x\_k = 1$ by $\mu\_k$ , the distribution of $\mathbf{x}$ is given by

$$
p(\mathbf{x}|\boldsymbol{\mu}) = \prod_{k=1}^{K} \mu_k^{x_k}
$$

where $\boldsymbol{\mu} = (\mu\_1, \dots, \mu\_K)^{\mathsf {T}}$ and $\sum\_k \mu\_k = 1$.

It is easy to see the distribution is normalized by

$$
\sum_{\mathbf{x}} p(\mathbf{x}|\boldsymbol{\mu}) = \sum_{k=1}^K \mu_k = 1
$$

and the mean is given by

$$
\operatorname{E}[\mathbf{x}] = \sum_{\mathbf{x}} \mathbf{x} p(\mathbf{x}|\boldsymbol{\mu})
= \boldsymbol{\mu}
$$

Consider a data set $D$ comprised of $N$ independent observations $\mathbf{x}\_1, \dots, \mathbf{x}\_N$, the likelihood function is given

$$
p(D|\boldsymbol{\mu}) = \prod_{n=1}^{N} \prod_{k=1}^{K} \mu_k^{x_{nk}}
= \prod_{k=1}^{K} \mu_k^{\sum_n x_{nk}}
= \prod_{k=1}^{K} \mu_k^{m_k}
$$

where $m\_k = \sum\_n x\_{nk}$ represents the number of observations of $x\_k=1$. 
And we see that $\{m\_k\}$ are sufficient statistics for this distribution.

To maximize the likelihood function under the constraint $\sum\_k \mu\_k = 1$, we first take the logarithm, and then use a Lagrange multiplier $\lambda$ to maximize

$$
\sum_k m_k \ln \mu_k + \lambda (\sum_k \mu_k - 1)
$$

Setting the derivative to 0 with respect to $\mu\_k$, we obtain

$$
\begin{align*}
\frac{m_k}{\mu_k} + \lambda &= 0 \\
\sum_k m_k + \lambda \sum_k \mu_k &= 0 \\
\end{align*}
$$

from which we have $\lambda = -N$.

Thus the estimator for $\mu\_k$ is

$$
\mu_k^{ML} = \frac{m_k}{N}
$$

which is the fraction of $x\_k=1$ out of $N$ observations.

Normalizing the likelihood function, the joint distribution of quntities $m\_1, \dots, m\_K$ is given by

$$
\mathrm{Mult} (m_1, \dots, m_K | \boldsymbol{\mu}, N) 
= \binom{N}{m_1 \dots m_K} \prod_{k=1}^{K} \mu_k^{m_k}
$$

in which $\sum\_k m\_k = N$. This is known as the multinomial distribution. The coefficient

$$
\binom{N}{m_1 \dots m_K} = \frac{N!}{m_1! \dots m_K!}
$$

can be interpreted as the number of ways to partition N instinct objects into K instinct groups of size $m\_1, \dots, m\_K$, and the object order inside each group is irrelevent.

### Dirichlet distribution

The conjugate prior of multinomial distribution can take the form

$$
p(\boldsymbol{\mu} | \boldsymbol{\alpha}) \propto \prod_{k=1}^{K} \mu_k^{\alpha_k - 1}
$$

where $\boldsymbol{\alpha} = (\alpha\_1, \dots, \alpha\_K)^{\mathsf {T}}$ and $\sum\_k \mu\_k = 1$.

After normalization, the distribution given by

$$
\mathrm{Dir} (\boldsymbol{\mu} | \boldsymbol{\alpha}) 
= \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_K)} \prod_{k=1}^{K} \mu_k^{\alpha_k - 1}
$$

is called the Dirichlet distribution. Here $\alpha\_0 = \sum\_{k=1}^K \alpha\_k$.

To show that the distributio is normalized, we prove by induction

$$
\int \prod_{k=1}^{K} \mu_k^{\alpha_k - 1} \,d\boldsymbol{\mu} 
= \frac{\Gamma(\alpha_1) \cdots \Gamma(\alpha_K)}{\Gamma(\alpha_1 + \cdots + \alpha_K)}
$$

If $K=2$, the Dirichlet distribution reduces to the binomial distribution. The identity holds.

Assume the identity holds for $K=M$, we show that for $K=M+1$

$$
\begin{align*}
&\int \prod_{k=1}^{M+1} \mu_k^{\alpha_{k} - 1} \,d\boldsymbol{\mu} \\
=& \int _0^1 \!\cdots\! \int_{0}^{1 - \sum_{k=1}^{M-1} \mu_k} \prod_{k=1}^{M} \mu_k^{\alpha_{k} - 1} \cdot (1 - \sum_{k=1}^M \mu_k)^{\alpha_{M+1} - 1} \,d\mu_M \cdots d\mu_1 \\
=& \int _0^1 \!\cdots\! \int _{0}^{1 - \sum_{k=1}^{M-2} \mu_k} \,d\mu_{M-1} \cdots d\mu_1 \\
&\cdot \int_{0}^{1 - \sum_{k=1}^{M-1} \mu_k} \prod_{k=1}^{M} \mu_k^{\alpha_{k} - 1} \cdot (1 - \sum_{k=1}^M \mu_k)^{\alpha_{M+1} - 1} \,d\mu_M \cdots d\mu_1 \\
\end{align*}
$$

By substituting with $\mu\_M = t u$ and $u = 1 - \sum\_{k=1}^{M-1} \mu\_k$, we have

$$
\begin{align*}
& \int _0^1 \!\cdots\! \int _{0}^{1 - \sum_{k=1}^{M-2} \mu_k} \,d\mu_{M-1} \cdots d\mu_1 \\
&\cdot \int_{0}^{1} \prod_{k=1}^{M-1} \mu_k^ {\alpha_{k} - 1} \cdot ( tu ) ^{\alpha_M -1} \cdot ( u - tu ) ^{\alpha_{M+1} - 1} u \,dt                                                                                        \\
= & \int_0^1 \!\cdots\! \int _{0}^{1 - \sum_{k=1}^{M-2} \mu_k} \prod_{k=1}^{M-1} \mu_k^ {\alpha_{k} - 1} \cdot (1 - \sum_{k=1}^{M-1} \mu_k)^{\alpha_{M+1} + \alpha_{M} - 1} \,d\mu_{M-1} \cdots d\mu_1 \\
&\cdot \int_{0}^{1} t^{\alpha{M} - 1} (1-t)^{\alpha_{M+1} - 1} \,dt \\
= & \frac{\Gamma(\alpha_1) \cdots \Gamma(\alpha_{M-1})\Gamma(\alpha_{M}+\alpha_{M+1})}{\Gamma(\alpha_1 + \cdots + \alpha_{M+1})} \cdot \frac{\Gamma(\alpha_M) \cdots \Gamma(\alpha_{M+1})}{\Gamma(\alpha_{M}+\alpha_{M+1})}                                               \\
= & \frac{\Gamma(\alpha_1) \cdots \Gamma(\alpha_{M-1})\Gamma(\alpha_{M})\Gamma(\alpha_{M+1})}{\Gamma(\alpha_1 + \cdots + \alpha_{M+1})}
\end{align*}
$$

Whereby we show that the identity holds.

Since the posterior distribution for $\boldsymbol{\mu}$ satisfies

$$
p (\boldsymbol{\mu} | D, \boldsymbol{\alpha})
\propto p (D | \boldsymbol{\mu}) p(\boldsymbol{\mu} | \boldsymbol{\alpha})
\propto \prod_{k=1}^{K} \mu_k^{\alpha_k + m_k- 1}
$$

we obtain

$$
\begin{align*}
p (\boldsymbol{\mu} | D, \boldsymbol{\alpha}) 
&= \mathrm{Dir} (\boldsymbol{\mu} | \boldsymbol{\alpha} + \boldsymbol{m}) \\
&= \frac{\Gamma(\alpha_0 + N)}{\Gamma(\alpha_1 + m_1) \cdots \Gamma(\alpha_K + m_K)} \prod_{k=1}^{K} \mu_k^{\alpha_k + m_k- 1}
\end{align*}
$$

Here, $\boldsymbol{m} = (m\_1, \dots, m\_K)^{\mathsf {T}}$
