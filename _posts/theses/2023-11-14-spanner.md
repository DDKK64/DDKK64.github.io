---
layout: posts
title: '6.824 - Spanner'
---
# Spanner

Paper: Spanner: Google's Globally-Distributed Database (2012)

- Wide-area transactions: sharded data across many sets of Paxos state machines in datacenters all over the world.
  - R/W: 2PC + 2PL + Paxos
  - R/O: snapshot isolation
- Linearizability

## Deployment

Universe: a spanner deployment. E.g. production/development

Zone: deployment unit, phisical isolated???

## Data model

Spanner provides a semi-relational view of the database.

Applications create one or more databases in a universe. A database can contain arbitrary number of tables.

Tables are declared using extended SQL:

```SQL
CREATE TABLE Users {
    uid INT64 NOT NULL, email STRING
} PRIMARY KEY (uid), DIRECTORY;

CREATE TABLE Albums {
    uid INT64 NOT NULL, aid INT64 NOT NULL,
    name STRING
} PRIMARY KEY (uid, aid),
    INTERLEAVE IN PARENT Users ON DELETE CASCADE;
```

A table is a k-v map from the primary column to other non-primary columns.

Tables are organized hierarchically with the key word `INTERLEAVE IN`.
Each row with key $K$, along with the rows in descendent tables whose key starts with k,
is grouped together to form a directory.

![](/assets/images/theses/spanner-fig4.png)

A directory is simply a shard of data. It is the smallest data placement unit.
The placement includes two aspects:

- Replication number
- Geographic placement of the shard. E.g. North America.

## Archietecture

![](/assets/images/theses/spanner-stack.jpeg)

Tablet: a multi-version kv store: `(key:string, timestamp:int64) &rarr; value: string`

A tablet can contain multiple directories. Tablet's states (data file, WAL) are stored in Colossus (a distributed file system).

Each spanner server is responsible for multiple tablets.
Each spanner server maintains a lock table to support 2-phase locking and a transaction manager to support distributed transactions.

A Paxos state machine is installed on top of each tablet.
The set of replicas(tablets) forms a Paxos group. The leader of the group is called participant leader; the other replicas are participant slaves.

Within a Paxos group, writes initiate at the leader; reads initiate at any sufficiently up-to-date replica.

For cross-Paxos-group transactions, one of the participant groups is chosen as the coordinator. The coordinator perms the 2PC.
The leader of the coorinator is called the coordinator leader and other replicas of the coordinator are called coordinator slaves.

## True Time

The uncertainty (asyncrony) in clocks is exposed to help to achieve linearizability.

API:

| Method       | Returns                                                                      |
| ------------ | ---------------------------------------------------------------------------- |
| TT.now()     | TTinterval [earliest, latest] which contains the absolute time of invocation |
| TT.after(t)  | true if t is definitely passed                                               |
| TT.before(t) | true if t has definitely not arrived                                         |

Time reference devices: GPS and atomic clocks. Availability achieved by different failure modes.

- A set of time master machines equipped with either GPS receiver or atomic clock. Maters checks against each other and its local clock.
- A timeslave daemon on each machine polls multiple masters for the time.

Time error bound = master uncretainty + worst-case local clock drift + communication delay

## Concurrency control

![](/assets/images/theses/spanner-tab2.png)

2PL is used for single-site transaction. Dead lock are avoided with wound-wait strategy.

### Timestamp management

Disjoiness invariant: within each Paxos group, each leader's lease interval never overlaps with any other.

Timestamp of a transaction is chosen at its lock point.

Monotomicity invariant: within each Paxos group, the timestamp assigned to Paxos writes are monotonically increasing, even after the leader is changed.

### Read-write transactions

1. Client acquires read locks and reads the most recent data from appropriete leader replicas. Writes are buffered only at the client before commit.
3. The client chooses a coordinator group, sends commit requests to each group's leader along with the identity of the coordinator and buffered writes.
4. The coordinator begins the 2PC:
   1. Each non-coordinator participant acquires write locks and chooses a prepare timestamp conforming to the monotonicity.
   2. The leader aquires write locks, waits for all other participants prepare messages.
   3. The coorinator leader choose a commit timestamp $s$ and then logs the commit.
   4. After commit-wait ($TT.after(s)$ becomes true), the coordinator sends the commit timestamp to participants.
   5. All participants commit their parts with timestamp $s$ and release locks.

The choice of the client driven 2PC is to reduce the wide-area traffics.

Define the event $e^{commit}$ as the last participants commit the transaction.

The commit timestamp $s$ must satisfy:

1. Greater than or equal to the prepare timestamps. This ensures that $s$ is greater than the absolute commit time (when the last participants committed).
2. Greater than $TT.now().latest$ when the coordinator leader received the commit request.
This ensures that $s$ is not ealier than the time when the client starts the transaction.
3. Greater than any timestamp the leader assigned to the previous transactions. This preserves  monotonicity.
4. Within its lease interval. This along with the disjoitness property preserves monotonicity across leaders.

Commit wait ensures that the changes is not applied (not visible to clients) until $s$.

These properties together ensure the external consistency (linearizability): If transaction $T\_1$ commits before $T\_2$ starts, then the timestamp satisfies $s\_1 < s\_2$ and $T\_2$ sees the changes of $T\_1$.

### Snapshot reads

Every replica keeps track of a safe time $t\_{safe} = \min (t\_{safe}^{Paxos}, t\_{safe}^{TM})$. A replica can serve any read at a timestamp $t \le t\_{safe}$.

$t\_{safe}^{Paxos}$, associated with each Paxos state machine, takes the highest timestamp of applied Paxos writes.

$t\_{safe}^{TM}$, associate with each transaction manager, takes $\infty$ if there is no prepared yet uncommited transaction.
Otherwise, additional consideration must be taken.

Let $s\_{i, g}^{prepare}$ denote the prepare timestamp assigned by the participant leader of group $g$ for transaction $T\_i$.
Let $s\_i$ denote the timestamp of transaction $T\_i$.
If $t > s\_i$, the replica must return a result reflecting the writes of $T\_i$.
However, the replica does not known $s\_i$ yet. The only knowledge it has about $s\_i$, is a lower bound given by $s\_{i, g}^{prepare}$.
Therefore the safe time for group g is determined by $t\_{safe}^{TM} = \min\_i (s\_{i, g}^{prepare}) - 1$.

### Read-only transactions

A read-only transaction is simply a snapshot reads at a system selected timestamp.

The simplest choice is $s\_{read} = TT.now().latest()$. However, this may block the read if $t\_{safe}$ has not advanced sufficiently.

TODO: Better choice for timetstamp

## Performance

