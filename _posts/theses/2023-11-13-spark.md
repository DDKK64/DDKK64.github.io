---
layout: posts
title: '6.824 - Spark'
---
# Spark

Paper: Resilient Distributed Datasets: A Fault-Tolerent Abstraction for In-Memory Cluter Computing

Successor of hadoop
- Support wider range of applications
- In-memory computation (stores intermidiate data)

## RDD

RDD: read-only partitioned collection of records.

- Created through transformation operations (e.g. map, filter, join) on data in storage or other RDDs.
- The lineage (how the RDD is derived) is logged instead of RDD itself.
- Computation of RDDs is defered until an action is invoked (e.g. count, collect, save).

Persistence strategy: memory or stable storage

Support partitioning records across machines based on a key.

## Spark API

Lauguage-integrated API based on Scala.

A driver program connects to a cluster of long-lived worker processes that compute and store RDDs.

![](/assets/images/theses/spark-fig2.png)

Transformation and actions:

![spark API](/assets/images/theses/spark-api.png)

## RDD representation

Five properties about each RDD:

- partitions
- partition scheme
- dependencies on parents
- operations which derive the RDD from its parent
- data placement

E.g. HDFS is a supported partition scheme. Each block is a partition of the RDD. Location is where the partition is.

Two kinds of dependencies:

- narrow dependencies: each child partition effectively depend on one parent partition
- wide(shuffle) dependencies: each child partition effectively depends on multiple parent partitions

![](/assets/images/theses/spark-fig4.png)

Narrow dependencies have advantages that

- Execution is pipelined
- Recovery after node failure is efficient, since only one parent partition need to be recomputed.

## Example: Log Mining

![](/assets/images/theses/spark-fig1.png)

The first line creates an RDD with each partition backed by a HDFS block.

Call *persist* to keep RDDs in memory or stable storage for future reuse (for fault-tolerance or performance reason).

No actual computation is performed by now.

## Example: PageRank

PageRank:

1. In each page, the number of outbound links are counted. Multiple ocurrences of the same link only count once. Links to itself are ignored.
2. At the beginning, each page is initialized with the same probability, so that $P(u)^{(0)} = 1/N$
3. Let $L(u)$ be the number of distinct outbound links of page $u$ and $T\_u$ be the set of pages linking to $u$. Each page is re-evaluated with

$$
P(u)^{(t+1)} = \sum_{v \in T_u} \frac{P(v)^{(t)}}{L(v)}
$$

4. Repeat step 2 until convergence.

Page rank with dumping factor:

$$
P(u)^{(t+1)} = \frac{\alpha}{N} + (1-\alpha) \sum_{v \in T_u} \frac{P(v)^{(t)}}{L(v)}
$$

![](/assets/images/theses/spark-fig3.png)

Data flow:

1. Build an RDD of (URL, outlinks) from page files.
2. Initialize rank RDD (URL, rank)
3. Build an RDD of (targetURL, given weight)
4. Re-evaluate the rank
5. Repeat from step 3.

## Implementation

### job scheduling

- Respect data locality
- For wide dependencies, persist intermediate result.
- If task fails, re-runs the stage of missing partition. (deterministic function -> identical partition)

### Interpreter integration

Let uesr interactively query big datasets through the interpreter.

### Memory management

3 options for persistent RDDs:

- in-memory deserialized Java objects
- in-memory serialized objects
- on-disk serialized data

LRU is used to evict partitions on need.

Checkpointing: persist some RDDs to stable storage for quick recovery. Useful for RDDs with long lineage.
