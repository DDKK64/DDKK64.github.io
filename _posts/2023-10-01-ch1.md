---
layout: posts
title: "PRML - Introduction"
---
# Ch1 Introduction

Training (learning) phase: determine the function of the input

Generalization: the ability of the model on the new data

Pre-processing (feature extractoin): process input to simplify the model

Supervised learning: training data consists of the input and corresponding target variables

- Classification: assign each input vector to a specific class
- Regression: output consists of one or more continuous variables

Unsupervided learning: traininng data consists of only the input

- Clustering: grouping data points by some similarity
- Density estimation
- Visualization

Reinforcement learning: find optimal actions to maximize a reward.

## Example: Polynomial curve fitting

## Decision theory

Consider a input $\mathbf{x}$ and a corresponding target $\mathbf{t}$.

One of our goal is to determine the joint probability $p(\mathbf{x}, \mathbf{t})$ from a set of training data.
This is the process of inference.

After that, we may also want to make decisions based on our knownledge on the distribution. This is the decision step.

Decision theory helps us to make optimal decision based on the probability.




## Information Theory

Consider a discrete random variable x. The intuition about the information that x contains is that: 

- The amount of information can be viewed as 'degree of surprise' on learning a value of x. 
An unlikely event contains more information. So h(x) should be a monotonically decreasing function of p(x).
- If two variables x and y are independent, the amount of information gained by observing both of them is the sum of that by observing them seperately. That is, $h(x,y)=h(x)+h(y)$

Therefore the information that event x contains can be defined as

$$
h(x) = - \log_2 p(x)
$$

where we see that $h(x)$ is non-negtive and monotonically decreasing with $p(x)$. 

The choice of the basis for logarithm is arbitrary. In this case, the unit of $h(x)$ are bit.

Suppose we wish to communicate the outcome of the random variable. The average (expected) amount of information to transmit is measured by

$$
\operatorname{H}[x] = -\sum_x p(x) \log_2 p(x)
$$

which is called the entropy of x.

For x such that $p(x)=0$, since $\lim\_{p \rightarrow 0} p \log\_2 p = 0$, we shall take $p(x) \log\_2 p(x) = 0$.

However, the entropy cannot be maximized if there are no other constraints except that the distribution is normalized. 

To see this, consider a uniform distribution $U(a, b)$, whose entropy is given by $H = \ln (b-a)$. Without any constraint, the entropy grows to infinity as $b-a$ increases.


## Gamma function

Motivation: find a smooth curve that connects the points $(x, y)$ given by $y=(x-1)!$ at positive integer values for x.

Definition:

$$
\Gamma (z)=\int _{0}^{\infty }t^{z-1}e^{-t}\,dt
$$

converges absolutely if $z > 0$.

### Properties

Property 1

$$
\begin{align*}
\Gamma (1)&=\int _{0}^{\infty }t^{1-1}e^{-t}\,dt
\\
&=\int _{0}^{\infty }e^{-t}\,dt
\\
&=1
\end{align*}
$$

Property 2

$$
\Gamma (z+1)=z\Gamma (z)
$$

Proof:

$$
\begin{align*}
\Gamma (z+1)&=\int _{0}^{\infty }t^{z}e^{-t}\,dt\\
&=\Bigl [ -t^{z}e^{-t} \Bigr ] _{0}^{\infty }+\int _{0}^{\infty }zt^{z-1}e^{-t}\,dt\\
&=z\int _{0}^{\infty }t^{z-1}e^{-t}\,dt
\end{align*}
$$

Property 3

For any postive interger n, $\Gamma (n) = (n-1)!$.