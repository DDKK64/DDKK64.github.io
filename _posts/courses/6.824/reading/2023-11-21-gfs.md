---
layout: posts
title: '6.824 - The Google File System'
---
# The Google File System

Paper: The Google File System

A successful storage system:

- Fault tolerance
- High throughput
- Large capacity

Novelties:

- Single master
- Allow inconsistency

## Assumptions

- Commodity hardware that fails often
- Many large files
- Large sequential read/append and small random read/write
- Concurrent append operations with efficient implementation and well-defined semantics
- Bandwidth is more importent than latency

## Interface

Files are organized hierarchically and identified by pathnames like in POSIX fs.

Unix-like file I/O function: create, delete, open, close, read, write

Snapshot: creates a copy of a file or directory tree at low cost.

Record append: append user-specified data *atomically at least once* at an offset of GFS's choice. Offset is returned to the client.

## Architecture

![arch](/assets/images/courses/6.824/reading/gfs-fig1.png)

Components:

- single master
- multiple chunkservers
- multiple clients

No file data are cached at neither client nor chunkservers &rarr;  eliminating cache coherence issues.

File data never flow thourgh the master. Clients read/write file data directly from/to chunkservers.

Client caches metadata to reduce the traffic from/to the master.

The master communicate with chunkservers by periodic *HeartBeat* messages which carry instructions and chunkserver states.

Fixed-size chunks (64MB):

- Identified by an immutable and globally unique chunk handle assigned by the master.
- Each chunk are optionally replicated across machines and racks.

Larger chunk size:

- Pros:
  + Less client reqeusts to master because of less chunks in a file
  + Less metadata for storing chunk locations
- Cons:
  - Fragments caused by small files
  - Hot spots: clients are more likely to read from the same chunk

Alleviate hot spots:

- More replicas to balance the load
- P2P between clients


## Metadata

- Namespace: a look-up table from pathname to file/chunk metadata, more like a kv store. No actual 'directory' exists.
- File-to-chunk mapping: filename to chunk handle list
- Chunk locations: chunk servers that holds chunks

Namespace and file-to-chunk mapping are presistent and replicated with operation log.

Chunk locations are delivered in hearbeat messages.

Operation log:

- Critical metadata changes: file creation/deletion e.t.c
- Serilize concurrent operations

Log records are replicated before responding to client.

The master builds checkpoint from the log to reduce its size:
  - Asynchronous checkpointing without delay client requests
  - Replicated after completion
  - Detect invalid checkpoint (due to failure)

## Read operation

1. Client sends filename, offset to the master
2. The master replies: chunkhandle, chunk locations, version number
3. Client caches metadata
4. Client reads from the cloest chunkserver
5. Chunk server checks version number with. Sends data if up-to-date.

## Mutation

A chunk lease is granted to one of replicas by the master, which becomes the primary. A lease has a timeout of 60 seconds and a primary can request for extension from the master.

![](/assets/images/courses/6.824/reading/gfs-mutation.jpeg)

1. Client asks the master for chunk locations and the identity of the primary.
2. The master replies chunk list and version number. If no primary replica for a chunk, the master increases the chunk version number, grants a lease to a replica, and informs replicas to update their version number.
3. Data is pushed linearly from the client along a chain of chunkserverse. Each endpoint sends data to its closest endpoint.
4. The client issue write requests to the primary.
5. The primary serializes concurrent mutations, applies them, and sends the arrangement for mutations (order and offsets) to the secondaries.
6. The secondary replies.
7. The primary replies an error if any secondary fails to complete. Client retries the write after the failure.

Advantages of pipelining:

- lower latency
- high throughtput

'distance' is estimated from IP address

## Consistency

Namespace mutations are atomic and consistent.

![](/assets/images/courses/6.824/reading/gfs-consistency.jpeg)

A file region is *consistent* if all clients see the same data.

A file region is *defined* if it is consistent and no concurrent writes from different clients. For instance, the written data may be interleaved with each other.

Mutations are applied to all replicas in the same order.

Stale replicas are detected by chunk version numbers.

Corruptions are detected by checksumming.

Remedies of weak consistensy for applications:

- Checkpoint: application periodically checkpoints. Readers only read to the last checkpoint.
- Duplicate records: id for each record
- Undefined region by checksumming

## Snapshot

1. Revokes leases on chunks to snapshot.
2. Log the operation

Copy-on-write takes place for snapshotted chunks. Data is copied locally.

## Replica placement

Cared factors:

- Availability
- bandwidth utilization

Spread chunk replicas across machines and racks.

Initail empty chunk placement:

- below-average disk space usage
- lower the priority of servers with recent creations
- spread

Re-replication and rebalance.

## Garbage collection

Delete opration renames a file to a hidden name. Before expiration, the file can be renamed back as recovery.

After expiration, the file metadata is remove at the master. From heartbeat messages, the chunkserver will identify removed chunks.

The master does not wait the file to actually delete.

## Stale replication detection

The master increases chunk version number whenever it grants a lease to a chunk.
Then it informs each up-to-date replica.

Chunk version number is persisted at both the master and chunkerservers.

Stale replicas are garbage collected and will not be in the metadata to the client.
However, with the cached metadata the there's a bounded time window when the client can possibly access the stale replica.

Chunk version numbers are replied to the client in metadata.

## Master replication

The master is addressed by canonical name to hide ip changes during failover.

Shadow masters follow the primary master by reading and applying its logs, which may lag slightly. It provieds read-only access to clients.

## Data integrity

Include a 32-bit checksum for each 64KB blocks.

Chunkservers do their own integirty checks.

Corruption is repoted to the master.

## Preformance

Observations on workloads: a few sequential reads transfers most data.
