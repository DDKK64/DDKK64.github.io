---
layout: posts
title: '6.824 - Memcache'
---
# Memcache

Paper: Scaling Memcache at Facebook (2013)

`memcached`: a running instance
`memcache`: a distributed system

General purpose cache:

- Reads are far more than writing
- Data are fetched from various sources. E.g. cache web server queries

## Archietecture

![](/assets/images/courses/6.824/reading/mc-fig2.png)

Items are distributed using consistent hashing.

All-to-all communication: all web servers communication with every memcached server.

- UDP for `get` request
- TCP for `set` and `delete` through a mcrouter on each server

In-cast congestion:

- Sliding window controls limits the number of outstanding requests.
- Multiple front-end clusters

### Regions

Regions are placed across geographic locations for HA.

A region contains one storage cluster (databases) and multiple front-end clusters (a group of web servers and memcache servers).

Based on MySQL replication: all writes go to primary. Backups are read-only

Sequeal:

### Failure

Gutter pool: when client receives no response from mc server, it send requests to the gutter pool and update it.

Massive failures: redirect requests to other clusters.

## Consistency

Eventual consistency

- Ordered writes (backed by databases)
- Reads may lag beghind 
- Client always sees its writes

### Lease

Q: What is the "stale set" problem in 3.2.1, and how do leases solve it?

A: Here's an example of the "stale set" problem that could occur if
there were no leases:

1. Client C1 asks memcache for k; memcache says k doesn't exist.
2. C1 asks MySQL for k, MySQL replies with value 1.
   C1 is slow at this point for some reason...
3. Someone updates k's value in MySQL to 2.
4. MySQL/mcsqueal/mcrouter send an invalidate for k to memcache,
   though memcache is not caching k, so there's nothing to invalidate.
5. C2 asks memcache for k; memcache says k doesn't exist.
6. C2 asks MySQL for k, mySQL replies with value 2.
7. C2 installs k=2 in memcache.
8. C1 installs k=1 in memcache.

Now memcache has a stale version of k, and it may never be updated.

The paper's leases fix the example:

1. Client C1 asks memcache for k; memcache says k doesn't exist,
   returns lease L1 to C1, and remembers the lease.
2. C1 asks MySQL for k, MySQL replies with value 1.
   C1 is slow at this point for some reason...
3. Someone updates k's value in MySQL to 2.
4. MySQL/mcsqueal/mcrouter send an invalidate for k to memcache,
   though memcache is not caching k, so there's nothing to invalidate.
   But memcache does invalidate C1's lease L1 (deletes L1 from its set
   of valid leases).
5. C2 asks memcache for k; memcache says k doesn't exist,
   and returns lease L2 to C2 (since there was no current lease for k).
6. C2 asks MySQL for k, mySQL replies with value 2.
7. C2 installs k=2 in memcache, supplying valid lease L2.
8. C1 installs k=1 in memcache, supplying invalid lease L1,
   so memcache ignores C1.

Q: What is the "thundering herd" problem in 3.2.1, and how do leases
solve it?

A: The thundering herd problem:

* key k is very popular -- lots of clients read it.
* ordinarily clients read k from memcache, which is fast.
* but suppose someone writes k, causing it to be invalidated in memcache.
* for a while, every client that tries to read k will miss in memcache.
* they will all ask MySQL for k.
* MySQL may be overloaded with too many simultaneous requests.

The paper's leases solve this problem by allowing only the first client that misses to ask MySQL for the latest data.

### Cold cluster warmup

during cold cluster warm-up

remember: on miss, clients try get() in warm cluster, copy to cold cluster

k starts with value v1

1. C1 updates k to v2 in DB
2. C1 delete(k) -- in cold cluster
3. C2 get(k), miss -- in cold cluster
4. C2 v1 = get(k) from warm cluster, hits
5. C2 set(k, v1) into cold cluster

now mc has stale v1, but delete() has already happened
   will stay stale indefinitely, until key is next written

solved with two-second hold-off, just used on cold clusters
   after C1 delete(), cold mc ignores set()s for two seconds
   by then, delete() will (probably) propagate via DB to warm cluster

### Across-region consistency

k starts with value v1
1. C1 is in a secondary region
1. C1 updates k=v2 in primary DB
1. C1 delete(k) -- local region
1. C1 get(k), miss
1. C1 read local DB  -- sees v1, not v2!

later, v2 arrives from primary DB

solved by "remote mark"
   C1 delete() marks key "remote"
   get() miss yields "remote"
   tells C1 to read from *primary* region
   "remote" cleared when new data arrives from primary region
