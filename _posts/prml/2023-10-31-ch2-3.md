---
layout: posts
title: 'PRML Ch2 - Probability distributions - Part III'
---
- [The exponential family](#the-exponential-family)
  - [Maximum likelihood and sufficient statistics](#maximum-likelihood-and-sufficient-statistics)
  - [Conjugate priors](#conjugate-priors)
  - [Noninformative priors](#noninformative-priors)
- [Nonparametric methods](#nonparametric-methods)
  - [Histogram methods](#histogram-methods)
  - [Kernel density estimator](#kernel-density-estimator)
  - [Nearest-neighbor methods](#nearest-neighbor-methods)

## The exponential family

The exponential family of ditributions over $\mathbf{x}$ with parameter $\boldsymbol{\eta}$ takes the form

$$
p(\mathbf{x}|\boldsymbol{\eta}) = h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\}
\tag{2.194}
$$

where $\mathbf{x}$ may be a scalar or vector, discrete or continuous. $\boldsymbol{\eta}$ is called the **natural parameter**. 
$g(\boldsymbol{\eta})$ ensures the distribution is nomalized so that

$$
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \,d\mathbf{x} = 1
$$

Consider the Bernoulli distribution, by writing in the form

$$
\begin{align*}
p(x|\mu) = \mathrm{Bern}(x|\mu) &= \mu^x (1-\mu)^{1-x} \\
&= (1-\mu) \exp \left\{ \ln \left( \frac{\mu}{1-\mu} \right) x \right\}
\end{align*}
$$

we see

$$
\eta = \ln \left( \frac{\mu}{1-\mu} \right)
$$

where we solve for $\mu$

$$
\mu = \sigma(\eta) = \frac{1}{1 + e^{-\eta}}
$$

where $\sigma$ is the logistic sigmoid function. Using $1- \sigma(\eta) = \sigma(- \eta)$, we have

$$
1 - \mu = \sigma(- \eta)
$$

By comparison we have

$$
\begin{align*}
g(\eta) &= \sigma(-\eta) \\
h(x) &= 1 \\
u(x) &= x \\
\end{align*}
$$

Consider a multinomial distribution of a single observation ($N=1$), which can be written as

$$
p(\mathbf{x}|\boldsymbol{\mu}) = \exp \left\{ \sum_{k=1}^K x_k \ln \mu_k \right\}
$$

where $\mathbf{x} = (x\_1, \dots, x\_K)^{\mathsf {T}}$ and $\boldsymbol{\mu} = (\mu\_1, \dots, \mu\_K)^{\mathsf {T}}$.

Defining $\boldsymbol{\eta} = (\eta\_1, \dots, \eta\_K)^{\mathsf {T}}$, we have $\eta\_k = \ln \mu\_k$.

By comparison

$$
\begin{align*}
g(\boldsymbol{\eta}) &= 1 \\
h(\mathbf{x}) &= 1 \\
\mathbf{u} (\mathbf{x}) &= \mathbf{x} \\
\end{align*}
$$

It can also be written in an alternative form. With constraint $\sum\_k \mu\_k = 1$, we express the $\mu\_K$ using the remaining $K-1$ parameters so that

$$
\mu_K = 1 - \sum_{k=1}^{K-1} \mu_k
$$

and the constraints is now given by

$$
0 \le \mu_k \le 1 , \quad \sum_{k=1}^{K-1} \mu_k \le 1
$$

Using similar transformation on $x\_K$, the distribution can be written in the form

$$
\begin{align*}
p(\mathbf{x}|\boldsymbol{\mu})
&= \exp \left\{ \sum_{k=1}^{K-1} x_k \ln \mu_k + \left( 1 - \sum_{k=1}^{K-1} x_k \right) \ln \left( 1- \sum_{k=1}^{K-1} \mu_k \right) \right\} \\
&= \exp \left\{ \sum_{k=1}^{K-1} x_k \ln \left( \frac{\mu_k}{1 - \sum_{j=1}^{K-1} \mu_j} \right) + \ln \left( 1- \sum_{k=1}^{K-1} \mu_k \right) \right\} \\
\end{align*}
$$

from which we identify

$$
\eta_k = \ln \left( \frac{\mu_k}{1 - \sum_{j=1}^{K-1} \mu_j} \right)
$$

Taking exponnetial and rearranging terms we have

$$
\exp(\eta_k) (1 - \sum_{j=1}^{K-1} \mu_j) = \mu_k
$$

Summing both sides over $k$

$$
\begin{align*}
\sum_{k=1}^{K-1} \exp(\eta_k) (1 - \sum_{j=1}^{K-1} \mu_j) &= \sum_{k=1}^{K-1} \mu_k \\
1- \sum_{k=1}^{K-1} \mu_k &= \frac{1}{1 + \sum_{k=1}^{K-1} \exp(\eta_k)}
\end{align*}
$$

Back-substituting

$$
\mu_k = \frac{\exp (\eta_k)}{1 + \sum_{j=1}^{K-1} \exp(\eta_j)}
$$

which is called the **softmax function** or the **normalized exponential**.

The multinomial distribution then takes the form

$$
p(\mathbf{x}|\boldsymbol{\mu})
= \left( 1 + \sum_{k=1}^{K-1} \exp (\eta_k) \right)^{-1} \exp \left( \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right)
$$

where we defined

$$
\begin{align*}
\boldsymbol{\eta} &= (\eta_1, \dots, \eta_{K-1})^{\mathsf {T}} \\
\mathbf{u}(\mathbf{x}) &= (x_1, \dots, x_{K-1})^{\mathsf {T}} \\
\end{align*}
$$

and we identify

$$
\begin{align*}
g(\boldsymbol{\eta}) &= \left( 1 + \sum_{k=1}^{K-1} \exp (\eta_k) \right)^{-1} \\
h(\mathbf{x}) &= 1 \\
\end{align*}
$$

Now consider the univariate Gaussian distribution given by

$$
\begin{align*}
p(x|\mu, \sigma^2)
&= \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left\{ - \frac{1}{2 \sigma^2} (x - \mu)^2 \right\} \\
&= \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left\{ - \frac{1}{2 \sigma^2} x^2 + \frac{\mu}{\sigma^2} x - \frac{1}{2 \sigma^2} \mu^2 \right\} \\
\end{align*}
$$

Defining

$$
\begin{align*}
\boldsymbol{\eta} &= (\eta_1, \eta_2)^{\mathsf{T}} = \left( \frac{\mu}{\sigma^2} , -\frac{1}{2 \sigma^2} \right)^{\mathsf {T}} \\
\mathbf{u} (x) &= (x, x^2)^{\mathsf {T}} \\
\end{align*}
$$

Solving for $\mu$ and $\sigma$ we have

$$
\begin{align*}
\sigma^2 &= - \frac{1}{2 \eta_2} \\
\mu &= - \frac{\eta_1}{2 \eta_2} \\
\end{align*}
$$

Remaining terms are given

$$
\begin{align*}
h(x) &= (2 \pi)^{1/2} \\
g(\boldsymbol{\eta}) &= (-2 \eta_2)^{1/2} \exp \left( \frac{\eta_1^2}{4 \eta_2} \right) \\
\end{align*}
$$

**Excercise 2.57**

For a D-dimensional multivariate Gaussian distribution, we write

$$
p(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})
= \frac{1}{(2 \pi)^{D/2}} |\boldsymbol{\Lambda}|^{1/2} \exp \left\{ - \frac{1}{2} \mathbf{x}^{\mathsf{T}} \boldsymbol{\Lambda} \mathbf{x} + \boldsymbol{\mu}^{\mathsf{T}} \boldsymbol{\Lambda} \mathbf{x} - \frac{1}{2} \boldsymbol{\mu}^{\mathsf{T}} \boldsymbol{\Lambda} \boldsymbol{\mu} \right\}
$$

Define vectors $\boldsymbol{x}$ and $\boldsymbol{\lambda}$ with dimensionality $D^2$, such that

$$
\begin{align*}
\boldsymbol{x}_{(i-1)*D+j} &= x_i x_j \\
\boldsymbol{\lambda}_{(i-1)*D+j} &= \Lambda_{ij} \\
\end{align*}
$$

in which $i,j = 1, \dots, D$.

Then we can define

$$
\begin{align*}
\mathbf{u} (\mathbf{x}) &= \begin{pmatrix}
\mathbf{x} \\
\boldsymbol{x} \\
\end{pmatrix} \\
\boldsymbol{\eta} &= \begin{pmatrix}
\boldsymbol{\eta}_1 \\ \boldsymbol{\eta}_2 \\
\end{pmatrix} 
= \begin{pmatrix}
\boldsymbol{\Lambda} \boldsymbol{\mu} \\
-\frac{1}{2} \boldsymbol{\lambda} \\
\end{pmatrix}^{\mathsf{T}} \\
h(\mathbf{x}) &= (2 \pi)^{- 1/2} \\
\end{align*}
$$

Define a simple function $f$ such that

$$
f(-2 \boldsymbol{\eta}_2) = f(\boldsymbol{\lambda}) = \boldsymbol{\Lambda}
$$

which simply packs elements of $\boldsymbol{\lambda}$ back into a $D \times D$ matrix. Then we have

$$
\begin{align*}
\boldsymbol{\mu} &= \boldsymbol{\Lambda}^{-1} \boldsymbol{\eta}_1 
= f(-2 \boldsymbol{\eta}_2)^{-1} \boldsymbol{\eta}_1
\end{align*}
$$

Therefore

$$
\begin{align*}
g(\boldsymbol{\eta}) &= |\boldsymbol{\Lambda}|^{1/2} \exp \left\{ - \frac{1}{2} \boldsymbol{\mu}^{\mathsf{T}} \boldsymbol{\Lambda} \boldsymbol{\mu} \right\} \\
&= \left| f(-2 \boldsymbol{\eta}_2) \right|^{1/2} \exp \left\{ - \frac{1}{2} \boldsymbol{\eta}_1^{\mathsf{T}} f(-2 \boldsymbol{\eta}_2)^{-1} \boldsymbol{\eta}_1 \right\}
\end{align*}
$$

### Maximum likelihood and sufficient statistics

Taking gradient of the following integral

$$
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \,d\mathbf{x} = 1
$$

with respect to $\boldsymbol{\eta}$, we have

$$
\nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \,d\mathbf{x}
+ g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) \,d\mathbf{x} = 0
$$

Rearranging terms

$$
- \frac{1}{g(\boldsymbol{\eta})} \nabla g(\boldsymbol{\eta}) 
= g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) \,d\mathbf{x}
= \operatorname{E} [\mathbf{u}(\mathbf{x})]
$$

Therby we have

$$
- \nabla \ln g(\boldsymbol{\eta}) = \operatorname{E} [\mathbf{u}(\mathbf{x})] \tag{2.226}
$$

**Excercise 2.58**

Take derivative of (2.226) again with respect to $\boldsymbol{\eta}$

$$
\begin{align*}
- \nabla \nabla \ln g(\boldsymbol{\eta})
&= \nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x})^{\mathsf {T}} \,d\mathbf{x} \\
&\quad + g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathsf {T}} \,d\mathbf{x} \\
&= \nabla g(\boldsymbol{\eta}) \frac{1}{g(\boldsymbol{\eta})} \operatorname{E} [\mathbf{u}(\mathbf{x})^{\mathsf {T}}]
+ \operatorname{E}[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathsf {T}}] \\
&= \operatorname{E}[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathsf {T}}]
- \operatorname{E} [\mathbf{u}(\mathbf{x})] \operatorname{E} [\mathbf{u}(\mathbf{x})^{\mathsf {T}}] \\
&= \operatorname{cov} [\mathbf{u}(\mathbf{x})]
\end{align*}
$$

This provided an alternative way to find the moment of a distribution, namely by differentiation.

Given a set of observations $\mathbf{X} = \{\mathbf{x}\_1, \dots, \mathbf{x}\_N\}$, the likelihood fucntion is

$$
p(\mathbf{X}|\boldsymbol{\eta})
= \left( \prod_{n=1}^N h(\mathbf{x}_n) \right) g(\boldsymbol{\eta})^N \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) \right\}
$$

Setting derivative of $\ln p(\mathbf{X}\|\boldsymbol{\eta})$ with respect to $\boldsymbol{\eta}$ to zero, we obtain the maximum likelihood estimator

$$
- \nabla \ln g(\boldsymbol{\eta}_{\mathrm{ML}}) = \frac{1}{N} \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n)
$$

Thus we see that $\sum\_n \mathbf{u}(\mathbf{x}\_n)$ is the sufficient statistic for distributions of exponential famity.

Moreover, by the law of large number, we have $\lim\_{N \rightarrow \infty} - \nabla \ln g(\boldsymbol{\eta}\_{\mathrm{ML}}) = E[\mathbf{u} (\mathbf{x})]$.
Comparing with (2.226), we see $\boldsymbol{\eta}\_{\mathrm{ML}}$ will converge to the true value $\boldsymbol{\eta}$.

### Conjugate priors

For distributions of exponential family, the conjugate prior can be shosen as

$$
p(\boldsymbol{\eta}|\boldsymbol{\chi}, \nu) = f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^\nu \exp (\nu \boldsymbol{\eta}^{\mathsf {T}} \boldsymbol{\chi})
$$

where $f(\boldsymbol{\chi}, \nu)$ is the normalization coefficient.

Indeed, conjugacy can be shown by the posterior

$$
p(\boldsymbol{\eta} | \mathbf{X}, \boldsymbol{\chi}, \nu)
\propto g(\boldsymbol{\eta})^{\nu+N} \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \left( \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) + \nu \boldsymbol{\chi} \right) \right\}
$$

which takes the same form as the prior.

Generally, $\nu$ can be interpreted as the number of fictitious observations, with each fictious observation taking the value of their average.

### Noninformative priors

In many cases, we have little knowledge about the prior distribution of parameters. Therefore, we seek a **noninformative prior** which have little influence on the posterior.

*Let data speak for themselves.*

Consider a distribution $p(x\|\lambda)$ governed by the parameter $\lambda$.
Intuitively, we want a prior such that all possible values of $\lambda$ have equal probability.
For a discrete random variable with $K$ possible states, the prior takes $p(\lambda) = 1/K$.

However, if $\lambda$ is an unbounded continous random variable. $p(\lambda)$ cannot be normalized since the integral over $\lambda$ diverges.
Such priors are called **improper**. In practice, however, improper priors can be used as long as the posterior is proper.
For instance, recall the case of unknown mean and known variance in Section 2.3.6.
By taking $\sigma\_0 \rightarrow \infty$, the prior becomes improper and uniform. Nonetheless, the posterior is well-defined as long as we have at least one data point.

Another problem arises from nonlinear transformation of variables. Suppose $p\_\lambda(\lambda)$ is constant and $\lambda = \eta^2$. 
Then the density of $\eta$ given by

$$
p_\eta(\eta) = p_\lambda(\lambda) \left| \frac{d\lambda}{d\eta} \right| 
= p_\lambda(\eta^2) 2 \eta
\propto \eta
$$

is not constant. Note that this problem does not arise when we use maximum likelihood, since the likelihood function is simply a function of $\lambda$.

Here two simple non-informative priors are introduced.

Consider a probability density which takes the form

$$
p(x|\mu) = f(x-\mu)
$$

The parameter $\mu$ is called a **location parameter**. 
Such distribution exihibit **translation invariance** since if $x$ is shifted using $\widehat{x} = x + c$, then the density of new variables given by

$$
p(\widehat{x}|\widehat{\mu}) = f(\widehat{x} - \widehat{\mu})
$$

where $\widehat{\mu} = \mu - c$, is the same as the original. 

If we want a prior to have translation invariance and thus be non-informative, the prior should satisfy

$$
p(\mu-c) = p(\mu)
$$

for any $c$, which implies $p(\mu) = \textrm{const}$.

As an example, the conjugate prior of the mean $\mu$ in Gaussian distribution gives $p(\mu\|\mu\_0, \sigma\_0^2) = \mathcal{N} (\mu\|\mu\_0, \sigma\_0^2)$. 
In the limit of $\sigma\_0^2 \rightarrow \infty$, we have $p(\mu\|\mu\_0, \sigma\_0^2) \rightarrow 0$. Then in the posterior, the contribution of the prior to the mean vanishes, which gives a noninformative prior of the mean.

Consider a density of the form

$$
p(x|\sigma) = \frac{1}{\sigma} f \left( \frac{x}{\sigma} \right)
$$

where $\sigma > 0$ is called the **scale parameter**. 

Note that $p(x\|\sigma)$ is normalized as long as the $f(x)$ is normalized. This can be shown by

$$
\int p(x|\sigma) \,dx = \frac{1}{\sigma} \int f(\frac{x}{\sigma}) \,dx
= \int f(\frac{x}{\sigma}) \,d(\frac{x}{\sigma}) = 1
$$

It can be shown that such form of distribution exibits **scale invariance**. If $\widehat{x} = c x$, we have

$$
p(\widehat{x}|\widehat{\sigma}) = \frac{1}{\widehat{\sigma}} f(\frac{x}{\widehat{\sigma}})
$$

where $\widehat{\sigma} = c \sigma$. A prior with scale invariance then takes the form

$$
p(\sigma) = \frac{1}{c} p(\frac{\sigma}{c})
$$

which gives a set of solutions $p(\sigma) \propto 1/\sigma$.

A example of scale parameter is the standard deviation of Gaussian distribution.
The conjugate prior for the standard deviation gives $p(\lambda \| a\_0, b\_0) = \mathrm{Gam}(\lambda \| a\_0, b\_0)$.
In the limit $a\_0, b\_0 \rightarrow 0$, we see that $p(\lambda\|a\_0, b\_0) \propto \frac{1}{\lambda}$.
And in the posterior, we see that the estimator depends only on the data instead of the prior.

## Nonparametric methods

The distributions discussed so far have a common functional form that is governed by a set of parameters. These parameters are determined from the data.
This is the **parametric** approach. A problem with this approach is that predictive performance largely depends on the choice of distribution.

With **nonparametric** approach, we make few assumptions about the form of the distribution.

### Histogram methods

Consider a random variable $x$ whose density we wish to estimate.

We first partition the space of $x$ into bins of width $\Delta\_i$, and then count the number $n\_i$ of observations that fall into bin $i$. 
Given the total number of observations $N$, the probability that $x$ falls into bin $i$ is given by

$$
P_i = \frac{n_i}{N}
$$

The corresponding density within the bin is then given by

$$
p_i = \frac{n_i}{N \Delta_i}
$$

The density $p(x)$ is constant within each bin. And it is easy to see $\int p(x) \,dx = 1$. 

Often the widths of bins are chosen to be equal, so that $\Delta\_i = \Delta$.

![](/assets/images/prml/2.24.png)

The figure above shows the density estimation based on 50 data points.
When $\Delta$ is small, the histogram tend to be spiky, the model of the distribution is obscure. 
When $\Delta$ is large, the histogram becomes smooth, and the multimodal property of the distribution is not captured.
Hence some intermidiate value for $\Delta$ is prefered.

Histogram method is applicable on sequential data or large data set.
It is also commonly used for data-virtualization in 1D or 2D space.

One of the disadvantages of histogram is that the density is discontinous at the boundaries of bins.

It also suffers from the curse of dimensionality. A $D$-deimensional with each variable devided into M bins has a total of $M^D$ bins.

### Kernel density estimator

Suppose observations are drawn from an unknwon distribution $p(\mathbf{x})$ in a D-dimensional space.

The probability mass of a region $\mathcal{R}$ is given by

$$
P = \int_{\mathcal{R}} p(\mathbf{x}) \,d\mathbf{x}
$$

Then the probability that K points out of a total of N lies inside $\mathcal{R}$ is given by the binomial distribution

$$
\mathrm{Bin}(K|N, P) = \binom{N}{K} P^K (1-P)^{N-K}
$$

from which we see the fraction of points falling in $\mathcal{R}$ satisfies

$$
\begin{align*}
\operatorname{E} \left[ \frac{K}{N} \right] &= P \\
\operatorname{var} \left[ \frac{K}{N} \right] &= \frac{P(1-P)}{N} \\
\end{align*}
$$

As $N$ increases, the variance decreases and $K/N$ converges to $P$.

If the region $\mathcal{R}$ is sufficiently small, the density within the region is approxmately constant and can be estimated by

$$
p(\mathbf{x}) \simeq \frac{P}{V} \simeq \frac{K}{NV}
$$

where V is the volume of $\mathcal{R}$.

However, small region is less likely to contain sufficient observations, which will require more data as a consequence.

We can exploit this estimator in two ways. 
Either by fixing $K$ and determining the value of $V$, which gives rise to K-nearest-neigbor or
by fixing V and determining the value of K, which gives rise to the kernel approach.
It can be shown that both estimators converge to the true density as $N \rightarrow \infty$ and $V$ shrinks properly.

We start with the kernel approach.
To count the number of points lying inside a region, we define a function

$$
k(\mathbf{u}) = \begin{cases}
1, & \text{if }|u_i| \le 1/2, \quad i= 1, \dots, D \\
0, & \text{otherwise}
\end{cases}
$$

which represent a unit cube centered on the origin.

The quantity $k((\mathbf{x} - \mathbf{x}\_n)/h)$ takes $1$ if $\mathbf{x}\_n$ lies inside/on the cube of side $h$ centered on $\mathbf{x}$.
or equivalently if $\mathbf{x}$ lies inside/on the cube centered on $\mathbf{x}\_n$.
Then the number of data points lying inside the cube centered on $\mathbf{x}$ is

$$
K = \sum_{n=1}^N k \left( \frac{\mathbf{x} - \mathbf{x}_n}{h} \right)
$$

The density at $\mathbf{x}$ is estimated with

$$
p(\mathbf{x}) = \frac{K}{NV} = \frac{1}{N h^D} \sum_{n=1}^N k \left( \frac{\mathbf{x} - \mathbf{x}_n}{h} \right)
\tag{2.249}
$$

where $V = h^D$ is the volume of the hypercube.

However, this density estimator is also discontinous at the boundaries of cubes.
A solution is to choose a smooth kernel function, e.g. the Gassian kernel function.
The estimator using Gaussian kernel takes the form

$$
p(\mathbf{x}) = \frac{1}{N} \sum_{n=1}^N \frac{1}{(2 \pi h^2)^{D/2}} \exp \left\{ - \frac{\left\| \mathbf{x} - \mathbf{x}_n \right\|^2}{2 h^2} \right\}
$$

where $h$ is the **smoothing parameter**. With large $h$, the model becomes smoothe; with small $h$, the model becomes spiky.
The factor $1/N$ ensures that the density is normalized.

Note that, the Gaussian estimator takes in contributions from all data points, with further points weighing less. 

Furthermore, We can choose any kernel function $k(\mathbf{u})$ in (2.249) subject to

$$
\begin{align*}
k(\mathbf{u}) &\ge 0 \\
\int k(\mathbf{u}) \,d\mathbf{u} &= 1 \\
\end{align*}
$$

The class of models given by (2.249) is called the kernel density estimator, or **Parzen** estimator.

Kernel methods do not require a seperate training phase before estimating density.
However, it requires storage for all training data, and the computation cost of a single density evaluation grows linearly with the size of data set.

### Nearest-neighbor methods

A restriction of the kernel methods is that $h$ is fixed. So in regions of high data density, $h$ is relatively large and may lead to local over-smoothing. 
In regions of low data density, $h$ is relatively small and may give noisy estimation.

Naturally, $h$ may be good to be chosen dependent on the location within data space.

Consider a sphere centered on $\mathbf{x}$ at which we want to estimate the density.
By setting $V$ to the volume of the smallest sphere that contains exactly K data points, $p(\mathbf{x})$ is estimated with $\frac{K}{NV}$.
This technique is called the **K nearest neighbours**.
Generally small $K$ results in noisy estimation and large $K$ results in over-smoothing.

Note that, the density estimated with KNN is not normalized, regardless of the choice of $K$.
It can be shown by following steps.

**Excercise 2.61**

Let $V(\mathbf{x})$ be the volume of the smallest sphere containing exactly K points. Then the density estimator takes the form

$$
p(\mathbf{x}) = \frac{K}{N V(\mathbf{x})}
$$

in which $K$ and $N$ are constant.

Now let $S$ be the smallest sphere centered on the origin which contains all $N$ data points.

For any $\mathbf{x}$ outside $S$, a sphere centered on $\mathbf{x}$ with radius $2 \left\\| \mathbf{x} \right\\|$ must also contains all $N$ points. 
If we define $V\_0(\mathbf{x})$ as the volume of such sphere, then we have $V\_0(\mathbf{x}) \ge V(\mathbf{x})$ and $V\_0(\mathbf{x}) \propto \left\\| x \right\\| ^D$

We will show that the integral $\displaystyle \int\_{\mathbb{R}^D - S} \frac{1}{\left\\| x \right\\| ^D} \,d \mathbf{x}$
diverges. Because if it does, then the integral $\displaystyle \int\_{\mathbb{R}^D - S} \frac{1}{V(\mathbf{x})} \,d \mathbf{x}$
also diverges. Then by the definition of improper integral, we can say that $\displaystyle \int\_{\mathbb{R}^D} \frac{1}{V(\mathbf{x})} \,d \mathbf{x}$
diverges.

Under the [hyperspherical coordinates](https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates), the integral takes the form

$$
\begin{align*}
\int_{\mathbb{R}^D - S} \frac{1}{\left\| x \right\| ^D} \,d \mathbf{x}
&= \int_{\mathbb{R}^D - S} \frac{1}{r^D} r^{D-1}\sin ^{D-2}(\theta _{1})\sin ^{D-3}(\theta _{2})\cdots \sin(\theta _{D-2})\,dr\,d\theta _{1}\,d\theta _{2}\cdots d\theta _{D-1} \\
&= \int_{0}^{2\pi} \sin ^{D-2}(\theta _{1}) \,d\theta_1 
\cdots \int_{0}^{2\pi} \sin(\theta _{D-2}) \,d\theta _{2}
\int_{0}^{2\pi} d\theta _{D-1} 
\int_{R_S}^{\infty} \frac{1}{r} \,dr
\end{align*}
$$

where $R\_S$ is the radius of $S$. We can easily see that this integral diverges since
$\displaystyle  \int\_{R\_S}^{\infty} \frac{1}{r} \,dr$ diverges.

\
Due to its singularity in density estimation, a more common problem for which we use KNN is classification.

Consider a data set of size $N$, with $N\_k$ points belonging to class $C\_k$. 
Suppose we want to classify $\mathbf{x}$, so we draw a sphere centered on $\mathbf{x}$ which contains exactly $K$ points regardless of their class.
Let $V$ be the volume of the sphere, and $K\_k$ be the number points of class $C\_k$ inside it.

The density associated with class $k$ is estimated by

$$
p(\mathbf{x}|C_k) = \frac{K_k}{N_k V}
$$

The unconditional density is given by

$$
p(\mathbf{x}) = \frac{K}{NV}
$$

The class prior is given by

$$
p(C_k) = \frac{N_k}{N}
$$

Using Bayes' theorem, we have the posterior

$$
p(C_k | \mathbf{x}) = \frac{K_k}{K}
$$

by which $\mathbf{x}$ is assigned to the class that corresponds to the largest $K\_k/K$. Ties are broken at random.

Generally small $K$ gives in more, small decision regions, and large $K$ results in fewer, large regions.

When $K=1$, the method is called the nearest-neighbor. It assigns a point to the same class as the nearest data point.

It can be shown that when $N \rightarrow \infty$, the nearest-neighbour ($K=1$) classifier has an error rate no more than twice the error rate of a classfifier that uses the true class distribution.

As with the kernel methods, KNN requires storage of the data set. On large data set, spatial index may be prefered in order to lower the cost of finding nearest neighbors.
