---
layout: posts
title: 'PRML Ch9 - Mixture models and EM'
---
- [Mixture Models and EM](#mixture-models-and-em)
  - [$K$-means clustering](#k-means-clustering)
    - [Image segmentation and compression](#image-segmentation-and-compression)
  - [Mixture of Gaussians](#mixture-of-gaussians)
    - [Maximum likelihood](#maximum-likelihood)
    - [EM for Gaussian mixtures](#em-for-gaussian-mixtures)
  - [An alternative view of EM](#an-alternative-view-of-em)
    - [Gaussian mixture revisited](#gaussian-mixture-revisited)
    - [Relation to $K$-means](#relation-to-k-means)
    - [Mixtures of Bernoulli distributions](#mixtures-of-bernoulli-distributions)
    - [EM for Bayesian linear regression](#em-for-bayesian-linear-regression)
  - [The EM algorithm in general](#the-em-algorithm-in-general)

# Mixture Models and EM

This chapter discuss several mixture models, starting from their application on clustering problems,
to further analysis of the EM algorithm by which we perform maximum likelihood or maximum posterior.

## $K$-means clustering

Consider a clustering problem of partitioning a set of $D$-dimensional data $\{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$ into $K$ clusters.
For the moment we assume $K$ is known.

Intuitively, points within a group should have smaller distances to that in the same group than other groups.
Based on this idea, we introduce a set of $D$-dimensional vectors $\boldsymbol{\mu}\_k$ where $k=1, \dots, K$ to represent the center of the cluster $k$.
Then our goal is to minimize the sum of squares of distances of each point to the center of its group.

Using 1-of-$K$ coding scheme, we define $r\_{nk} \in \{ 0, 1 \}$, where $k = 1, \dots, K$, for each point $\mathbf{x}\_n$.
And $r\_{nk} = 1$ if $\mathbf{x}\_n$ is assigned to cluster $k$ and for other $j \ne k$ we have $r\_{nj} = 0$.
The objective function is then given by

$$
J = \sum_{n=1}^N \sum_{k=1}^K r_{nk} \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2
$$

which is also called a **distortion measure**.

The minimization of $J$ is given by the EM algorithm with following steps

1. Initialize $\boldsymbol{\mu}\_k$
2. E step: optimize $r\_{nk}$ with $\boldsymbol{\mu}\_k$ fixed
3. M step: optimize $\boldsymbol{\mu}\_k$ with $r\_{nk}$ fixed
4. Repeat from step 2 until convergence (no assignment changes).

To optimize $r\_{nk}$, we choose a value $k$ of $r\_{nk} = 1$ so that $\\| \mathbf{x}\_n - \boldsymbol{\mu}\_k \\|^2$ takes the smallest value.
This is equivalent to assigning $\mathbf{x}\_n$ to the closest center $\boldsymbol{\mu}\_k$. 
Formally we write

$$
r_{nk} = \begin{cases}
1 & \text{ if } k = \operatorname{arg\,min}_j \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 \\
0 & \text{ otherwise}
\end{cases}
$$

As to optimization of $\boldsymbol{\mu}\_k$, setting derivative with respect to $\boldsymbol{\mu}\_k$ to zero, we have

$$
2 \sum_{n=1}^N r_{nk} ( \mathbf{x}_n - \boldsymbol{\mu}_k ) = 0
\tag{9.3}
$$

which is followed by

$$
\boldsymbol{\mu}_k = \frac{\sum_n r_{nk} \mathbf{x}_n}{\sum_n r_{nk}}
$$

The denominator is equal to number of points assigned to cluster $k$ and the numerator is
the sum of $\mathbf{x}\_n$ in cluster $k$.
For this reason, the algorithm is called the $K$-means.

Since each phase reduces the value of $J$, the convergence is gauranteed. However, 
it may converge to a local minimum rather than a global one.

Instead of completely random value, the initial value of $\boldsymbol{\mu}\_k$ can be chosen to be the value of a random $\mathbf{x}\_n$, which may lead to faster convergence.

The subproblem of finding the neareast neighbor can be accelerated with search trees or triangle inequality.

Applying multivariate Robbins-Monro procedure on finding roots of (9.3) with respect to $\boldsymbol{\mu}\_k$,
we obtain a sequential algorithm

$$
\boldsymbol{\mu}_k^{(n+1)} = \boldsymbol{\mu}_k^{(n)} + \eta_n (\mathbf{x}_n - \boldsymbol{\mu}_k^{(n)})
$$

where $\boldsymbol{\mu}\_k^{(n)}$ is the closest center to $\mathbf{x}\_n$. We iterate over $\mathbf{x}\_n$ to update $\boldsymbol{\mu}\_k$.

The use of squared Euclidean distance in $K$-means not only limits the type of data but also
makes the algorithm sensitive to outiliers.
A generalized similarity measure $\mathcal{V}(\mathbf{x}, \mathbf{x}^\prime)$ can be
used instead, which gives the $K$**-medoids** model

$$
\widetilde{J} = \sum_{n=1}^N \sum_{k=1}^K r_{nk} \mathcal{V} (\mathbf{x}_n, \boldsymbol{\mu}_k)
$$

### Image segmentation and compression

## Mixture of Gaussians

Recall the Gaussian mixture distribution given by

$$
p(\mathbf{x}) = \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\tag{9.7}
$$

Define a $K$-dimensional latent variable $\mathbf{z}$ with 1-of-$K$ binary coding scheme, such that

$$
p(z_k=1) = \pi_k
$$

Then the distribution of $\mathbf{z}$ is written

$$
p(\mathbf{z}) = \prod_{k=1}^K \pi_k^{z_k}
$$

The conditional distribution of $\mathbf{x}$ is given by

$$
p(\mathbf{x}|\mathbf{z}) = \prod_{k=1}^K \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)^{z_k}
$$

We can write (9.7) in the form of a marginal distribution

$$
p(\mathbf{x}) = \sum_{\mathbf{z}} p(\mathbf{z}) p(\mathbf{x} | \mathbf{z}) 
= \sum_{k=1}^K \pi_k  \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$

Defining so allows us to work with the joint distribution $p(\mathbf{x}, \mathbf{z})$,
which typically takes simpler form and can be optimized using the EM algorithm as we will see.

The posterior $p(z\_k = 1 \| \mathbf{x})$, denoted by $\gamma (z\_k)$, can be given by

$$
\begin{align*}
\gamma(z_k) \equiv p(z_k = 1 | \mathbf{x})
&= \frac{p(z_k=1) p(\mathbf{x}|z_k=1)}{\sum_j p(z_j=1) p(\mathbf{x}|z_j=1)} \\
&= \frac{\pi_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
{\sum_j \pi_j \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} \\
\end{align*}
$$

which is also called the **responsibility**.

### Maximum likelihood

Given a set of data $\mathbf{X} = (\mathbf{x}\_1, \dots, \mathbf{x}\_N)^{\mathsf{T}}$ distributed to the Gaussian mixture,
the corresponding log likelihood function is given by

$$
\ln p(\mathbf{X}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})
= \sum_{n=1}^N \ln \left[ \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]
\tag{9.14}
$$

The maximum likelihood of Gaussian mixture models suffers from singularities.
Consider a isotropic component with the covariance given by $\sigma\_j \mathbf{I}$ and the mean $\boldsymbol{\mu}\_j$ equal to some point $\mathbf{x}\_n$.
The contribution of $\mathbf{x}\_n$ to the likelihood function can be given by

$$
(2 \pi)^{-1/2} \sigma_j^{-D}
$$

In the limit $\sigma\_j \rightarrow \infty$, this term will go to infinity and thus the likelihood function also goes to infinity.
It can be seen as an example of over-fitting and can be avoided by adopting a Bayesian approach.
Another method to avoid such over-fitting is to reset corresponding mean to a random value and the variance to a larger value when singularity is detected, and then continue optimization.

Such singularity generally does not arise in single-Gaussian models because as the value of $\sigma$ decrease,
points that are not equal to the mean will contribute high-order small multiplicative factors to the likelihood function, giving a overall decrease in likelihood.

A further issue of maximum likelihood of mixture models is known as the **identifiability**.
A $K$-component mixture will have a total of $K!$ equivalent solutions, which corresponds to $K!$ ways to assign $K$ sets of parameters to $K$ components.

Due to the summation in the logarithm, the solution of maximum likelihood is no longer analytical. 
We could resort to gradient-based optimization algorithms as before. Here we will use
an alternative approach - EM algorithm - to find the solution. 

### EM for Gaussian mixtures

The **expectation-maximization** algorithm is a method to find maximum likelihood solutions
for models with latent variables.
Here we consider the case of mixture of Gaussians.

Setting derivative of the likelihood function (9.14) with respect to $\boldsymbol{\mu}\_k$ to zero, we obtain

$$
- \sum_{n=1}^N \frac{\pi_k \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
{\sum_j \pi_j \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
\boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_n - \boldsymbol{\mu}_k) = 0
$$

Solving for $\boldsymbol{\mu}\_k$ we obtain

$$
\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) \mathbf{x}_n
\tag{9.17}
$$

where we defined

$$
N_k = \sum_{n=1}^N \gamma(z_{nk})
$$

Note that the solution takes the form of weighted average of the data points. $N\_k$ can be interpreted as the effective number of points assigned to cluster $k$.

Settting derivative with respect to $\boldsymbol{\Sigma}\_k$, we obtain

$$
\boldsymbol{\Sigma}_k = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) (\mathbf{x}_n - \boldsymbol{\mu}_k) (\mathbf{x}_n - \boldsymbol{\mu}_k)^{\mathsf{T}}
\tag{9.19}
$$

To maximize with respect to $\pi\_k$, we should consider the constraint $\sum\_k \pi\_k = 1$,
which can be done by introducing a Lagrange multiplier $\lambda$ with the Lagragian

$$
\ln p(\mathbf{X}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) 
+ \lambda (\sum_{k=1}^K \pi_k - 1)
$$

Setting derivative with respect to $\pi\_k$ to zero, we have

$$
\sum_{n=1}^N  \frac{\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
{\sum_j \pi_j \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
+ \lambda = 0
$$

Multiplying both sides with $\pi\_k$ and summing over $k$, we obtain $\lambda = -N$.
Solving for $\pi\_k$ gives

$$
\pi_k = \frac{N_k}{N}
\tag{9.22}
$$

Note that these results are not closed-form solutions, since $\gamma(z\_{nk})$
itself depends on these parameters.
The EM algorithm optimizes these parameters by following steps

1. Initialize parameters $\{ \boldsymbol{\mu\_k}, \boldsymbol{\Sigma}\_k, \pi\_k \}$ and
evaluate the log likelihood.
2. E step: evaluate the responsibilities $\gamma(z\_{nk})$
3. M step: re-estimate parameters $\{ \boldsymbol{\mu\_k}, \boldsymbol{\Sigma}\_k, \pi\_k \}$ with $\gamma(z\_{nk})$ evaluated in step 2.
4. Evaluate the log likelihood function. Terminate if the convergence criterion is met, otherwise go to step 2.

In practive, the algorithm is considered to be converged when the change in log likelihood or parameters is sufficiently small.
We shall see the log likelihood is guaranteed to increase after each iteration.
However, it is not guaranteed to find the global maximum.

The EM algorithm generally has much higher computation cost than the $K$-means algorithm. 
It is common to find better initial values by running $K$-means before the EM.

As with the gradient-based methods, EM does not avoid singularity of over-fitting component.
Additional techniques are reuquired.

## An alternative view of EM

This section provides an alternative view of the EM algorithm.

Suppose that the data set is given by $\mathbf{X} = (\mathbf{x}\_1, \dots, \mathbf{x}\_N)^{\mathsf{T}}$ and the corresponding latent variables are given by $\mathbf{Z} = (\mathbf{z}\_1, \dots, \mathbf{z}\_N)^{\mathsf{T}}$.

Then the log likehood function

$$
\ln p(\mathbf{X}|\boldsymbol{\theta}) = \ln \left[ \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta}) \right]
$$

If the observations are drawn independently, it can be shown that

$$
\sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta}) = \prod_n \sum_{k} p(\mathbf{x}_n, \mathbf{z}_{nk}|\boldsymbol{\theta})
$$

If $\mathbf{z}$ is continuous, we simply replace summation with integration.

Suppose for each observation, we are also told the value of corresponding latent variable.
The data set $\{ \mathbf{X}, \mathbf{Z} \}$ is called the **Complete** data set,
and $\mathbf{X}$ is called **incomplete**.
With complete data set, the maximization of the joint likelihood $p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$ is typically straightforward.

However in practice, the values of latent variables in $\mathbf{Z}$ are not given.
The only knownledge about $\mathbf{Z}$ that we can derive from the data is the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta})$.
With it we can evaluate the expectation of log likelihood function $\ln p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$, which is given by

$$
\begin{align*}
\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})})
= \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{(old)}}) \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
\end{align*}
$$

where $\boldsymbol{\theta}^{(\text{old})}$ is the parameter with current values and $\boldsymbol{\theta}$ is parameter with undertermined values.
This corresponds to the E step.

In the M step, we maximize the expectation $\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})})$ with respect to $\boldsymbol{\theta}$.

A general EM for maximum likelihood is given by following steps:

1. Initialize prameters $\boldsymbol{\theta}^{(\text{old})}$
2. E step: evaluate $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{(\text{old})})$
3. M step: find $\boldsymbol{\theta}^{(\text{new})}$ such that

$$
\boldsymbol{\theta}^{(\text{new})} = \underset{\boldsymbol{\theta}}{\operatorname{arg\,max}}
\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})})
$$

4. Terminate if converged. Otherwise assign $\boldsymbol{\theta}^{(\text{new})}$ to $\boldsymbol{\theta}^{(\text{old})}$ and go to step 2.

EM can also be applied on MAP. By Bayes' therem we have

$$
p(\boldsymbol{\theta} | \mathbf{X}, \mathbf{Z}) \propto p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
p(\boldsymbol{\theta})
$$

Therefore in E step, we evaluate the expectation

$$
\sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{(old)}}) \left[ \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}) \right]
= \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})}) + \ln p(\boldsymbol{\theta})
$$

And in M step, we maximize

$$
\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})}) + \ln p(\boldsymbol{\theta})
$$

Further discussion on EM will be present in Section 9.4.

### Gaussian mixture revisited

Consider the maximum likelihood of a Gaussian mixture, with complete i.i.d data set $\{ \mathbf{X}, \mathbf{Z} \}$.

The likelihood function is given

$$
\begin{align*}
p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})
&= \prod_{n=1}^N p(\mathbf{x}_n, \mathbf{z}_n|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \\
&= \prod_{n=1}^N  \prod_{k=1}^K \left[ \pi_k \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]^{z_{nk}} \\
\end{align*}
$$

Taking logarithm gives

$$
\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})
= \sum_{n=1}^N \sum_{k=1}^K z_{nk} \left[ \ln \pi_k + \ln \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]
$$

Compared to (9.14), it does not involve the logarithm of sums, which leads to a simpler solution of maximum likelihood.

Suppose $z\_{nk}$ is known for the moment. Maximization with respect to $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ will be very much like the single-Gaussian model.
Maximization with respect to $\boldsymbol{\pi}$ again can be solved with Lagarange multiplier, which gives

$$
\pi_k = \frac{1}{N} \sum_{n=1}^N z_{nk}
$$

To work around unknown latent variable values in practice, we first show the posterior

$$
\begin{align*}
p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})
&= \frac{p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}
{p(\mathbf{X}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}  \\
&= \prod_{n=1}^N \frac{p(\mathbf{x}_n, \mathbf{z}_n|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}
{p(\mathbf{x}_n|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}  \\
&= \prod_{n=1}^N p(\mathbf{z}_n|\mathbf{x}_n, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \\
\end{align*}
$$

from which we see the posterior distributions of $\{ \mathbf{z}\_n \}$ are independent.

The expectation of $z\_{nk}$ under the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})$ is then given by

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [z_{nk}]
&= \sum_{\mathbf{Z}} \left[ z_{nk} p(\mathbf{z}_n|\mathbf{x}_n) \prod_{i \ne n} p(\mathbf{z}_i|\mathbf{x}_i) \right] \\
&= \sum_{\mathbf{z}_n} z_{nk} p(\mathbf{z}_n|\mathbf{x}_n) \left[ \prod_{i \ne n} \sum_{\mathbf{z}_i} p(\mathbf{z}_i|\mathbf{x}_i) \right] \\
&= \sum_{\mathbf{z}_n} z_{nk} p(\mathbf{z}_n|\mathbf{x}_n) \\
&= p(z_{nk} = 1 |\mathbf{x}_n)
= \gamma(z_{nk}) \\
\end{align*}
$$

where we have omitted the condition on $\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}$ to keep notations uncluttered.
The result is the responsibility of component $k$ for point $\mathbf{x}\_n$.

The expectation of the complete-data log likelihood under the posterior $p(\mathbf{Z}\|\mathbf{X})$ is then given by

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]
&= \sum_{n=1}^N \sum_{k=1}^K \operatorname{E}_{\mathbf{Z}} [z_{nk}] \left[ \ln \pi_k + \ln \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right] \\
&= \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk}) \left[ \ln \pi_k + \ln \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right] \tag{9.40}
\end{align*}
$$

Applying the general EM, we first initialize $\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}$.
Then in the E step, we evaluate the responsibilities $\gamma(z\_{nk})$ with current parameter values.
In the M step, we maximize the expectation (9.40) by setting derivatives to zero, which gives us closed-form solutions (9.17), (9.19) and (9.22).

### Relation to $K$-means

$K$-means is a limit form of EM of mixture Gaussians, in which we perform hard assigment of points instead of soft assignment based on probabilities.

Consider a Gaussian mixture model, in which compoenents share the same covariance matrix $\epsilon \mathbf{I}$, so that

$$
p(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
= \frac{1}{(2 \pi \epsilon)^{D/2}} \exp \left( - \frac{1}{2 \epsilon} \| \mathbf{x} - \boldsymbol{\mu}_k \|^2 \right)
$$

The responsibilities take the from

$$
\gamma(z_{nk}) = \frac{\pi_k \exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2 / 2 \epsilon \right)}
{\sum_j \pi_k \exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 / 2 \epsilon \right)}
$$

**Excercise 9.11**

Let $\boldsymbol{\mu}\_k$ be the closest cluster center from $\mathbf{x}\_n$. Then in the limit $\epsilon \rightarrow 0$, we have

$$
\begin{align*}
&\quad \lim_{\epsilon \rightarrow 0} \frac{\exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 / 2 \epsilon \right)}{\exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2 / 2 \epsilon \right)} \\
&= \lim_{\epsilon \rightarrow 0} \left[ - \frac{1}{2 \epsilon} \left( \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 - \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2 \right) \right] \\
&= \begin{cases}
1 & \text{ if } j = k \\
0 & \text{ otherwise}
\end{cases}
\end{align*}
$$

Assume that there's only one closest center for point $\mathbf{x}\_n$ and $\pi\_k \ne 0$. we have

$$
\lim_{\epsilon \rightarrow 0} \gamma(z_{nk}) = \begin{cases}
1 & \text{ if } j = k \\
0 & \text{ otherwise} \\
\end{cases}
$$

which corresponds to the definition of $r\_{nk}$ in $K$-means, where we assign a point to
the closest cluster.

The expectation of complete-data log likelihood can be written

$$
\lim_{\epsilon \rightarrow 0} \operatorname{E}_{\mathbf{Z}} [\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]
= \lim_{\epsilon \rightarrow 0} \sum_{n=1}^N \sum_{k=1}^K r_{nk} \left[ \ln \pi_k - \frac{D}{2} \ln (2 \pi \epsilon) - \frac{1}{2 \epsilon} \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2   \right]
$$

Since $\epsilon$ is considered constant in the context of $K$-means and will not be estimated,
maximization with respect to $\boldsymbol{\mu}$ is equivalent to minimizing the distortion measure $J$ in $K$-means.
And the mean estimated with (9.17) then reduces to the form (9.4) of $K$-means.

$\pi\_k$ is set to the fraction of points assigned to cluster $k$, which does not play a role
in $K$-means.

### Mixtures of Bernoulli distributions

A.k.a latent class analysis.

Consider a $D$-dimensional random variable $\mathbf{x}$, with each component independently governed by a Bernoulli distribution, so that

$$
p(\mathbf{x}|\boldsymbol{\mu}) = \prod_{i=1}^D \mu_i^{x_i} (1 - \mu_i)^{1- x_i}
$$

The mean and covariance are then given by

$$
\begin{align*}
\operatorname{E} [\mathbf{x}] &= \boldsymbol{\mu} \\
\operatorname{cov} [\mathbf{x}] &= \operatorname{diag} \left( \mu_i (1-\mu_i) \right)
\end{align*}
$$

A mixture distribution consisting of $K$ such components is given by

$$
p(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\pi})
= \sum_{k=1}^K \pi_k p(\mathbf{x}|\boldsymbol{\mu}_k)
\tag{9.47}
$$

with

$$
p(\mathbf{x}|\boldsymbol{\mu}_k) = \prod_{i=1}^D \mu_{ki}^{x_{i}} (1 - \mu_{ki})^{1- x_{i}}
$$

The mean and covariance are given by

$$
\begin{align*}
\operatorname{E} [\mathbf{x}] &= \sum_{k=1}^K \boldsymbol{\mu}_k \\
\operatorname{cov} [\mathbf{x}]
&= \sum_{k=1}^K \pi_k \left( \boldsymbol{\Sigma}_k + \boldsymbol{\mu}_k \boldsymbol{\mu}_k^{\mathsf{T}} \right) - \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}}
\end{align*}
$$

where $\boldsymbol{\Sigma}\_k = \operatorname{diag} \left( \mu\_{ki} (1 - \mu\_{ki}) \right)$.

**Exercise 9.12**

This comes from a more general property of mixture distributions. Given a general mixture distribution of the form

$$
p(\mathbf{x}) = \sum_{k=1}^K \pi_k p(\mathbf{x}|k)
$$

with the conditional mean and variance of each component denoted by $\boldsymbol{\mu}\_k$ and $\boldsymbol{\Sigma}\_k$ respectively.

The mean of the mixture distribution is

$$
\operatorname{E} [\mathbf{x}] = \sum_k \pi_k \operatorname{E} [\mathbf{x}|k] = \sum_{k=1}^K \boldsymbol{\mu}_k
$$

and the covaraince is

$$
\begin{align*}
\operatorname{cov} [\mathbf{x}] &= \operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} \right] - \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}} \\
&=  \sum_k \pi_k \operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} | k \right]
- \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}} \\
&= \sum_k \pi_k \left( \boldsymbol{\Sigma}_k + \boldsymbol{\mu}_k \boldsymbol{\mu}_k^{\mathsf{T}} \right)
- \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}} \\
\end{align*} 
$$

\
Given a incomplete data set $\mathbf{X} = \{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$, the log likelihood is given by

$$
\ln p(\mathbf{X}|\boldsymbol{\mu}, \boldsymbol{\pi})
= \sum_{n=1}^N \ln \left[ \sum_{k=1}^K \pi_k p(\mathbf{x}_n|\boldsymbol{\mu}_k) \right]
$$

which again involves the logarithm of summation. Thus the maximum likelihood solution no longer has closed form.

As with the Gaussian mixture model, we instead introduce a latent variable $\mathbf{z} = (z\_1, \dots, z\_K)$, with 1-of-$K$ binary coding scheme, so that

$$
p(\mathbf{x}|\mathbf{z}, \boldsymbol{\mu}) = \prod_{k=1}^K p(\mathbf{x}|\boldsymbol{\mu}_k)^{z_k}
$$

and

$$
p(\mathbf{z}|\boldsymbol{\pi}) = \prod_{k=1}^K \pi_k^{z_k}
$$

Marginalizing out $\mathbf{z}$, we obtain (9.47).

The complete-data log likelihood is then given by

$$
\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\pi})
= \sum_{n=1}^N \sum_{k=1}^K z_{nk} \left\{ \ln \pi_k + \sum_{i=1}^D \left[ x_{ni} \ln \mu_{ki} + (1-x_{ni}) \ln (1-\mu_{ki}) \right] \right\}
$$

The expectation of $z\_{nk}$ under the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\pi})$ is given

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [z_{nk}]
&= p(z_{nk} = 1|\mathbf{x}_n, \boldsymbol{\mu}, \boldsymbol{\pi}) \\
&= \frac{\pi_k p(\mathbf{x}_n|\boldsymbol{\mu}_k)} {\sum_j \pi_j p(\mathbf{x}_n|\boldsymbol{\mu}_j)}
\equiv \gamma(z_{nk})
\end{align*}
$$

Therefore the expectation of the complete-data log likelihood is given

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\pi})]
= \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk}) \left\{ \ln \pi_k + \sum_{i=1}^D \left[ x_{ni} \ln \mu_{ki} + (1-x_{ni}) \ln (1-\mu_{ki}) \right] \right\}
\end{align*}
$$

Setting derivative with respect to $\boldsymbol{\mu}\_k$ to zero, we obtain

$$
\boldsymbol{\mu}_k = \overline{\mathbf{x}}_k
= \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) \mathbf{x}_n
$$

where we defined

$$
N_k = \sum_{n=1}^N \gamma(z_{nk})
$$

The solution of $\pi\_k$ is given by

$$
\pi_k = \frac{N_k}{N}
$$

Unlike mixture of Gaussians, there is no singularity in which the likelihood function goes to infinity,
since $0 \le p(\mathbf{x}\_n\|\boldsymbol{\mu}\_k) \le 1$.
However, bad choice of initialization values can result in convergence to undesired stationary point. Consider an example where all component means are initialized to the same value. Then in the E step, we have

$$
\gamma(z_{nk}) = \pi_k
$$

The maximum likelihood solution is given by

$$
\boldsymbol{\mu}_k = \frac{1}{N} \sum_n \mathbf{x}_n
$$

which again takes equal values for all component means. Hence $\boldsymbol{\mu}\_k$ converges to $\frac{1}{N} \sum\_n \mathbf{x}\_n$.

TODO:

- digit example
- MAP
- Categorial distribution

### EM for Bayesian linear regression

Another application of EM arises in the evidence approximation.

Consider the Bayesian linear regression in Ch3 with the likelihood function defined as

$$
p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta)
= \prod_{n=1}^N \mathcal{N} (t_n|\mathbf{w}^{\mathsf {T}} \boldsymbol{\phi}(\mathbf{x}_n), \beta^{-1})
$$

the prior distribution defined as

$$
p(\mathbf{w} | \alpha) = \mathcal{N} (\mathbf{w} | \mathbf{0}, \alpha^{-1} \mathbf{I})
$$

The evidence is obtained from

$$
p(\mathbf{t}|\alpha, \beta) = \int p(\mathbf{t}|\mathbf{w}, \beta) p(\mathbf{w}|\alpha) \,d\mathbf{w}
$$

Since $\mathbf{w}$ is marginalized out, it can be seen as a latent variable. Then the complete-data log likelihood is given by

$$
\ln p(\mathbf{t}, \mathbf{w}|\alpha, \beta) = \ln p(\mathbf{t}|\alpha, \beta) + \ln p(\mathbf{w} | \alpha)
\tag{9.61}
$$

**Excercise 9.20 9.21**

In the E step, we evaluate the posterior

$$
p(\mathbf{w}|\mathbf{t}) = \mathcal{N} (\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N)
$$

where

$$
\begin{align*}
\mathbf{m}_N &= \beta \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf {T}} \mathbf{t}  \\
\mathbf{S}_N^{-1} &= \alpha I + \beta \boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi}  \\
\end{align*}
$$

Then the expectation of the log likelihood is given by

$$
\begin{align*}
\operatorname{E} [\ln p(\mathbf{t}, \mathbf{w}|\alpha, \beta)]
&= \frac{M}{2} \ln \left( \frac{\alpha}{2 \pi} \right)
- \frac{\alpha}{2} \operatorname{E} \left[ \mathbf{w}^{\mathsf{T}} \mathbf{w} \right]
+ \frac{N}{2} \ln \left( \frac{\beta}{2} \right) \\
&\quad - \frac{\beta}{2} \sum_{n=1}^N \operatorname{E} \left[ (t_n - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}_n)^2 \right]
\end{align*}
$$

Evaluate the expectation

$$
\begin{align*}
\operatorname{E} \left[ \mathbf{w}^{\mathsf{T}} \mathbf{w} \right]
&= \operatorname{Tr} \left( \operatorname{E} \left[ \mathbf{w} \mathbf{w}^{\mathsf{T}} \right] \right) \\
&= \operatorname{Tr} \left( \mathbf{S}_N + \mathbf{m}_N \mathbf{m}_N^{\mathsf{T}} \right) \\
&= \operatorname{Tr} \left( \mathbf{S}_N \right) + \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N
\end{align*}
$$

And 

$$
\begin{align*}
\sum_{n=1}^N \operatorname{E} \left[ (t_n - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}_n)^2 \right]
&= \operatorname{E} \left[ \| \mathbf{t} - \boldsymbol{\Phi} \mathbf{w} \|^2 \right] \\
&= \mathbf{t}^{\mathsf{T}} \mathbf{t} - 2 \mathbf{t}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{m}_N 
+ \operatorname{Tr} \left\{ \operatorname{E} \left[ \mathbf{w} \mathbf{w}^{\mathsf{T}} \right] \boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi} \right\} \\
&= \| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \|^2 + \operatorname{Tr} \left( \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi} \right) \\
&= \| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \|^2 + \beta^{-1} \operatorname{Tr} \left( \mathbf{I} - \alpha \mathbf{S}_N \right) \\
&= \| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \|^2 + \beta^{-1} \left( M - \alpha \operatorname{Tr} (\mathbf{S}_N) \right) \\
\end{align*}
$$

In the M step, we maxize the expectation by taking derivatives. Setting derivative with respect to $\alpha$ to zero, we obtain

$$
\alpha = \frac{M}{\operatorname{Tr} (\mathbf{S}_N) + \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N}
\tag{9.63}
$$

As of $\beta$, we have

$$
\frac{1}{\beta} = \frac{\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \|^2}{N - M + \alpha \operatorname{Tr} (\mathbf{S}_N) }
$$

Note that

$$
\operatorname{Tr} (\mathbf{S}_N) = \sum_i \frac{1}{\alpha + \lambda_i}
$$

Therefore

$$
\gamma = M - \alpha \operatorname{Tr} (\mathbf{S}_N)
$$

by which we can verify that the EM formulation is equivalent to (3.92) and (3.95).

Another application to the evidence maximization appeared in the RVM. TODO

## The EM algorithm in general

[Additional References]

- Neal, R.M., Hinton, G.E. (1998). A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants.

Given a set of observations $\mathbf{X}$ and corresponding latent variables $\mathbf{Z}$,
our goal is to maximize the likelihood function

$$
p(\mathbf{X} | \boldsymbol{\theta}) = \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
$$

where $\boldsymbol{\theta}$ is a set of parameters. Here we assumed $\mathbf{z}$ is discrete.
Generalization to continuous variables is straightforward.

Introducing a distribution over latent variables $q(\mathbf{Z})$, we give the following decomposition

$$
\ln p(\mathbf{X} | \boldsymbol{\theta}) = \mathcal{L}(q, \boldsymbol{\theta}) + \operatorname{KL}(q||p)
\tag{9.70}
$$

where

$$
\begin{align*}
\mathcal{L}(q, \boldsymbol{\theta}) &= \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left[ \frac{p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})}{q(\mathbf{Z})} \right] \tag{9.71}\\
\operatorname{KL}(q||p) &= \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left[ \frac{p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})} \right] \tag{9.72} \\
\end{align*}
$$

> Motivation: negative free energy in statistical physics

$\mathcal{L}(q, \boldsymbol{\theta})$ is a functional of $q(\mathbf{Z})$ and a function of $\boldsymbol{\theta}$.
$\operatorname{KL}(q\|\|p)$ is the Kullback-Leibler divergence.

From an alternatice view of two-stage maximization, we show that the EM indeed increases the value of likelihood.

Let $\boldsymbol{\theta}^{\text{old}}$ be the current value of parameters. 
Recalling that $\operatorname{KL}(q\|\|p) \ge 0$, we see $\mathcal{L}(q, \boldsymbol{\theta}^{\text{old}})$ is a lower bound of $\ln p(\mathbf{X} \| \boldsymbol{\theta})$.
Since $\ln p(\mathbf{X} \| \boldsymbol{\theta})$ is independent of $q$, $\mathcal{L}(q, \boldsymbol{\theta}^{\text{old}})$ is maximized when $\operatorname{KL}(q\|\|p) = 0$, which gives $q(\mathbf{Z}) = p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$.
This corresponds to the E step.

In the second stage, while holding $q(Z)$ fixed, we maximize $\mathcal{L}(q, \boldsymbol{\theta})$ with respect to $\boldsymbol{\theta}$.
Unless the maximum is already reached, this will raise the lower bound $\mathcal{L}$.
Also changes in $\boldsymbol{\theta}$ will result in change of $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta})$,
so that $q(\mathbf{Z}) = p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ne p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{new}})$ and leads to $\operatorname{KL}(q\|\|p) > 0$ again.

Substituting $q(\mathbf{Z})$ with $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$ in (9.71), we obtain

$$
\begin{align*}
\mathcal{L}(q, \boldsymbol{\theta})
&= \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
- \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \\
&= \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})
+ \operatorname{H} \left[ \mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}} \right]\\
\end{align*}
$$

where $\operatorname{H} \left[ \mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}} \right]$ is the conditional entropy of $\mathbf{Z}$ and is independent of $\boldsymbol{\theta}$.
Therefore, maximization $\mathcal{L}(q, \boldsymbol{\theta})$ is equivalent to maximize $\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})$,
which corresponds to the M step in EM algorithm.

When $p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$ is a member of exponential family or the product of them,
$\ln p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$ generally gives a simpler optimization objective than that obtained by directly working with $p(\mathbf{X}\|\boldsymbol{\theta})$.

**View from parameter space**

The EM algorithm can also be view from the parameter space. With the current value of parameters $\boldsymbol{\theta}^{\text{old}}$,
we evaulate the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$.
Setting $q(\mathbf{Z}) = p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$,
we have $\mathcal{L}(q, \boldsymbol{\theta}^{\text{old}}) = \ln p(\mathbf{X}\|\boldsymbol{\theta}^{\text{old}})$.

Moreover

$$
\begin{align*}
\frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{L}(q, \boldsymbol{\theta})
&= \frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}) \\
&= \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})
\frac{\partial}{\partial \boldsymbol{\theta}} \left[ \ln p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}) + \ln p(\mathbf{X}|\boldsymbol{\theta}) \right] \\
&= \frac{\partial}{\partial \boldsymbol{\theta}} \ln p(\mathbf{X}|\boldsymbol{\theta})
+ \sum_{\mathbf{Z}} \frac{p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})}{p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta})} 
\frac{\partial}{\partial \boldsymbol{\theta}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta})
\end{align*}
$$

Taking $\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}$, we obtain

$$
\begin{align*}
\frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{L}(q, \boldsymbol{\theta}) \bigg|_{\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}}
&= \frac{\partial}{\partial \boldsymbol{\theta}} \ln p(\mathbf{X}|\boldsymbol{\theta}) \bigg|_{\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}}
\end{align*}
$$

which means $\mathcal{L}(q, \boldsymbol{\theta})$ and $p(\mathbf{X}\|\boldsymbol{\theta})$ make tangential contact at $\boldsymbol{\theta}^{\text{old}}$ where they have the same gradient.
In the M step, maximization of $\mathcal{L}$ with respect to $\boldsymbol{\theta}$ is convex for components of exponential family,
which will raise the lower bound as well as increase the log likelihood.

**EM for MAP**

EM can also be applied on MAP where we maximize $p(\mathbf{X} \| \boldsymbol{\theta})$ with a prior $p(\boldsymbol{\theta})$. By

$$
\ln p(\boldsymbol{\theta}|\mathbf{X}) = \ln p(\mathbf{X}|\boldsymbol{\theta})
+ \ln p(\boldsymbol{\theta}) - \ln p(\mathbf{X})
$$

and (9.70), we have

$$
\ln p(\boldsymbol{\theta}|\mathbf{X}) = \mathcal{L}(q, \boldsymbol{\theta}) + \operatorname{KL}(q||p) 
+ \ln p(\boldsymbol{\theta}) - \ln p(\mathbf{X})
$$

Note that $p(\mathbf{X})$ is constant. Then the E step remains the same as the maximum likelihood. 
The M step involves an additional term $\ln p(\boldsymbol{\theta})$ to be considered.

TODO: **Generalized EM**



