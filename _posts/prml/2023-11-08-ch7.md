---
title: 'PRML Ch7 - Sparse Kernel Machines'
layout: posts
---
- [Sparse Kernel Machines](#sparse-kernel-machines)
  - [Maximum Margin Classifiers](#maximum-margin-classifiers)
    - [Overlapping class distributions](#overlapping-class-distributions)
    - [Relation to logistic regression](#relation-to-logistic-regression)
    - [Multiclass SVMs](#multiclass-svms)
    - [SVMs for regression](#svms-for-regression)
    - [Computational learning theory](#computational-learning-theory)
  - [Relevence vector machines](#relevence-vector-machines)
    - [RVM for regression](#rvm-for-regression)
    - [Analysis of sparsity](#analysis-of-sparsity)
    - [RVM for classification](#rvm-for-classification)

# Sparse Kernel Machines

The kernel methods introduced in the previous chapter share a common disadvantage that the kernel function is required to be evaluated at every training data point when making a prediction, which can be computationaly infeasible if the data set is large.

In this chapter, a class of sparse models are developed, by which we train the model once with full data set but make predictions with only a subset of data.

## Maximum Margin Classifiers

Consider a two-class classification problem with a liearn model model defined by

$$
y(\mathbf{x}) = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}) + b
\tag{7.1}
$$

where the bias $b$ is seperately given. The target variable $t \in \{ -1, 1\}$.
Data points are classified according to the sign of $y(\mathbf{x})$.

Assume that the training data set is linearly seperable in the feature space. 
Then we could find a solution such that $y(\mathbf{x}\_n) > 0$ for $t\_n = 1$ and 
$y(\mathbf{x}\_n) < 0$ for $t\_n = -1$. Thus we have $t\_n y(\mathbf{x}\_n) > 0$ for all training data.

The idea of support vector machine is to find a decision boundary that maximize the **margin**,
which is defined to be the smallest distance between the decision boundary and any data point.

With (7.1), the distance of a point $\mathbf{x}\_n$ to the decision boundary is given by

$$
\frac{|y(\mathbf{x_n})|}{\|\mathbf{w}\|} = \frac{t_n y(\mathbf{x}_n)}{\|\mathbf{w} \|}
= \frac{t_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n) + b \right)}{\|\mathbf{w}\|}
$$

Then the maximum margin problem can be expressed as

$$
\underset{\mathbf{w}, b}{\operatorname{arg\,max}} \left\{ 
\|\mathbf{w}\|^{-1} \min_n \left[ t_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n) + b \right) \right]
\right\}
$$

Since the distance between the data points and decision boundary stays invariant under the transformation $\mathbf{w} \rightarrow k \mathbf{w}, b \rightarrow k b$,
there will be infinitely many solutions.
To simplify the problem, we can set

$$
t_n y(\mathbf{x}_n) = t_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n) + b \right) = 1
$$

for the closest point $\mathbf{x}\_n$ to the surface. Then for all data points we have

$$
t_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n) + b \right) \ge 1,
\quad n = 1, \dots, N
$$

This is the canonical representation of the decision surface. For data points at which the euqlity holds,
the corresponding constraints are said to be active, and the rest of constraints are said to be inactive.

Since there will always be at least one closest point, at which the corresponding constraint becomes active, we have

$$
\min_n \left[ t_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n) + b \right) \right] = 1
$$

Then we simply need to maximize $\\| \mathbf{w} \\|^{-1}$, which is equivalent to minimize $\\| \mathbf{w} \\|^2$. Thus the problem becomes

$$
\underset{\mathbf{w}, b}{\operatorname{arg\,min}} \frac{1}{2} \| \mathbf{w} \|^2
\tag{7.6}
$$

This is a quadratic programming problem in which we minimize a quadratic function
subject to a set of linear constraints.

Note that, although parameter $b$ does not appear in (7.6), the constraints implicitly determine its value.

To solve the constrained optimization problem, we introduce a set of Lagrange multipliers
$a\_n \ge 0$ and the Lagragian function

$$
L(\mathbf{w}, b, \mathbf{a}) = \frac{1}{2} \| \mathbf{w} \|^2
- \sum_{n=1}^N a_n \left[ t_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n) + b \right) \right]
$$

where $\mathbf{a} = (a\_1, \dots, a\_N)^{\mathsf{T}}$.

Since $L$ is a quadratic form of $\mathbf{w}$ and a linear function of $b$,
the minimization with respect to $\mathbf{w}$ and $b$ is achieved by setting derivative of $L$ to 0, which gives

$$
\begin{align*}
\mathbf{w} &= \sum_n a_n t_n \boldsymbol{\phi}(\mathbf{x}_n) \tag{7.8} \\
0 &= \sum_n a_n t_n \tag{7.9}\\
\end{align*}
$$

Eliminating $\mathbf{w}$ and $b$ in $L$, we obtain the dual problem in which we maximize

$$
\widetilde{L} (\mathbf{a}) = \sum_{n=1}^N a_n - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m k(\mathbf{x}_n, \mathbf{x}_m)
\tag{7.10}
$$

with respect to $\mathbf{a}$ and subject to

$$
\begin{align*}
a_n &\ge 0 \\
\sum_n a_n t_n &= 0 \tag{7.12} \\
\end{align*}
$$

Here the kernel function is defined by $k(\mathbf{x}\_n, \mathbf{x}\_m) = \boldsymbol{\phi}(\mathbf{x}\_n)^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}\_m)$.
It is requried to be postive definite to ensure the dual problem is bounded above.


The computational complexity of solving a quadratic programming problem of $M$ variables is generally $O(M^3)$.
As to the dual problem, the cost will be $O(N^3)$, which will exposes disadvantage when $N$ is greater than the number of parameters $M$ in the primal problem.
However, working with kernel gives the capability of introducing high-dimensionality feature space.

To classify new data poitnts, we substitute $\mathbf{w}$ and use

$$
y(\mathbf{x}) = \sum_n a_n t_n k(\mathbf{x}, \mathbf{x}_n) + b
\tag{7.13}
$$

From the KKT conditions given by

$$
\begin{align*}
a_n &\ge 0 \\
t_n y(\mathbf{x}_n) -1 &\ge 0 \\
a_n \left[ t_n y(\mathbf{x}_n) - 1 \right] &= 0 \tag{7.16}\\
\end{align*}
$$

we see either $a\_n = 0$ or $t\_n y(\mathbf{x}\_n) = 1$ must be true. Any data points with $a\_n = 0$ will not contribute to the decision surface.
Other points, which satisfies $t\_n y(\mathbf{x}\_n) = 1$ and thus lie in the decision suface,
are called **support vectors**. Once the model is trained, we see points other than support vectors can be discarded.

After the optimal value for $\mathbf{a}$ is found, the value of $b$ is determined by

$$
t_n y(\mathbf{x}_n) = t_n \left( \sum_{m \in \mathcal{S}} a_m t_m k(\mathbf{x}_n, \mathbf{x}_m) + b \right) = 1
$$

For better numerical stabability, we can make use of $1/t\_n = t\_n$ and take average over $n$ to obtain

$$
b = \frac{1}{N_\mathcal{S}} \sum_{n \in \mathcal{S}} \left( t_n - \sum_{m \in \mathcal{S}} a_m t_m k(\mathbf{x}_n, \mathbf{x}_m) \right)
$$

where $N\_\mathcal{S}$ is the number of support vectors.

\
An alternative form of the objective function can be written

$$
\lambda \sum_n E_{\infty} (t_n y(\mathbf{x}_n) - 1) + \| \mathbf{w} \|^2
\tag{7.19}
$$

where $\lambda > 0$ and $E\_{\infty} (\cdots)$ is defined by

$$
E_{\infty} (z) = \begin{cases}
0 & \text{ if } z \ge 0 \\
\infty & \text{ otherwise }  \\
\end{cases} 
$$

The first term can be seen as a penalty term for breaking constraint. For points that break the constraint, this term takes $\infty$.
For other points, the term takes $0$ and has no effect.

Indeed, we will show that the error function given by (7.19) is equivalent to the original
optimization problem.

### Overlapping class distributions

When the training data is not linearly seperable, the error function (7.19) is unsolvable.
To allow points to be on the wrong side of the margin boundary, we want a 
softer penalty term that incrases with the distantce from the margin boundary.

The **slack variables** $\xi\_n \ge 0$ where $n=1, \dots, N$, is defined as $\xi\_n = 0$ for points on or inside the margin boundary of correct side and $\xi\_n = \|t\_n - y(\mathbf{x}\_n)\|$ for other points.
By this definition, the points on the decision boundary will take $y(\mathbf{x}\_n) = 0$ and $\xi\_n = 1$, and misclassified points will take $\xi\_n > 1$.
Note that, points of $0 < \xi\_n \le 1$ lie on the wrong side of the margin boundary,
but are still correctly classified.

![](/assets/images/prml/7.3.png)

Defining

$$
t_n y(\mathbf{x}_n) + \xi_n = 1
$$

we 'pushes' the points that lie on the wrong side of margin back onto the margin boundary,
so that they become support vectors. For other correctly located points, we have $\xi\_n = 0$ and

$$
t_n y(\mathbf{x}_n) + \xi_n \ge 1 + \xi_n \ge 1
$$

Thereby we obtain constraints

$$
\begin{align*}
\xi_n &\ge 0 \\
t_n y(\mathbf{x}_n) &\ge 1 - \xi_n \\
\end{align*}
$$

It is sometiems called the **soft margin** since it allows misclassified points.
However, this framework is sensitive to outliers since the penalty is linear in $\xi$.

> A more precise proof of equivelence is given [here](https://math.stackexchange.com/questions/1021586/about-the-slack-variable-for-hinge-loss-svm)

To control the degree of relaxing at a moderate point, we add a penelty term and obtain

$$
C \sum_n \xi_n + \frac{1}{2} \|\mathbf{w}\|^2
$$

where $C >0$ controls the magnitude of misclassification panelty with larger value giving less tolerance. It is analogous to a regularization parameter.
In the limit $C \rightarrow \infty$, we fall back to (7.19).

Introducing a set of Lagrange multipliers $\{a\_n, \mu\_n\}$, the Lagarangian function
is given

$$
L(\mathbf{w}, b, \mathbf{a}, \boldsymbol{\mu})
= \frac{1}{2} \| \mathbf{w} \|^2 + C \sum_n \xi_n
- \sum_n a_n \left[ t_n y(\mathbf{x}_n) - 1 + \xi_n \right]
- \sum_n \mu_n \xi_n
$$

subject to

$$
\begin{align*}
a_n &\ge 0 \\
t_n y(\mathbf{x}_n) - 1 + \xi_n &\ge 0 \\
a_n \left[ t_n y(\mathbf{x}_n) - 1 + \xi_n \right] &= 0 \\
\mu_n &\ge 0 \tag{7.26} \\
\xi_n &\ge 0 \\
\mu_n \xi_n &= 0 \\
\end{align*}
$$

Setting derivatives with respect to $\mathbf{w}$, $b$ and $\{\xi\_n\}$ to 0, we obtain

$$
\begin{align*}
\frac{\partial L}{\partial \mathbf{w}} = 0
&\Rightarrow \mathbf{w} = \sum_n a_n t_n \boldsymbol{\phi}(\mathbf{x}_n) \\
\frac{\partial L}{\partial b} = 0
&\Rightarrow  \sum_n a_n t_n = 0 \\
\frac{\partial L}{\partial \xi_n} = 0
&\Rightarrow a_n = C - \mu_n \tag{7.31} \\
\end{align*}
$$

By substitution we obtain the dual function

$$
\widetilde{L} (\mathbf{a}) = \sum_n a_n - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m k(\mathbf{x}_n, \mathbf{x}_m)
$$

which is identical to (7.10) in the linearly seperable case. The constraints are given by

$$
\begin{align*}
0 \le a_n \le C \tag{7.33} \\
\sum_n a_n t_n = 1 \\
\end{align*}
$$

in which (7.33) is sometimes called the **box constraints**. The condition $a\_n \le C$ comes from (7.26) and (7.31). 

By substitution, new predictions are again made by (7.13). As before, points of $a\_n = 0$ make no contributioin to predictions. 
For points of $a\_n > 0$, we have $t\_n y(\mathbf{x}\_n) = 1 - \xi\_n$. 
If $a\_n < C$, then $\mu\_n > 0$ and $\xi\_n = 0$, points lie on the margin boundary.
Points of $a\_n = C$ lie in the margin but correctly classified if $\xi\_n \le 1$ or are misclassified if $\xi\_n > 1$.

To determine $b$, we use points of $0 < a\_n < C$, which have $\xi\_n = 0$, to obtain

$$
t_n \left( \sum_{m \in \mathcal{S}} a_m t_m k(\mathbf{x}_n, \mathbf{x}_m) + b \right) = 1
$$

Averaging over $n$ we obtain

$$
b = \frac{1}{N_{\mathcal{M}}} \sum_{n \in \mathcal{M}} \left( t_n - \sum_{m \in \mathcal{S}} a_m t_m k(\mathbf{x}_n, \mathbf{x}_m) \right)
$$

An alternative equivalent formulation called $\nu$-SVM maximizes

$$
\widetilde{L}(\mathbf{a}) = - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N a_n a_m t_n t_m k(\mathbf{x}_n, \mathbf{x}_m)
$$

subject to

$$
\begin{align*}
0 \le a_n \le 1/N \\
\sum_n a_n t_n = 0 \\
\sum_n a_n \ge \nu \\
\end{align*}
$$

The parameter $\nu$ is interpreted as an upper bound of fraction of margin errors (i.e. points that lie in the margin but correctly classified), or a lower bound of fraction of support vectors.

Quadratic programming algorithms: 

- Protected conjugate gradients
- Decomposition methods
- Sequential minimal optimization (SMO)

### Relation to logistic regression

Consider a SVM model that allows missclassification points. For points that stays on the correct side of the margin boundary, we have $y\_n t\_n \ge 1$ and $\xi\_n = 0$. And for remaining points $\xi\_n = 1 - y\_n t\_n$.
Hence the objective function can be written in the form

$$
\sum_n E_{SV} (y_n t_n) + \lambda \| \mathbf{w} \|^2
$$

where $\lambda = (2C)^{-1}$, and $E\_{SV}$ is the **hinge error function** given by

$$
E_{SV}(y_n t_n) = [1 - y_n t_n]_+
$$

where $[\cdot]+$ is the postive part function.

Now consider a logistic regression problem with target vairiable $t \in \{-1, 1\}$. This gives us

$$
\begin{align*}
p(t=1|y) &= \sigma(y) \\
p(t=-1|y) &= 1 - \sigma(y) = \sigma(-y) \\
\end{align*}
$$

which can be collectively expressed

$$
p(t|y) = \sigma(yt)
$$

where $y$ is defined by (7.1).

Then by taking the negtive log of the likelihood function and adding a quadratic regularizer,
we obtain

$$
\sum_n E_{LR}(y_n t_n) + \lambda \| \mathbf{w} \|^2
$$

where

$$
E_{LR} (y_n t_n) = \ln (1 + \exp(- y_n t_n))
$$

The similarity of different error functions are shown in the figure below, in which the
black line is an indicator of misclassification, the blue curve is the hinge erro function,
the red curve is the logistic error scaled by $1/\ln 2$ so that it pass $(0, 1)$, and
the green curve is the squared error.

![](/assets/images/prml/7.5.png)

Both the hinge error and the logistic error are measures of misclassification.
The difference is that the flat part of the hinge error function gives sparse solutions.

### Multiclass SVMs

Recall Ch4, with a decision boundary of the form $y(\mathbf{x}) = 0$, we can use
multiple binary classifiers to build a multi-class classfifier. However, there methods
have the problem of ambiguous regions.

The decision boundary given by

$$
y(\mathbf{x}) = \max_k y_k(\mathbf{x})
$$

eliminates the ambiguity, but the scales between different $y\_k(\mathbf{x})$ may not be matched approprietely.

### SVMs for regression

Starting from the regularized sum-of-squares error given by

$$
\frac{1}{2} \sum_n (y_n - t_n)^2 + \frac{\lambda}{2} \| \mathbf{w} \|^2
$$

to obtain sparse solutions, we replace the sum-of-squarze error with an $\epsilon$-**insensitive error function** given by

$$
E_\epsilon (y(\mathbf{x}) - t) = \begin{cases}
0 & \text{ if } |y(\mathbf{x}) - t| < \epsilon \\
|y(\mathbf{x}) - t| - \epsilon & \text{otherwise}
\end{cases}
$$

Then we minimize

$$
C \sum_{n=1}^N E_\epsilon (y(\mathbf{x}) - t) + \frac{1}{2} \| \mathbf{w} \|^2
$$

where $C$ is analogous to the inverse of a regularization term.

By setting the constraints $y\_n - \epsilon \le t\_n \le y\_n + \epsilon$, the ojective function becomes continuous and differentiable.
However, this will disallow points to lie outside the $\epsilon$-tube.
Therefore we can introduce two slack variables $\xi\_n \ge 0$ and $\widehat{\xi}\_n \ge 0$ to
give relxed constraints

$$
y_n - \epsilon - \widehat{\xi}_n \le t_n \le y_n + \epsilon + \xi_n
$$

And the error function becomes

$$
C \sum_n (\xi_n + \widehat{\xi}_n) + \frac{1}{2} \| \mathbf{w} \|^2
$$

Introducing the Lagrange multipliers $\{ a\_n, \widehat{a}\_n, \mu\_n, \widehat{\mu}\_n \}$,
we minimize the Lagrangian function

$$
\begin{align*}
L &= C \sum_n (\xi_n + \widehat{\xi}_n) + \frac{1}{2} \| \mathbf{w} \|^2
- \sum_n \mu_n \xi_n - \sum_n \widehat{\mu}_n \widehat{\xi}_n \\
&\quad - \sum_n a_n (y_n + \epsilon + \xi_n - t_n)
- \sum_n \widehat{a}_n (t_n - y_n + \epsilon + \widehat{\xi}_n)
\end{align*}
$$

suject to

$$
\begin{align*}
a_n \ge 0, \widehat{a}_n \ge 0, \mu_n \ge 0, \widehat{\mu}_n \ge 0
\end{align*}
$$

along with additional constraints given by KKT conditions.

Setting derivatives with respect to zero gives

$$
\begin{align*}
\frac{\partial L}{\partial \mathbf{w}} = 0 &\Rightarrow
\mathbf{w} = \sum_n (a_n - \widehat{a}_n) \boldsymbol{\phi}(\mathbf{x}_n) \\
\frac{\partial L}{\partial b} = 0 &\Rightarrow
\sum_n (a_n - \widehat{a}_n) = 0 \\
\frac{\partial L}{\partial \xi_n} = 0 &\Rightarrow
a_n + \mu_n = C \\
\frac{\partial L}{\partial \widehat{\xi}_n} = 0 &\Rightarrow
\widehat{a}_n + \widehat{\mu}_n = C \\
\end{align*}
$$

By substitution we obtain the dual function

$$
\begin{align*}
\widetilde{L} (\mathbf{a}, \widetilde{\mathbf{a}})
&= - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N (a_n - \widehat{a}_n) (a_m - \widehat{a}_m) k(\mathbf{x}_n, \mathbf{x}_m) \\
&\quad - \epsilon \sum_n (a_n - \widehat{a}_n) + \sum_n (a_n - \widehat{a}_n) t_n
\end{align*}
$$

The constraints with respect to $\mathbf{a}$ and $\widehat{\mathbf{a}}$ is given by

$$
\begin{align*}
0 \le a_n \le C \\
0 \le \widehat{a}_n \le C \\
\sum_n (a_n - \widehat{a}_n) = 0 \\
\end{align*}
$$

Substituting $\mathbf{w}$, predictions are then made by

$$
y(\mathbf{x}) = \sum_n (a_n - \widehat{a}_n) k(\mathbf{x}, \mathbf{x}_n) + b
$$

From following constrains given by KKT conditions

$$
\begin{align*}
a_n (y_n + \epsilon + \xi_n - t_n) &= 0 \\
\widehat{a}_n (t_n - y_n + \epsilon + \widehat{\xi}_n) &= 0 \\
\mu_n \xi_n = (C - a_n) \xi_n &= 0 \\
\widehat{\mu}_n \xi_n = (C - \widehat{a}_n) \widehat{\xi}_n &= 0 \\
\end{align*}
$$

we see that when $a\_n$ are nonzero, $y\_n + \epsilon + \xi\_n - t\_n = 0$. Therefore the corresponding points lie either on the upper boundary ($\xi\_n = 0$) or above the upper boundary ($\xi\_n > 0$).
Similarly, for points with nonzero $\widehat{a}\_n$, they lie
either on the lower boundary or below the boundary.

Also note that, either one or both of $a\_n$ and $\widehat{a}\_n$ must be zero, because if not,
by summing up $y\_n + \epsilon + \xi\_n - t\_n = 0$ and $t\_n - y\_n + \epsilon + \widehat{\xi}\_n = 0$,
we have $2 \epsilon + \xi\_n + \widehat{\xi}\_n = 0$, which should not hold since $\epsilon > 0$.

The support vectors are those points for which either $a\_n \ne 0$ or $\widehat{a}\_n \ne 0$.
They lie either on the boundary of the tube or outside the tube.
Points inside the tube will not contribute to predictions.

The parameter $b$ can be found by considering points with $0 < a\_n < C$, which takes $\xi\_n = 0$. 
We solve for $b$ using

$$
\begin{align*}
b &= t_n - \epsilon - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}_n) \\
&= t_n - \epsilon - \sum_m (a_m - \widehat{a}_m) k(\mathbf{x}_m, \mathbf{x}_n)
\end{align*}
$$

Similar results can be obtained for points with $0 < \widehat{a}\_n < C$,
and we should average over these results for better numerical error.

An alternative SVM model for regression involves maxmizing

$$
\begin{align*}
\widetilde{L} (\mathbf{a}, \widetilde{\mathbf{a}})
&= - \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N (a_n - \widehat{a}_n) (a_m - \widehat{a}_m) k(\mathbf{x}_n, \mathbf{x}_m) \\
&\quad + \sum_n (a_n - \widehat{a}_n) t_n
\end{align*}
$$

subject to

$$
0 \le a_n \le C/N \\
0 \le \widehat{a}_n \le C/N \\
\sum_n (a_n - \widehat{a}_n) = 0 \\
\sum_n (a_n + \widehat{a}_n) \le \nu C \\
$$

where $\nu$ bounds the fraction of points lying outside the tube. It can shown that
at most $\nu N$ data points lies outside the tube.

### Computational learning theory

A.k.a statical learning theory

Probably approximately correct (PAC) framework estimates how large a data set needs in order to achive good generalization.

## Relevence vector machines

RVM is a Bayesian sparse kernel method based on the same idea as the SVM. It gives a probability interpretation of the output, instead of decisions.

### RVM for regression

The RVM for regression follows the definition of linear model discussed in Ch3 but with a modified prior that gives sparse solutions.

The conditional distribution of the target is defined as

$$
p(t|\mathbf{x}, \mathbf{w}, \beta) = \mathcal{N} (t | y(\mathbf{x}), \beta^{-1})
$$

The mean is given by a linear function

$$
y(\mathbf{x}) = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x})
\tag{7.77}
$$

RVM chooses a particular set of basis functions given by kernel, so that

$$
y(\mathbf{x}) = \sum_{n=1}^N w_n k(\mathbf{x}, \mathbf{x}_n) + b
\tag{7.78}
$$

where $b$ is the bias. However, it no longer requires the kernel function to be positive semidefinite.
In subsequent analysis, the general form (7.77) is assumed for generality.

Given a set of inputs $\mathbf{X} = ( \mathbf{x}\_1, \dots, \mathbf{x}\_N )^{\mathsf{T}}$ and the corresponding target values $\mathbf{t} = (t\_1, \dots, t\_N)^{\mathsf{T}}$,
the likelihood function is given by

$$
p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta)
= \prod_{n=1}^N \mathcal{N} (t_n | y(\mathbf{x}_n), \beta^{-1})
$$

The prior distribution in RVM is a Gaussian with a diagonal covariance matrix such taht

$$
p(\mathbf{w}|\boldsymbol{\alpha}) = \prod_{i=1}^M \mathcal{N}(w_i|0, \alpha_i^{-1})
$$

where $\boldsymbol{\alpha} = (\alpha\_1, \dots, \alpha\_M)^{\mathsf{T}}$.

Using results in Section 3.3.1, we obtain the posterior

$$
p(\mathbf{w}|\mathbf{t}, \mathbf{X}, \boldsymbol{\alpha}, \beta)
= \mathcal{N} (\mathbf{w} | \mathbf{m}, \boldsymbol{\Sigma})
$$

where

$$
\begin{align*}
\mathbf{m} &= \beta \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
\boldsymbol{\Sigma} &= (\mathbf{A} + \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1} \\
\end{align*}
$$

$\boldsymbol{\Phi}$ is the design matrix, and $\mathbf{A} = \operatorname{diag} (\alpha\_i)$. With the mean given by (7.78), we have $\boldsymbol{\Phi} = \mathbf{K}$.

To determine the values of hyperparameters, we use the evidence framework as in ch3, by integrating out the parameters

$$
p(\mathbf{t} |\mathbf{X}, \alpha, \beta)
= \int p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \beta) p(\mathbf{w}|\boldsymbol{\alpha})
\,d\mathbf{w}
$$

This can be done by either completing the square or using the results in Section 2.3.3.
Here we use the latter approach.

First note that the likelihood function can be written

$$
p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta)
= \mathcal{N} (\mathbf{t} | \boldsymbol{\Phi} \mathbf{w}, \beta^{-1} \mathbf{I}_N)
$$

With following change of variables

$$
\begin{align*}
\mathbf{x} &\rightarrow \mathbf{w},
&\boldsymbol{\mu} &\rightarrow \mathbf{0}, 
&\boldsymbol{\Lambda}^{-1} &\rightarrow \mathbf{A}^{-1} \\
\mathbf{y} &\rightarrow \mathbf{t},
& \mathbf{A} &\rightarrow \boldsymbol{\Phi},
& \mathbf{b} &\rightarrow \mathbf{0}, 
& \mathbf{L}^{-1} &\rightarrow \beta^{-1} \mathbf{I} \\
\end{align*}
$$

we obtain the marginal likelihood function

$$
p(\mathbf{t}|\mathbf{X}, \boldsymbol{\alpha}, \beta) = \mathcal{N} (\mathbf{t}| \mathbf{0}, \mathbf{C})
$$

where

$$
\mathbf{C} = \beta^{-1} \mathbf{I} + \boldsymbol{\Phi} \mathbf{A}^{-1} \boldsymbol{\Phi}^{\mathsf{T}}
$$

Taking logarithm gives

$$
\ln p(\mathbf{t}|\mathbf{X}, \boldsymbol{\alpha}, \beta)
= - \frac{1}{2} \left\{ N \ln 2 \pi + \ln |\mathbf{C}| + \mathbf{t}^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t} \right\}
\tag{7.85}
$$

Again, there are two approachs for maximize the evidence.
The first is to set derivatives to zero and obtain the iterative estimator

$$
\begin{align*}
\alpha_i^{(new)} &= \frac{\gamma_i}{m_i^2} \tag{7.87} \\
\frac{1}{\beta^{(new)}} &= \frac{\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m} \|^2}{N - \sum_i \gamma_i} \tag{7.88} \\
\end{align*}
$$

where

$$
\gamma_i = 1 - \alpha_i \Sigma_{ii}
$$

Then the procedure is given by

1. Initialize $\boldsymbol{\alpha}$ and $\beta$
2. Evaluate $\boldsymbol{\Sigma}$ and $\mathbf{m}$
3. Re-estimate $\boldsymbol{\alpha}$ and $\beta$
4. Terminate on convergence. Otherwise repeat from step 2

The second approach is the EM algorithm, which will be discussed in 9.3.4.
It is found that the first approach is somewhat gives faster convergence than the second.
Here we show the derivation of the first approach.

Consider first the derivative with respect to $\alpha\_i$. We have

$$
\partial \ln |\mathbf{C}|
= - \operatorname{Tr} \left[ \mathbf{A}^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{C}^{-1} \boldsymbol{\Phi} \mathbf{A}^{-1} \partial \mathbf{A} \right]
$$

Using the Woodbury identity on $\mathbf{C}^{-1}$ gives

$$
\mathbf{C}^{-1} = \beta \mathbf{I} - \beta^2 \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathsf{T}}
$$

Combining with

$$
\beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} = \boldsymbol{\Sigma}^{-1} - \mathbf{A}
$$

we have

$$
\mathbf{A}^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{C}^{-1} \boldsymbol{\Phi} \mathbf{A}^{-1}
= \mathbf{A}^{-1} - \boldsymbol{\Sigma}
$$

Therefore

$$
\partial \ln |\mathbf{C}| = \operatorname{Tr} \left[ (\boldsymbol{\Sigma} - \mathbf{A}^{-1}) \partial \mathbf{A} \right]
$$

and

$$
\frac{\partial \ln |\mathbf{C}|}{\partial \alpha_i} = \Sigma_{ii} - \alpha_i^{-1}
$$

where $\Sigma\_{ii}$ is the i-th diagonal element of $\boldsymbol{\Sigma}$.

For the term $\mathbf{t}^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t}$, we have

$$
\begin{align*}
\partial (\mathbf{t}^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t})
&= \beta^2 (\boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t})^{\mathsf{T}} (\partial \mathbf{A}) (\boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}) \\
&= \mathbf{m}^{\mathsf{T}} (\partial \mathbf{A}) \mathbf{m}
\end{align*}
$$

from which we obtain

$$
\frac{\partial (\mathbf{t}^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t})}{\partial \alpha_i}
= m_i^2
$$

Therefore

$$
\begin{align*}
&\qquad \frac{\partial}{\partial \alpha_i} \ln p(\mathbf{t}|\mathbf{X}, \boldsymbol{\alpha}, \beta)
&= 0 \\
&\Rightarrow \Sigma_{ii} - \alpha_i^{-1} + m_i^2 = 0 \\
&\Rightarrow \alpha_i = \frac{1 - \alpha_i \Sigma_{ii}}{m_i^2} \\
\end{align*}
$$

Now consider the derivative with respect to $\beta$. First we have

$$
\partial \ln |\mathbf{C}| = - \operatorname{Tr} (\mathbf{C}^{-1} \beta^{-2} \partial \beta)
$$

Applying Woodbury inverse identiy, we have

$$
\begin{align*}
\operatorname{Tr} (\mathbf{C}^{-1})
&= \operatorname{Tr} (\beta \mathbf{I}) - \operatorname{Tr} (\beta^2 \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathsf{T}}) \\
&= N \beta - \operatorname{Tr} (\beta^2 \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \boldsymbol{\Sigma} ) \\
&= N \beta - \beta \operatorname{Tr} (\mathbf{I} - \mathbf{A} \boldsymbol{\Sigma}) \\
&= \beta (N - \sum_i \gamma_i)
\end{align*}
$$

For the term $\mathbf{t}^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t}$, we have

$$
\begin{align*}
\partial (\mathbf{t}^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t})
&= \left( \mathbf{t}^{\mathsf{T}} \mathbf{t} - 2 \mathbf{t}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{m} + \| \boldsymbol{\Phi} \mathbf{m} \|^2 \right) \partial \beta \\
&= \| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m} \|^2 \partial \beta \\
\end{align*}
$$

Combining these we obtain the iterative formula (7.88) for $\beta$.

As we shall see in next section, during optimization, some of $\{ \alpha\_i \}$ are driven to large values

Having found the optimal values $\boldsymbol{\alpha}^\ast$ and $\beta^\ast$, by the results in Section 3.3.2, we obtain the predictive distribution

$$
p (t|\mathbf{x}, \mathbf{X}, \mathbf{t}, \boldsymbol{\alpha}^\ast, \beta^\ast)
= \mathcal{N} (\mathbf{m}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}), \sigma^2 (\mathbf{x}))
$$

where

$$
\sigma^2 (\mathbf{x}) = (\beta^\ast)^{-1} + \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}}
\boldsymbol{\Sigma} \boldsymbol{\phi}(\mathbf{x})
$$

Note that, with localized basis functions, the model still have the problem that
the variance becomes small at regions far from the data.
This can be addressed with Gaussian process, but this will incur much higher computational cost than an simple RVM.

Compared to SVM, RVM has the advantage that

- RVM can typically achive better sparsity with little or no reduction in generalization ability compared to SVM.
- RVM determines the hyperparameters by using training data only
- RVM allow more flexible choices of basis functions. It does not require kernel function to
be postive semidefinite.

The disadvantage of RVM is that it involves optimizing a nonconvex function, and training time may be longer than a SVM.
An RVM with $M$ parameters costs $O(M^3)$ to train since it involves inversion of matrix.
For SVM, there are algorithms to solve quadratic programming in $O(M^2)$.

### Analysis of sparsity

> Additional reference:
> - Tipping and Faul. Fast marginal likelihood maximisation for sparse Bayesian models. AISTATS 2003.

Inspecting (7.87), we see that even when all $\alpha\_j$ where $j \ne i$ is fixed, the optimization with respect to $\alpha\_i$ still requires iteratice optimization.
This can be improved by determining a stationary point of $\alpha\_i$ analytically with other $\alpha\_j$ fixed, which will lead to faster convergence.

First we separate out terms dependent of $\alpha\_i$

$$
\begin{align*}
\mathbf{C}
&= \beta^{-1} \mathbf{I} + \sum_{j \ne i} \alpha_j^{-1} \boldsymbol{\varphi}_j \boldsymbol{\varphi}_j^{\mathsf{T}}
+ \alpha_i^{-1} \boldsymbol{\varphi}_i \boldsymbol{\varphi}_i^{\mathsf{T}} \\
&= \mathbf{C}_{-i} + \alpha_i^{-1} \boldsymbol{\varphi}_i \boldsymbol{\varphi}_i^{\mathsf{T}} \tag{7.93} \\
\end{align*}
$$

where $\boldsymbol{\varphi}\_i$ is the i-th column of $\boldsymbol{\Phi}$, and $\mathbf{C}\_{-i}$ is the matrix $\mathbf{C}$ with dependence on $\alpha\_i$ removed, 

By matrix identiy (C.15) we have

$$
\begin{align*}
|\mathbf{C}|
&= \left| \mathbf{C}_{-i} + \alpha_i^{-1} \boldsymbol{\varphi}_i \boldsymbol{\varphi}_i^{\mathsf{T}} \right| \\
&= |\mathbf{C}_{-i}| \left| \mathbf{I}_N + \alpha_i^{-1} \mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_i \boldsymbol{\varphi}_i^{\mathsf{T}} \right| \\
&= |\mathbf{C}_{-i}| \left( 1 + \alpha_i^{-1} \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_i \right) \\
\end{align*}
$$

Using Woodbury identiy on (7.93) we have

$$
\mathbf{C}^{-1} = \mathbf{C}_{-i}^{-1} - \frac{\mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_i \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{C}_{-i}^{-1}}
{\alpha_i + \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_i }
$$

Defining

$$
\begin{align*}
s_i &= \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_i \\
q_i &= \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{C}_{-i}^{-1} \mathbf{t} \\
\end{align*}
$$

we have

$$
\ln |\mathbf{C}| = \ln |\mathbf{C}_{-i}| + \ln \left( \alpha_i + s_i \right) - \ln \alpha_i
$$

and

$$
\mathbf{t}^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t}
= \mathbf{t}^{\mathsf{T}} \mathbf{C}_{-i}^{-1} \mathbf{t}
- \frac{q_i^2}{\alpha_i + s_i}
$$

Then the log marginal likelihood given by (7.85) can be written

$$
L(\boldsymbol{\alpha}) = L(\boldsymbol{\alpha}_{-i}) + \lambda(\alpha_i)
$$

where $L(\boldsymbol{\alpha}\_{-i})$ includes terms independent of $\alpha\_i$ and
$\lambda(\alpha\_i)$ is defined as

$$
\lambda(\alpha_i) = \frac{1}{2} \left[ \ln \alpha_i - \ln (\alpha_i + s_i) + \frac{q_i^2}{\alpha_i + s_i} \right]
$$

$s\_i$ is called the **sparsity**, which measures the overlap between $\boldsymbol{\varPhi}\_i$ and other basis vectors.
$q\_i$ is the **quality** of $\boldsymbol{\varphi}\_i$, which measures the ....
We will see these two quatities together gives sparse solutions.

Taking derivative with respect to $\alpha\_i$ gives

$$
\frac{\partial \lambda(\alpha_i)}{\partial \alpha_i}
= \frac{\alpha_i^{-1} s_i^2 - (q_i^2 - s_i)}{2 (\alpha_i + s_i)^2}
$$

Setting it to zero, we obtain

$$
\alpha_i = \frac{s_i^2}{q_i^2 - s_i}
\tag{7.101}
$$

Since $\alpha\_i \ge 0$, the solution is given by (7.101) only when $q\_i^2 > s\_i$.
When $q\_i^2 \le s\_i$, $\lambda(\alpha\_i)$ is non-decreasing, and thus the solution takes $\alpha\_i \rightarrow \infty$.

When $\alpha\_i \rightarrow \infty$, it drives $w\_i$ to zero and the corresponding basis function $\phi\_i$ can be removed from the basis set as well as the $\boldsymbol{\varphi}\_i$ from the design matrix.
Otherwise, the basis function $\phi\_i$ should be included in the baiss function.

Sequential sparse Bayesian learning algorithm:

1. If solving a regression problem, initialize $\beta$
2. Initialize a single hyperparameter $\alpha\_1$ using (7.101), with other $\alpha\_j$ for $j \ne i$ set to infinity.
3. Evaluate $\boldsymbol{\Sigma}$, $\mathbf{m}$, $q\_i$ and $s\_i$ for all basis functions
4. Select a candidate basis function $\boldsymbol{\varphi}\_i$ from the set of all basis functions.
This can can be done randomly or in a specific order.
5. If $q\_i^2 > s\_i$ and $\alpha\_i < \infty$, the basis function $\boldsymbol{\varphi}\_i$ is already included in the model. Update $\alpha\_i$ using (7.101)
6. If $q\_i^2 > s\_i$ and $\alpha\_i = \infty$, add the basis function $\boldsymbol{\varphi}\_i$ to the model. Update $\alpha\_i$ using (7.101)
7. If $q\_i^2 \le s\_i$ and $\alpha\_i < \infty$, remove the basis function $\boldsymbol{\varphi}\_i$ from the model. Set $\alpha\_i$ to $\infty$.
8. If solving a regression problem, update $\beta$
9. Repeat from step 3 until convergence.

In practice, it is convenient to evaluate

$$
\begin{align*}
Q_i &= \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{C}^{-1} \mathbf{t} \\
S_i &= \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{C}^{-1} \boldsymbol{\varphi}_i \\
\end{align*}
$$

by which we can evaluate the quality and sparsity with

$$
\begin{align*}
q_i &= \frac{\alpha_i Q_i}{\alpha_i - S_i} \\
s_i &= \frac{\alpha_i S_i}{\alpha_i - S_i} \\
\end{align*}
$$

When $\alpha\_i = \infty$, we have $q\_i = Q\_i$ and $s\_i = S\_i$. Using Woodbury identity on $\mathbf{C}^{-1}$, we have

$$
\begin{align*}
Q_i &= \beta \boldsymbol{\varphi}_i^{\mathsf{T}} \mathbf{t}
- \beta^2 \boldsymbol{\varphi}_i^{\mathsf{T}} \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
S_i &= \beta \boldsymbol{\varphi}_i^{\mathsf{T}} \boldsymbol{\varphi}_i
- \beta^2 \boldsymbol{\varphi}_i^{\mathsf{T}} \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathsf{T}}  \boldsymbol{\varphi}_i \\
\end{align*}
$$

where $\boldsymbol{\Phi}$ and $\boldsymbol{\Sigma}$ are evaluated with only those basis vectors corresponding to finite $\alpha\_i$.

The computational cost is $O(M^3)$ where $M$ is the number of selected basis vectors. In the case of the kernel methods, $M$ is typically smaller than $N$.

### RVM for classification

Similar to the extension of SVM for classification, we can extend the RVM.

TODO


