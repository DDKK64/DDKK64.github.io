---
layout: posts
title: 'PRML Ch9 - Mixture models and EM'
---
- [Mixture Models and EM](#mixture-models-and-em)
  - [$K$-means clustering](#k-means-clustering)
    - [Image segmentation and compression](#image-segmentation-and-compression)
  - [Mixture of Gaussians](#mixture-of-gaussians)
    - [Maximum likelihood](#maximum-likelihood)
    - [EM for Gaussian mixtures](#em-for-gaussian-mixtures)
  - [EM for Gaussian mixtures](#em-for-gaussian-mixtures-1)
  - [An alternative view of EM](#an-alternative-view-of-em)
    - [Gaussian mixture revisited](#gaussian-mixture-revisited)
    - [Relation to $K$-means](#relation-to-k-means)
    - [Mixtures of Bernoulli distributions](#mixtures-of-bernoulli-distributions)
    - [EM for Bayesian linear regression](#em-for-bayesian-linear-regression)
  - [The EM algorithm in general](#the-em-algorithm-in-general)

# Mixture Models and EM

This chapter discuss several mixture models, starting from their application on clustering problems,
to further analysis of the EM algorithm by which we perform maximum likelihood or maximum posterior.

## $K$-means clustering

Consider a clustering problem of partitioning a set of $D$-dimensional data $\{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$ into $K$ clusters.
And for the moment we assume $K$ is known.

Intuitively, points within a group should have smaller distances to that in the same group than other groups.
Based on this idea, we introduce a set of $D$-dimensional vectors $\boldsymbol{\mu}\_k$ where $k=1, \dots, K$ to represent the center of the cluster $k$.
Then our goal is to determine the sum of squares of distances of each point to the center of its group.

Using 1-of-$K$ coding scheme, we define $r\_{nk} \in \{ 0, 1 \}$, where $k = 1, \dots, K$, for each point $\mathbf{x}\_n$.
And $r\_{nk} = 1$ if $\mathbf{x}\_n$ is assigned to cluster $k$ and for other $j \ne k$ we have $r\_{nj} = 0$.
The objective function is then given by

$$
J = \sum_{n=1}^N \sum_{k=1}^K r_{nk} \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2
$$

which is also called a **distortion measure**.

The minimization of $J$ is given by the EM algorithm with following steps

1. Initialize $\boldsymbol{\mu}\_k$
2. E step: optimize $r\_{nk}$ with $\boldsymbol{\mu}\_k$ fixed
3. M step: optimize $\boldsymbol{\mu}\_k$ with $r\_{nk}$ fixed
4. Repeat from step 2 until convergence (no assignment changes).

To optimize $r\_{nk}$, we choose a value $k$ of $r\_{nk} = 1$ so that $\\| \mathbf{x}\_n - \boldsymbol{\mu}\_k \\|^2$ takes the smallest value.
This is equivalent to assigning $\mathbf{x}\_n$ to the closest center $\boldsymbol{\mu}\_k$. 
Formally we write

$$
r_{nk} = \begin{cases}
1 & \text{ if } k = \operatorname{argmin}_j \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 \\
0 & \text{ otherwise}
\end{cases}
$$

As to optimization of $\boldsymbol{\mu}\_k$, setting derivative with respect to $\boldsymbol{\mu}\_k$, we have

$$
2 \sum_{n=1}^N r_{nk} \| \mathbf{x}_n - \boldsymbol{\mu}_k \| = 0
$$

which is followed by

$$
\boldsymbol{\mu}_k = \frac{\sum_n r_{nk} \mathbf{x}_n}{\sum_n r_{nk}}
$$

The denominator is equal to number of points assigned to cluster $k$ and the numerator is
the sum of $\mathbf{x}\_n$ in cluster $k$.
For this reason, the algorithm is called the $K$-means.

Since each phase reduces the value of $J$, the convergence is gauranteed. However, 
it may converge to a local minimum rather than a global one.

Instead of completely random value, the initial value of $\boldsymbol{\mu}\_k$ can be chosen to be the value of a random $\mathbf{x}\_n$, which may lead to faster convergence.

The subproblem of finding the neareast neighbor can be accelerated by using search trees.

The use of squared Euclidean distance in $K$-means not only limits the type of data but also
makes the algorithm sensitive to outiliers.
A generalized similarity measure $\mathcal{V}(\mathbf{x}, \mathbf{x}^\prime)$ can be
used instead, which gives the $K$-medoids model

$$
\widetilde{J} = \sum_{n=1}^N \sum_{k=1}^K r_{nk} \mathcal{V} (\mathbf{x}_n, \boldsymbol{\mu}_k)
$$

### Image segmentation and compression

## Mixture of Gaussians

Recall the Gaussian mixture distribution given by

$$
p(\mathbf{x}) = \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\tag{9.7}
$$

Consider a $K$-dimensional latent variable $\mathbf{z}$ with 1-of-$K$ binary coding scheme, such that

$$
p(z_k=1) = \pi_k
$$

Then the distribution of $\mathbf{z}$ is written

$$
p(\mathbf{z}) = \prod_{k=1}^K \pi_k^{z_k}
$$

The conditional distribution of $\mathbf{x}$ is given by

$$
p(\mathbf{x}|\mathbf{z}) = \prod_{k=1}^K \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)^{z_k}
$$

We can write (9.7) in the form of a marginal distribution

$$
p(\mathbf{x}) = \sum_{\mathbf{z}} p(\mathbf{z}) p(\mathbf{x} | \mathbf{z}) 
= \sum_{k=1}^K \pi_k  \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$

Defining so allows us to work with the joint distribution $p(\mathbf{x}, \mathbf{z})$,
which will give rise to the EM algorithm.

The posterior $p(z\_k = 1 \| \mathbf{x})$, denoted by $\gamma (z\_k)$, can be given by

$$
\begin{align*}
\gamma(z_k) \equiv p(z_k = 1 | \mathbf{x})
&= \frac{p(z_k=1) p(\mathbf{x}|z_k=1)}{\sum_j p(z_j=1) p(\mathbf{x}|z_j=1)} \\
&= \frac{\pi_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
{\sum_j \pi_j \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} \\
\end{align*}
$$

which is also called the responsibility.

### Maximum likelihood

### EM for Gaussian mixtures

Given a set of data $\mathbf{X} = (\mathbf{x}\_1, \dots, \mathbf{x}\_N)^{\mathsf{T}}$ distributed to Gaussian mixture,
the corresponding log likelihood function is given by

$$
\ln p(\mathbf{X}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})
= \sum_{n=1}^N \ln \left[ \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]
\tag{9.14}
$$

The maximum likelihood of Gaussian mixture models suffers from singularities.
Consider a isotropic component with the mean $\boldsymbol{\mu}\_j$ equal to some point $\mathbf{x}\_n$ and the covariance given by $\sigma\_j \mathbf{I}$.
The point's contribution to the likelihood function can be given by

$$
(2 \pi)^{-1/2} \sigma_j^{-D}
$$

In the limit $\sigma \rightarrow \infty$, this term will go to infinity and thus the likelihood function also goes to infinity.
It can be seen as an example of over-fitting and will not occur when we adopt a Bayesian approach.
Another method to avoid over-fitting is to reset corresponding mean to a random value and the variance to a larger value when singularity is detected, and then continue optimization.

Such singularity generally does not arise in single-Gaussian models because as the value of $\sigma$ decrease,
points that are not equal to the mean will contribute high-order small multiplicative factors to the likelihood function.

A further issue of maximum likelihood of mixture models is known as the **identifiability**.
A $K$-component mixture will have a total of $K!$ equivalent solutions, which corresponds to $K!$ ways to assign $K$ sets of parameters to $K$ components.

Due to the summation of the components, the solution of maximum likelihood is no longer analytical. 
We could resort to gradient-based optimization algorithms as before. Here we will use
an alternative approach - EM algorithm - to find the solution. 

## EM for Gaussian mixtures

The **expectation-maximization** algorithm is a method to find maximum likelihood solutions
for models with latent variables.
Here we consider the case of a mixture of Gaussians.

Setting derivative of the likelihood function (9.14) with respect to $\boldsymbol{\mu}\_k$ to zero, we obtain

$$
- \sum_{n=1}^N \frac{\pi_k \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
{\sum_j \pi_j \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
\boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_n - \boldsymbol{\mu}_k) = 0
$$

Solving for $\boldsymbol{\mu}\_k$ we obtain

$$
\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) \mathbf{x}_n
\tag{9.17}
$$

where we defined

$$
N_k = \sum_{n=1}^N \gamma(z_{nk})
$$

Note that the solution takes the form of weighted average of the data points. $N\_k$ can be interpreted as the effective number of points assigned to cluster $k$.

Settting derivative with respect to $\boldsymbol{\Sigma}\_k$, we obtain

$$
\boldsymbol{\Sigma}_k = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) (\mathbf{x}_n - \boldsymbol{\mu}_k) (\mathbf{x}_n - \boldsymbol{\mu}_k)^{\mathsf{T}}
\tag{9.19}
$$

To maximize with respect to $\pi\_k$, we should consider the constraint $\sum\_k \pi\_k = 1$,
which can be done by introducing a Lagrange multiplier $\lambda$ with the Lagragian given by

$$
\ln p(\mathbf{X}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) 
+ \lambda (\sum_{k=1}^K \pi_k - 1)
$$

Setting derivative with respect to $\pi\_k$ to zero, we have

$$
\sum_{n=1}^N  \frac{\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
{\sum_j \pi_j \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
+ \lambda = 0
$$

Multiplying both sides with $\pi\_k$ and summing over $k$, we obtain $\lambda = -N$.
Solving for $\pi\_k$ gives

$$
\pi_k = \frac{N_k}{N}
\tag{9.22}
$$

Note that these results are not closed-form solutions for parameters, since $\gamma(z\_{nk})$
itself depends on these parameters.
The EM algorithm to optimize these parameters is given by following steps

1. Initialize parameters $\{ \boldsymbol{\mu\_k}, \boldsymbol{\Sigma}\_k, \pi\_k \}$ and
evaluate the log likelihood.
2. E step: evaluate the responsibilities $\gamma(z\_{nk})$
3. M step: re-estimate parameters $\{ \boldsymbol{\mu\_k}, \boldsymbol{\Sigma}\_k, \pi\_k \}$ with the values of $\gamma(z\_{nk})$ evaluate in step 2.
4. Evaluate the log likelihood function. Terminate if the convergence criterion is met, otherwise go to step 2.

In practive, the algorithm is considered to be converged when the change in log likelihood or parameters is small enough.

We shall see the log likelihood is guaranteed to increase after each iteration.
However, it is not guaranteed to find the global maximum.

The EM algorithm generally has much higher computation cost than the $K$-means algorithm. 
It is common to find better initial values for parameters by running $K$-means before
the EM.

As with the gradient-based methods, EM does not avoid singularity of over-fitting component.
Additional techniques are reuquired to handle such singularity.

## An alternative view of EM

This section provides an alternative view of the EM algorithm.

Suppose that the data set is given by $\mathbf{X} = (\mathbf{x}\_1, \dots, \mathbf{x}\_N)^{\mathsf{T}}$ and the corresponding latent variables are given by $\mathbf{Z} = (\mathbf{z}\_1, \dots, \mathbf{z}\_N)^{\mathsf{T}}$.

Then the log likehood function

$$
\ln p(\mathbf{X}|\boldsymbol{\theta}) = \ln \left[ \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta}) \right]
$$

If the observations are drawn independently, it can be shown that

$$
\sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} | \boldsymbol{\theta}) = \prod_n \sum_{k} p(\mathbf{x}_n, \mathbf{z}_{nk}|\boldsymbol{\theta})
$$

If $\mathbf{z}$ is continuous, we simply replace summation with integration.

As we will see in the case of a mixture of Gaussians, due to the summation over $\mathbf{Z}$,
the marginal distribution $p(\mathbf{X}\|\boldsymbol{\theta})$ is generally not a member of exponential family even if $p(\mathbf{X}, \mathbf{Z})$ is.

Suppose for each observation, we are also told the value of corresponding latent variable.
The data set $\{ \mathbf{X}, \mathbf{Z} \}$ is called the **Complete** data set,
and $\mathbf{X}$ is called **incomplete**.
With complete data set, we shall suppose that the maximization of the joint likelihood $p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$ is straightforward.

However in practice, the values of latent variables in $\mathbf{Z}$ are not given.
The only knownledge about $\mathbf{Z}$ that we can derive from the data is the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta})$.
With it we can evaluate the expectation of log likelihood function $\ln p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$, which is given by

$$
\begin{align*}
\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})})
= \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{(old)}}) \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
\end{align*}
$$

where $\boldsymbol{\theta}^{(\text{old})}$ is the parameter with current values and $\boldsymbol{\theta}$ is parameter with undertermined values.
This corresponds to the E step.

In the M step, we maximize the expectation $\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})})$ with respect to $\boldsymbol{\theta}$.

A general EM for maximizing likelihood is given by following steps:

1. Initialize prameters $\boldsymbol{\theta}^{(\text{old})}$
2. E step: evaluate $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{(\text{old})})$
3. M step: find $\boldsymbol{\theta}^{(\text{new})}$ such that

$$
\boldsymbol{\theta}^{(\text{new})} = \underset{\boldsymbol{\theta}}{\operatorname{arg\,max}}
\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})})
$$

4. Terminate if converged. Otherwise assign $\boldsymbol{\theta}^{(\text{new})}$ to $\boldsymbol{\theta}^{(\text{old})}$ and go to step 2.

EM can also be applied on MAP. By Bayes' therem we have

$$
p(\boldsymbol{\theta} | \mathbf{X}, \mathbf{Z}) \propto p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
p(\boldsymbol{\theta})
$$

Therefore in E step, we evaluate the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{(\text{old})})$ to give the expectation

$$
\sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{(old)}}) \left[ \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}) \right]
= \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})}) + \ln p(\boldsymbol{\theta})
$$

And in M step, we maximize

$$
\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{(\text{old})}) + \ln p(\boldsymbol{\theta})
$$

Further discussion on EM will be present in Section 9.4.

### Gaussian mixture revisited

Consider maximization of likelihood of mixture of Gaussians, with complete i.d.d data set $\{ \mathbf{X}, \mathbf{Z} \}$.

The likelihood function is given

$$
\begin{align*}
p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})
&= \prod_{n=1}^N p(\mathbf{x}_n, \mathbf{z}_n|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \\
&= \prod_{n=1}^N  \prod_{k=1}^K \left[ \pi_k \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]^{z_{nk}} \\
\end{align*}
$$

Taking logarithm gives

$$
\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})
= \sum_{n=1}^N \sum_{k=1}^K z_{nk} \left[ \ln \pi_k + \ln \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]
$$

Compared to (9.14), it does not involve logarithm of sums, which leads to a simpler solution of maximum likelihood.

Suppose $z\_{nk}$ is known for the moment. Maximization with respect to $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ will be very much like the single-Gaussian model.
Maximization with respect to $\boldsymbol{\pi}$ with summation constraint, again can be
obtained using Lagarange multiplier, which gives

$$
\pi_k = \frac{1}{N} \sum_{n=1}^N z_{nk}
$$

To work around unknown latent variable values, we first show the posterior

$$
\begin{align*}
p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})
&= \frac{p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}
{p(\mathbf{X}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}  \\
&= \prod_{n=1}^N \frac{p(\mathbf{x}_n, \mathbf{z}_n|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}
{p(\mathbf{x}_n|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})}  \\
&= \prod_{n=1}^N p(\mathbf{z}_n|\mathbf{x}_n, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \\
\end{align*}
$$

from which we see the posterior distributions of $\{ \mathbf{z}\_n \}$ are independent.

The expectation of $z\_{nk}$ under the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})$ is then given by

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [z_{nk}]
&= \sum_{\mathbf{Z}} \left[ z_{nk} p(\mathbf{z}_n|\mathbf{x}_n) \prod_{i \ne n} p(\mathbf{z}_i|\mathbf{x}_i) \right] \\
&= \sum_{\mathbf{z}_n} z_{nk} p(\mathbf{z}_n|\mathbf{x}_n) \left[ \prod_{i \ne n} \sum_{\mathbf{z}_i} p(\mathbf{z}_i|\mathbf{x}_i) \right] \\
&= \sum_{\mathbf{z}_n} z_{nk} p(\mathbf{z}_n|\mathbf{x}_n) \\
&= p(z_{nk} = 1 |\mathbf{x}_n)
= \gamma(z_{nk}) \\
\end{align*}
$$

where we have omitted the condition on $\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}$ to keep notations uncluttered.
The result is the responsibility of component $k$ for points $\mathbf{x}\_n$.

The expectation of the complete-data log likelihood under the posterior is then given by

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]
&= \sum_{n=1}^N \sum_{k=1}^K \operatorname{E}_{\mathbf{Z}} [z_{nk}] \left[ \ln \pi_k + \ln \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right] \\
&= \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk}) \left[ \ln \pi_k + \ln \mathcal{N} (\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]
\end{align*}
$$

Applying the general EM, we first initialize $\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}$.
Then in the E step, we evaluate the responsibilities $\gamma(z\_{nk})$ with current parameter values.
In the M step, we maximize the expectation by setting derivatives to zero, which gives us closed-form solutions (9.17), (9.19) and (9.22).

### Relation to $K$-means

$K$-means is a limit form of EM of mixture Gaussians, in which we perform hard assigment of points instead of soft assignment based on probabilities.

Consider a Gaussian mixture model, in which compoenents share the same covariance matrix $\epsilon \mathbf{I}$, so that

$$
p(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
= \frac{1}{(2 \pi \epsilon)^{D/2}} \exp \left( - \frac{1}{2 \epsilon} \| \mathbf{x} - \boldsymbol{\mu}_k \|^2 \right)
$$

The responsibilities take the from

$$
\gamma(z_{nk}) = \frac{\pi_k \exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2 / 2 \epsilon \right)}
{\sum_j \pi_k \exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 / 2 \epsilon \right)}
$$

Let $\boldsymbol{\mu}\_k$ be the closest cluster center from $\mathbf{x}\_n$. Then in the limit $\epsilon \rightarrow 0$, we have

$$
\begin{align*}
&\quad \lim_{\epsilon \rightarrow 0} \frac{\exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 / 2 \epsilon \right)}{\exp \left( - \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2 / 2 \epsilon \right)} \\
&= \lim_{\epsilon \rightarrow 0} \left[ - \frac{1}{2 \epsilon} \left( \| \mathbf{x}_n - \boldsymbol{\mu}_j \|^2 - \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2 \right) \right] \\
&= \begin{cases}
1 & \text{ if } j = k \\
0 & \text{ otherwise}
\end{cases}
\end{align*}
$$

If there's only one closest center for point $\mathbf{x}\_n$ and $\pi\_k \ne 0$, we have

$$
\lim_{\epsilon \rightarrow 0} \gamma(z_{nk}) = \begin{cases}
1 & \text{ if } j = k \\
0 & \text{ otherwise} \\
\end{cases}
$$

which corresponds to the definition of $r\_{nk}$ in $K$-means, where we assign a point to
the closest cluster.

The expectation of complete-data log likelihood can be written

$$
\lim_{\epsilon \rightarrow 0} \operatorname{E}_{\mathbf{Z}} [\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]
= \lim_{\epsilon \rightarrow 0} \sum_{n=1}^N \sum_{k=1}^K r_{nk} \left[ \ln \pi_k - \frac{D}{2} \ln (2 \pi \epsilon) - \frac{1}{2 \epsilon} \| \mathbf{x}_n - \boldsymbol{\mu}_k \|^2   \right]
$$

The covariances are considered constant in the context of $K$-means and will not be estimated.

Maximization with respect to $\boldsymbol{\mu}$ is equivalent to minimizing the distortion measure $J$ in $K$-means.
And the mean estimated with (9.17) then reduces to the form (9.4) of $K$-means.

$\pi\_k$ is set to the fraction of points assigned to cluster $k$, which does not play a role
in $K$-means.

### Mixtures of Bernoulli distributions

Consider a $D$-dimensional random variable $\mathbf{x}$, with each component independently governed by a Bernoulli distribution, so that

$$
p(\mathbf{x}|\boldsymbol{\mu}) = \prod_{i=1}^D \mu_i^{x_i} (1 - \mu_i)^{1- x_i}
$$

The mean and covariance are then given by

$$
\begin{align*}
\operatorname{E} [\mathbf{x}] &= \boldsymbol{\mu} \\
\operatorname{cov} [\mathbf{x}] &= \operatorname{diag} \left( \mu_i (1-\mu_i) \right)
\end{align*}
$$

A mixture distribution consisting of $K$ such components is given by

$$
p(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\pi})
= \sum_{k=1}^K \pi_k p(\mathbf{x}|\boldsymbol{\mu}_k)
\tag{9.47}
$$

with

$$
p(\mathbf{x}|\boldsymbol{\mu}_k) = \prod_{i=1}^D \mu_{ki}^{x_{i}} (1 - \mu_{ki})^{1- x_{i}}
$$

The mean and covariance are given by

$$
\begin{align*}
\operatorname{E} [\mathbf{x}] &= \sum_{k=1}^K \boldsymbol{\mu}_k \\
\operatorname{cov} [\mathbf{x}]
&= \sum_{k=1}^K \pi_k \left( \boldsymbol{\Sigma}_k + \boldsymbol{\mu}_k \boldsymbol{\mu}_k^{\mathsf{T}} \right) - \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}}
\end{align*}
$$

where $\boldsymbol{\Sigma}\_k = \operatorname{diag} \left( \mu\_{ki} (1 - \mu\_{ki}) \right)$.

**Exercise 9.12**

This comes from a more general result of mixture distributions. Given a mixture distribution of the form

$$
p(\mathbf{x}) = \sum_{k=1}^K \pi_k p(\mathbf{x}|k)
$$

with the conditional mean and variance of each component denoted by $\boldsymbol{\mu}\_k$ and $\boldsymbol{\Sigma}\_k$ respectively.

The mean of mixture distribution is

$$
\operatorname{E} [\mathbf{x}] = \sum_k \pi_k \operatorname{E} [\mathbf{x}|k] = \sum_{k=1}^K \boldsymbol{\mu}_k
$$

and the covaraince is

$$
\begin{align*}
\operatorname{cov} [\mathbf{x}] &= \operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} \right] - \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}} \\
&=  \sum_k \pi_k \operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} | k \right]
- \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}} \\
&= \sum_k \pi_k \left( \boldsymbol{\Sigma}_k + \boldsymbol{\mu}_k \boldsymbol{\mu}_k^{\mathsf{T}} \right)
- \operatorname{E}[\mathbf{x}] \operatorname{E}[\mathbf{x}]^{\mathsf{T}} \\
\end{align*} 
$$

\
Given a incomplete data set $\mathbf{X} = \{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$, the log likelihood is given by

$$
\ln p(\mathbf{X}|\boldsymbol{\mu}, \boldsymbol{\pi})
= \sum_{n=1}^N \ln \left[ \sum_{k=1}^K \pi_k p(\mathbf{x}_n|\boldsymbol{\mu}_k) \right]
$$

which again involves logarithm of summation. Thus the maximum likelihood solution no longer has  closed form.

As with the Gaussian mixture model, we introduce a latent variable $\mathbf{z} = (z\_1, \dots, z\_K)$, with 1-of-$K$ binary coding scheme, so that

$$
p(\mathbf{x}|\mathbf{z}, \boldsymbol{\mu}) = \prod_{k=1}^K p(\mathbf{x}|\boldsymbol{\mu}_k)^{z_k}
$$

and

$$
p(\mathbf{z}|\boldsymbol{\pi}) = \prod_{k=1}^K \pi_k^{z_k}
$$

Marginalizing out $\mathbf{z}$, we obtain (9.47).

The complete-data log likelihood is then given by

$$
\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\pi})
= \sum_{n=1}^N \sum_{k=1}^K z_{nk} \left\{ \ln \pi_k + \sum_{i=1}^D \left[ x_{ni} \ln \mu_{ki} + (1-x_{ni}) \ln (1-\mu_{ki}) \right] \right\}
$$

The expectation of $z\_{nk}$ under the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\pi})$ is given

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [z_{nk}]
&= p(z_{nk} = 1|\mathbf{x}_n, \boldsymbol{\mu}, \boldsymbol{\pi}) \\
&= \frac{\pi_k p(\mathbf{x}_n|\boldsymbol{\mu}_k)} {\sum_j \pi_j p(\mathbf{x}_n|\boldsymbol{\mu}_j)}
\equiv \gamma(z_{nk})
\end{align*}
$$

Therefore the expectation of the complete-data log likelihood is given

$$
\begin{align*}
\operatorname{E}_{\mathbf{Z}} [\ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\mu}, \boldsymbol{\pi})]
= \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk}) \left\{ \ln \pi_k + \sum_{i=1}^D \left[ x_{ni} \ln \mu_{ki} + (1-x_{ni}) \ln (1-\mu_{ki}) \right] \right\}
\end{align*}
$$

Setting derivative with respect to $\boldsymbol{\mu}\_k$ to zero, we obtain

$$
\boldsymbol{\mu}_k = \overline{\mathbf{x}}_k
= \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) \mathbf{x}_n
$$

where we defined

$$
N_k = \sum_{n=1}^N \gamma(z_{nk})
$$

which can be interpreted as the effective number of parameters.

The solution of $\pi\_k$ is given by

$$
\pi_k = \frac{N_k}{N}
$$

Unlike mixture of Gaussians, there is no singularity in which the likelihood function goes to infinity,
since $0 \le p(\mathbf{x}\_n\|\boldsymbol{\mu}\_k) \le 1$.
However, bad choice of initialization values can result in convergence to undesired stationary point. Consider an example where all component means are initialized to the same value. Then in the E step, we have

$$
\gamma(z_{nk}) = \pi_k
$$

The maximum likelihood solution is given by

$$
\boldsymbol{\mu}_k = \frac{1}{N} \sum_n \mathbf{x}_n
$$

which again takes equal values for all component means. Hence the final solution will converge to $\frac{1}{N} \sum\_n \mathbf{x}\_n$.

TODO:

- digit example
- MAP
- Categorial distribution

### EM for Bayesian linear regression

## The EM algorithm in general

[References]

- Neal, R.M., Hinton, G.E. (1998). A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants.

Given a set of observations $\mathbf{X}$ and corresponding latent variables $\mathbf{Z}$,
our goal is to maximize the likelihood function

$$
p(\mathbf{X} | \boldsymbol{\theta}) = \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
$$

where $\boldsymbol{\theta}$ is a set of parameters. Here we assumed $\mathbf{z}$ is discrete.
Generalization to continuous variables is straightforward.

Introducing a distribution over latent variables $q(\mathbf{Z})$, we give the following decomposition

$$
\ln p(\mathbf{X} | \boldsymbol{\theta}) = \mathcal{L}(q, \boldsymbol{\theta}) + \operatorname{KL}(q||p)
\tag{9.70}
$$

where

$$
\begin{align*}
\mathcal{L}(q, \boldsymbol{\theta}) &= \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left[ \frac{p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})}{q(\mathbf{Z})} \right] \tag{9.71}\\
\operatorname{KL}(q||p) &= \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left[ \frac{p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})} \right] \tag{9.72} \\
\end{align*}
$$

> Motivation: negative free energy in statistical physics

$\mathcal{L}(q, \boldsymbol{\theta})$ is a functional of $q(\mathbf{Z})$ and a function of $\boldsymbol{\theta}$.
$\operatorname{KL}(q\|\|p)$ is the Kullback-Leibler divergence.

From an alternatice view of two-stage maximization, we show that the EM indeed increases the value of likelihood.

Let $\boldsymbol{\theta}^{\text{old}}$ be the current value of parameters. 
Recalling that $\operatorname{KL}(q\|\|p) \ge 0$, we see $\mathcal{L}(q, \boldsymbol{\theta}^{\text{old}})$ is a lower bound of $\ln p(\mathbf{X} \| \boldsymbol{\theta})$.
Since $\ln p(\mathbf{X} \| \boldsymbol{\theta})$ is independent of $q$, $\mathcal{L}(q, \boldsymbol{\theta}^{\text{old}})$ is maximized when $\operatorname{KL}(q\|\|p) = 0$, which gives $q(\mathbf{Z}) = p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$.
This corresponds to the E step.

In the second stage, while holding $q(Z)$ fixed, we maximize $\mathcal{L}(q, \boldsymbol{\theta})$ with respect to $\boldsymbol{\theta}$.
Unless the maximum is already reached, this will raise the lower bound $\mathcal{L}$.
Also changes in $\boldsymbol{\theta}$ will result in change of $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta})$,
so that $q(\mathbf{Z}) = p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ne p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{new}})$ and leads to $\operatorname{KL}(q\|\|p) > 0$ again.

Substituting $q(\mathbf{Z})$ with $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$ in (9.71), we obtain

$$
\begin{align*}
\mathcal{L}(q, \boldsymbol{\theta})
&= \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})
- \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \\
&= \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})
+ \operatorname{H} \left[ \mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}} \right]\\
\end{align*}
$$

where $\operatorname{H} \left[ \mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}} \right]$ is the conditional entropy of $\mathbf{Z}$ and is independent of $\boldsymbol{\theta}$.
Therefore, maximization $\mathcal{L}(q, \boldsymbol{\theta})$ is equivalent to maximize $\mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})$,
which corresponds to the M step in EM algorithm.

When $p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$ is a member of exponential family or the product of them,
$\ln p(\mathbf{X}, \mathbf{Z}\|\boldsymbol{\theta})$ generally gives a simpler optimization objective than that obtained by directly working with $p(\mathbf{X}\|\boldsymbol{\theta})$.

**View from parameter space**

The EM algorithm can also be view from the parameter space. With the current value of parameters $\boldsymbol{\theta}^{\text{old}}$,
we evaulate the posterior $p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$.
Setting $q(\mathbf{Z}) = p(\mathbf{Z}\|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})$,
we have $\mathcal{L}(q, \boldsymbol{\theta}^{\text{old}}) = \ln p(\mathbf{X}\|\boldsymbol{\theta}^{\text{old}})$.

Moreover

$$
\begin{align*}
\frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{L}(q, \boldsymbol{\theta})
&= \frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}) \\
&= \sum_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})
\frac{\partial}{\partial \boldsymbol{\theta}} \left[ \ln p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}) + \ln p(\mathbf{X}|\boldsymbol{\theta}) \right] \\
&= \frac{\partial}{\partial \boldsymbol{\theta}} \ln p(\mathbf{X}|\boldsymbol{\theta})
+ \sum_{\mathbf{Z}} \frac{p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta}^{\text{old}})}{p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta})} 
\frac{\partial}{\partial \boldsymbol{\theta}} p(\mathbf{Z}|\mathbf{X}, \boldsymbol{\theta})
\end{align*}
$$

Taking $\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}$, we obtain

$$
\begin{align*}
\frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{L}(q, \boldsymbol{\theta}) \bigg|_{\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}}
&= \frac{\partial}{\partial \boldsymbol{\theta}} \ln p(\mathbf{X}|\boldsymbol{\theta}) \bigg|_{\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}}
\end{align*}
$$

which means $\mathcal{L}(q, \boldsymbol{\theta})$ and $p(\mathbf{X}\|\boldsymbol{\theta})$ make tangential contact at $\boldsymbol{\theta}^{\text{old}}$ where they have the same gradient.
In the M step, maximization of $\mathcal{L}$ with respect to $\boldsymbol{\theta}$ is convex for components of exponential family,
which will raise the lower bound as well as increase the log likelihood.

**EM for MAP**

EM can also be applied on MAP where we maximize $p(\mathbf{X} \| \boldsymbol{\theta})$ with a prior $p(\boldsymbol{\theta})$. By

$$
\ln p(\boldsymbol{\theta}|\mathbf{X}) = \ln p(\mathbf{X}|\boldsymbol{\theta})
+ \ln p(\boldsymbol{\theta}) - \ln p(\mathbf{X})
$$

and (9.70), we have

$$
\ln p(\boldsymbol{\theta}|\mathbf{X}) = \mathcal{L}(q, \boldsymbol{\theta}) + \operatorname{KL}(q||p) 
+ \ln p(\boldsymbol{\theta}) - \ln p(\mathbf{X})
$$

Note that $p(\mathbf{X})$ is constant. Then the E step remains the same as the maximum likelihood. 
The M step involves an additional term $\ln p(\boldsymbol{\theta})$ to be considered.

TODO: **Generalized EM**



