---
layout: posts
title: 'PRML Ch8 - Graphical Models'
---
- [Graphical models](#graphical-models)
  - [Bayesian networks](#bayesian-networks)
    - [Example: polynomial regression](#example-polynomial-regression)
    - [Generative models](#generative-models)
    - [Discrete variables](#discrete-variables)
    - [Linear-Gaussian models](#linear-gaussian-models)
  - [Conditional independnece](#conditional-independnece)
    - [Three example cases](#three-example-cases)
    - [D-Separation](#d-separation)
  - [Markov random fields](#markov-random-fields)
    - [Conditional independence properties](#conditional-independence-properties)
    - [Factorization properties](#factorization-properties)
    - [Illustration: image de-noising](#illustration-image-de-noising)
# Graphical models


## Bayesian networks

An arbitrary joint distribution $p(a,b,c)$ can be written in the form

$$
p(a,b,c) = p(c|a,b) p(b|a) p(a)
$$

The graph representation of the r.h.s is shown as

![](/assets/images/prml/fig-8.1.png)

A graphical model is constructed by two steps:

1. Represent each random variable by a node
2. Add directed eadges from any node a to any node b if b is conditioned on a

Given a joint distribution over $K$ variables $p(x\_1, \dots, x\_K)$, we can write

$$
p(x_1, \dots, x_K) = p(x_K | x_{K-1}, \dots, x_1) \dots p(x_2|x_1) p(x_1)
$$

The resulting graph will be such that every node sends an outbound edge to each higher numberd nodes.
It is said to be fully connected since there's a link between every pair of nodes.

Given a graph

![](/assets/images/prml/fig-8.2.png)

the distribution is uniquely defined as

$$
p(x_1) p(x_2) p(x_3) p(x_4|x_1, x_2, x_3) p(x_5|x_1, x_3) p(x_6|x_4) p(x_7|x_4, x_5)
$$

Formally for a graph with $K$ nodes, the joint distribution is given by

$$
p(\mathbf{x}) = \prod_{k=1}^K p(x_k|\operatorname{pa}_k)
$$

where $\operatorname{pa}\_k$ are the set of parents of node $k$ and $\mathbf{x} = \{ x\_1, \dots, x\_K \}$.

Note that, a valid probability graph must be a DAG.

### Example: polynomial regression

### Generative models

Since the graphical models captures the **casual** process by which the observations are generated, they are often called generative models.
An example of image generation:

![](/assets/images/prml/fig-8.8.png)


Given a joint distribution $p(x\_1, \dots, x\_K)$ and its graph, the **ancestral sampling** samples random variables one by one in a topological order.

Typically the terminal nodes correspond to the ovserved variables and the internal nodes correspond to the latent (or hidden) variables.

### Discrete variables

Consider two discrete variables $\mathbf{x}\_1$ and $\mathbf{x}\_2$, each of which has $K$ states. We denote the probability of ${x}\_{1k} = {x}\_{2l} = 1$ by $\mu\_{kl}$.
The joint probability is then given by

$$
p(\mathbf{x}_1, \mathbf{x}_2 | \boldsymbol{\mu}) = \prod_{k=1}^K \prod_{l=1}^K \mu_{kl}^{x_{1k} x_{2l}}
$$

where $\sum\_{k,l} \mu\_{kl} = 1$. This distribution is governed by $K^2 -1$ parameters.
For a distribution consisting of $M$ such discrete variables, which corresponds to a fully-connected graph, the total parameters will be $K^M - 1$.

If we write the joint distribution in the form

$$
\mathbf{x}_1 \rightarrow \mathbf{x}_2
$$

then $p(\mathbf{x}\_1)$ is governed $K-1$ parameters. And for each state $\mathbf{x}\_1$ takes,
$\mathbf{x}\_2$ is governed $K-1$ independent parameters, which gives $p(\mathbf{x}\_2\|\mathbf{x}\_1)$ a total of $K(K-1)$ parameters. 
The total number of parameters in the model is still $K^2 - 1$.

We can reduce the number of parameters in several ways at the expanse of restricted model.

The simplest way is to drop all dependences of between variables, which gives a model

$$
p(\mathbf{x}_1, \mathbf{x}_2) = p(\mathbf{x}_1) p(\mathbf{x}_2)
$$

Then a model with $M$ variables has $M(K-1)$ parameters.

A stronger connectivity is achieved by the model

![](/assets/images/prml/fig-8.10.png)

where the distribution $p(\mathbf{x}\_1)$ has $K-1$ parameters, and each $p(\mathbf{x}\_i\|\mathbf{x}\_{i-1})$ has $K(K-1)$ parameters, giving a total of $K-1 + (M-1) K (K-1)$ parameters.

An alternative way to reduce the number of parameters is by sharing (a.k.a. tying) parameters.
E.g. by making $p(\mathbf{x}\_i\|\mathbf{x}\_{i-1})$ where $i=2,\dots, M$ share the same set of $K(K-1)$ parameters, we reduce the number of total parameters to $K(K-1)$.

TODO

### Linear-Gaussian models

A multivariate Gaussian can be expressed as a directed graph of a linear-Gaussian model over the components.

Consider a $D$-dimensional Gaussian variable $\mathbf{x}$, with each component defined by a graphical model

$$
p(x_i|\operatorname{pa}_i) = \mathcal{N} (x_i | \sum_{j \in \operatorname{pa}_i} w_{ij} x_j + b_i, v_i)
$$

where the mean is a linear combination of its parents and $v\_i$ is the variance.

The log of the joint distribution is then written

$$
\begin{align*}
\ln p(\mathbf{x}) &= \sum_{i=1}^D p(x_i|\operatorname{pa}_i) \\
&= - \sum_{i=1}^D \frac{1}{2 v_i} \left( x_i - \sum_{j \in \operatorname{pa}_i} w_{ij} x_j - b_i \right)^2 + \text{const} \\
\end{align*}
$$

where the $\text{const}$ denotes terms independent of $\mathbf{x}$. We verify that the joint distribution is a Gaussian.

To determine the mean and covariance of the joint distribution, we first write

$$
x_i = \sum_{j \in \operatorname{pa}_i} w_{ij} x_j + b_i + \sqrt{v_i} \epsilon_i
$$

where $\epsilon\_i$ follows the unit Gaussian $\mathcal{N} (0, 1)$. Taking the expectation, we have

$$
\operatorname{E} [x_i] = \sum_{j \in \operatorname{pa}_i} w_{ij} \operatorname{E} [x_j] + b_i 
\tag{8.15}
$$

The covariace can be written

$$
\begin{align*}
\operatorname{cov} [x_i, x_j]
&= \operatorname{E} \left[ (x_i - \operatorname{E} [x_i]) (x_j - \operatorname{E} [x_j]) \right] \\
&= \operatorname{E} \left[ (x_i - \operatorname{E} [x_i]) \left( \sum_{k \in \operatorname{pa}_j} w_{jk} (x_k - \operatorname{E} [x_k])  + \sqrt{v_j} \epsilon_j \right) \right] \\
&= \sum_{k \in \operatorname{pa}_j} w_{jk} \operatorname{cov} [x_i, x_k] + I_{ij} v_j
\tag{8.16}
\end{align*}
$$

Here we used $\operatorname{E} [\epsilon\_i \epsilon\_j] = I\_{ij}$.

The values can then be calculated in following procedure:

1. Topologically sort the nodes. Assume that the result is $x\_1, \dots, x\_D$.
2. Starting from $i=1$, we evaluate $\operatorname{E} [x\_i]$ with (8.15). For nodes with no parents, we have $\operatorname{E} [x\_i] = b\_i$.
3. Starting from $i = 1$, we evaluate $\operatorname{cov} [x\_i, x\_j]$ with (8.16), where $j \le i$. For any node, we have $\operatorname{cov} [x\_i, x\_i] = v\_i$. Note that $\operatorname{cov} [x\_i, x\_j] = \operatorname{cov} [x\_j, x\_i]$.

In the particular case where all components are independent, i.e. the graph has no edges, we have

$$
\begin{align*}
\operatorname{E} [\mathbf{x}] &= (b_1, \dots, b_D)^{\mathsf{T}} \\
\operatorname{cov} [\mathbf{x}] &= \operatorname{diag} (v_1, \dots, v_D) \\
\end{align*}
$$

If the graph is fully connected, there will be $D(D-1)/2$ edges, which corresponds to the same number of $\{w\_{ij}\}$.
The total number of parameters in the covariance matrix is therefore $D(D+1)/2$ corresponding to a general covariance matrix.

A intermediate level of connectivity can be achived by the model

$$
x_1 \rightarrow x_2 \rightarrow \cdots \rightarrow x_D
$$

which gives $2D - 1$ parameters in the covariance matrix and a total of $3D-1$ independen parameters in the model.

The linear-Gaussian model can be extended to the case in which each node is a multivariate Gaussian variable.

$$
p(\mathbf{x}_i | \operatorname{pa}_i) = \mathcal{N} \left( \mathbf{x}_i | \sum_{j \in \operatorname{pa}_i} \mathbf{W}_{ij} \mathbf{x}_j , \boldsymbol{\Sigma}_i \right)
$$

## Conditional independnece

Given randome variables $a, b$ and $c$, if

$$
p(a|b,c) = p(a|c)
$$

we say $a$ is conditionally independent of $b$ given $c$. This can be equivalently expressed as

$$
p(a,b|c) = p(a|b,c) p(b|c) = p(a|c) p(b|c)
$$

It can be written in the notation

$$
a \perp\!\!\!\perp b \mid c
$$

Note that, the dependence must hold for all values of $c$.

### Three example cases

Consider the joint distribution defined by

$$
p(a, b, c) = p(a|c) p(b|c) p(c)
$$

which is shown by the graphical model

$$
a \leftarrow c \rightarrow b
$$

Without observing $c$, the dependence between $a$ and $b$ can be infered by marginalizing $c$, so that

$$
p(a, b) = \sum_c p(a|c) p(b|c) p(c)
$$

which is generally not equivalent to $p(a) p(b)$. So we have

$$
a \not\!\perp\!\!\!\perp b \mid \empty
$$

where $\empty$ denotes the empty set.

If $c$ is observed, we have

$$
p(a, b | c) = \frac{p(a,b,c)}{p(c)} = p(a|c) p(b|c)
$$

Thereby

$$
a \perp\!\!\!\perp b \mid c
$$

The graph is shown

![](/assets/images/prml/fig-8.16.png)

The node c is said to be **tail-to-tail** with respect to the path between $a$ and $b$ because both arrows starts from node c.

\
Consider the second case where

$$
p(a,b,c) = p(a) p(c|a) p(b|c)
$$

Corresponding graphical model is shown by

$$
a \rightarrow c \rightarrow b
$$

Without $c$ observed, the dependence between $a$ and $b$ can be seen from

$$
p(a, b) = p(a) \sum_c p(c|a) p(b|c)
$$

which is generally not equivalent to $p(a) p(b)$. So we have

$$
a \not\!\perp\!\!\!\perp b \mid \empty
$$

If $c$ is observed, then

$$
p(a, b | c)  = \frac{p(a,b,c)}{p(c)} = \frac{p(a) p(c|a) p(b|c)}{p(c)}
= p(a|c) p(b|c)
$$

which indicates

$$
a \perp\!\!\!\perp b \mid c
$$

![](/assets/images/prml/fig-8.18.png)

The node c is said to be **head-to-tail**.

\
The third case is the joint distribution defined by

$$
p(a,b,c) = p(a) p(b) p(c|a,b)
$$

Corresponding graph is shown by

$$
a \rightarrow c \leftarrow b
$$

Marginalizing out $c$ we have

$$
p(a, b) = p(a) p(b)
$$

Therefore

$$
a \perp\!\!\!\perp b \mid \empty
$$

If $c$ is observed, then

$$
p(a,b | c) = \frac{p(a,b,c)}{p(c)} = \frac{p(a) p(b) p(c|a,b)}{p(c)}
$$

which is generally not equivalent to $p(a) p(b)$. So we have

$$
a \not\!\perp\!\!\!\perp b \mid c
$$

![](/assets/images/prml/fig-8.20.png)

The node c is said to be **head to head**.

It can be shown that a head-to-head path will become blocked if either the head-to-head node or any of its descendents is observed.

### D-Separation

Let $A$, $B$ and $C$ be disjoint subsets of a graphical model. The union of the three sets does not need to includes all nodes in the model.
The statements

$$
A \perp\!\!\!\perp B \mid C
$$

which is read $A$ is d-separated from $B$ by $C$, holds if all paths from any node in $A$ to any node in $B$ is blocked by a node in $C$.

In d-separation, parameters in the model is treated as observed nodes. These nodes do not have parents and paths through them are always tail-to-tail and hence are blocked.

Consider a $D$-dimensional observation $\mathbf{x} = (x\_1, \dots, x\_D)^{\mathsf{T}}$.
The **naive Bayes** assumption states that $x\_1, \dots, x\_D$ are independent when conditioned on some randome variable $\mathbf{z}$.
This assumption simplies the model when $D$ is large and also allows different distributions to model the each component.
In the case of classfication problem, $\mathbf{z}$ is a 1-of-$K$ encoded vector indicating the calss.

It can be shown that, the set of all conditional dependences a graph implies is equivalent to the factorization of the graph,
in a sense that both types of constraints define the same set of distributions.

## Markov random fields

A.k.a Markov network or undirected graphical model.

### Conditional independence properties

Define a undirected graphical model where the conditional independence

$$
A \perp\!\!\!\perp B \mid C
$$

holds if all paths from any node in A to any node in B pass through a node in $C$.

![](/assets/images/prml/fig-8.27.png)

This kind of graphs breaks the asymmetry between nodes and thus has a simpler criterion to determine the conditional independence.

### Factorization properties

To find the factorization of such graph, we first note that any $x\_i$ and $x\_j$ that are not connected directly should satisfy

$$
x_i \perp\!\!\!\perp x_j \mid x_{-\{i, j\}}
$$

where $x\_{-\{i, j\}}$ denotes the set of nodes with $x\_i$ and $x\_j$ removed.
Therefore $x\_i$ and $x\_j$ cannot appear in the same factor. For instance, both $p(x\_i\|\dots, x\_j, \dots)$ and $p(x\_j\|\dots, x\_i, \dots)$ are not valid.

The factors can thus be defined as functions over cliques. A clique is a subgraph in which each node connects to others.
E.g., $\{ x\_2, x\_3, x\_4\}$ and $\{ x\_1, x\_2\}$ both are cliques but $\{x\_1, x\_2, x\_4\}$ is not a clique.
A maximal clique is defined as a maximum set of nodes that forms a clique. E.g., $\{x\_1, x\_2\}$ is not a maximal clique.

![](/assets/images/prml/fig-8.29.png)

This gives the factorization

$$
p(\mathbf{x}) = \frac{1}{Z} \prod_{C} \psi_C (\mathbf{x}_C)
$$

where $C$ is a clique of the graph, $\psi\_C$ is the **potential function** over the clique $C$, and $\mathbf{x}\_C$ is the set of nodes of $C$.

The potential function can be interpreted generally as the compatability of nodes within the clique.

The quantity $Z$, called the partition function, is a normalization constant defined as

$$
Z = \sum_{\mathbf{x}} \prod_{C} \psi_C (\mathbf{x}_C)
$$

The factorization of the graph above can be taken as

$$
p(\mathbf{x}) = \psi_{234} (\{x_2, x_3, x_4\}) +  \psi_{12} (\{x_1, x_2\})
+ \psi_{13} (\{x_1, x_3\})
$$

It can also be represented in the form that contains maximal cliques only

$$
p(\mathbf{x}) = \psi_{234} (\{x_2, x_3, x_4\}) +  \psi_{123} (\{x_1, x_2, x_3\})
$$

The potential function is constrained to $\psi\_C (\mathbf{x}\_C) \ge 0$ so that $p(\mathbf{x}) \ge 0$.
However, it is not forced to have a probabilistic interpretation, which is dinstinct from directed garphs.

As with the directed graph, the Hammersley-Clifford theorem states that the set of conditional independences that can be infered from a graph is equivalent to the factorization defined by the graph.

The potential function can be defined as an exponential function such that

$$
\psi_C (\mathbf{x}_C) = \exp \{ - E(\mathbf{x}_C) \}
$$

where $E(\mathbf{x}\_C)$ is called an energy function. By taking the negative logarithm of $p(\mathbf{x})$,
we see the total energy is obtained by summing up the energy of each clique.

### Illustration: image de-noising



