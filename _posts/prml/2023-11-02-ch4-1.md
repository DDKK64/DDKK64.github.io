---
layout: posts
title: 'PRML Ch4 - Linear Models for Classification - Part I'
---
- [Linear Models for Classification](#linear-models-for-classification)
  - [Discriminant Functions](#discriminant-functions)
    - [Two classes](#two-classes)
    - [Multiple Classes](#multiple-classes)
    - [Least squares for classification](#least-squares-for-classification)
    - [Fisher's linear discriminant](#fishers-linear-discriminant)
    - [Relation to least squares](#relation-to-least-squares)
    - [Fisher's discriminant for multiple classes](#fishers-discriminant-for-multiple-classes)
    - [The perceptron algorithm](#the-perceptron-algorithm)
  - [Probability generative models](#probability-generative-models)
    - [Continuous inputs](#continuous-inputs)
    - [Maximum likelihood solution](#maximum-likelihood-solution)
    - [Discrete features](#discrete-features)
    - [Exponential family](#exponential-family)

# Linear Models for Classification

## Discriminant Functions

Linearly separable: data sets that can be exactly separated by linear decision boundaries.

Discriminant functions: functions that assign an input $\mathbf{x}$ to a specific class.

Linear discriminants: decision surfaces are hyperplanes.

### Two classes

A simple decision boundary can be defined

$$
y(\mathbf{x}) = \mathbf{w}^{\mathsf {T}} \mathbf{x} + w_0.
$$

by which an input $\mathbf{x}$ is assigned to $C\_1$ if $y(\mathbf{x}) \ge 0$ and to $C\_2$ otherwise.

Note that $\mathbf{w}$ is orthogonal to the decision boundary, since $\mathbf{w}^{\mathsf{T}} (\mathbf{x}\_A - \mathbf{x}\_B) = \mathbf{0}$ for any points $\mathbf{x}\_A$ and $\mathbf{x}\_B$ on the surface.

Let $\mathbf{x}\_{\perp}$ be the orthogonal projection of $\mathbf{x}$ on the surface, then we have

$$
\begin{align*}
r \frac{\mathbf{w}}{\left\| \mathbf{w} \right\|} &= \mathbf{x} - \mathbf{x}_{\perp} \\
r \frac{\mathbf{w}^{\mathsf{T}} \mathbf{w}}{\left\| \mathbf{w} \right\|} &= \mathbf{w}^{\mathsf{T}} \mathbf{x} - \mathbf{w}^{\mathsf{T}} \mathbf{x}_{\perp} \\
r &= \frac{y(\mathbf{x})}{\left\| \mathbf{w} \right\|}  \\
\end{align*}
$$

where $r$ is the signed distance from the hyperplane to $\mathbf{x}$.
We also see that $\mathbf{w}$ points to the region of $y(\mathbf{x}) > 0$.

Introducing an dummy term $x\_0 = 1$ and defining $\widetilde{\mathbf{w}} = (w\_0, \mathbf{w}^{\mathsf{T}})^{\mathsf{T}}$ and $\widetilde{\mathbf{x}} = (x\_0, \mathbf{x}^{\mathsf{T}})^{\mathsf{T}}$
the discriminant function is collectively written

$$
y(\mathbf{x}) = \widetilde{\mathbf{w}}^{\mathsf {T}} \widetilde{\mathbf{x}}
$$

### Multiple Classes

We can build multi-class classifier from multiple binary classfifiers in two approaches:

- One-versus-the-rest classifier: $K-1$ binary classifiers. 
- One-versus-one classfifier: $K(K-1)/2$ binary classifiers.

However they both have ambiguous regions.

![](/assets/images/prml/fig-4.2.png)

Ambiguous regions can be eliminated by using one linear function for each class, so that for class $C\_k$

$$
y_k (\mathbf{x}) = \mathbf{w}_k^{\mathsf{T}} \mathbf{x} + w_{k0}
$$

The class that a point $\mathbf{x}$ belongs to is determined by

$$
k = \underset{j}{\operatorname{arg\,max}} \, y_j(\mathbf{x})
$$

The boundary between two classes $C\_j$ and $C\_k$ is then given by $y\_j (\mathbf{x}) = y\_k (\mathbf{x})$.
It can be shown that decision regions obtained from such discriminant are connected and convex.

Let $\mathbf{x}\_A$ and $\mathbf{x}\_B$ be any two points in region $R\_k$.
Define a point $\widehat{\mathbf{x}}$ such that

$$
\widehat{\mathbf{x}} = \lambda \mathbf{x}_A + (1-\lambda) \mathbf{x}_B
$$

where $0 \le \lambda \le 1$. Then we have

$$
y_k (\widehat{\mathbf{x}}) = \lambda y_k (\mathbf{x}_A) + (1-\lambda) y_k (\mathbf{x}_B)
$$

Since $\mathbf{x}\_A$ and $\mathbf{x}\_B$ are in $R\_k$, we have $y\_k (\mathbf{x}\_A) > y\_j (\mathbf{x}\_A)$ and $y\_k (\mathbf{x}\_B) > y\_j (\mathbf{x}\_B)$ for all $j \ne k$.
Therefore, $y\_k (\widehat{\mathbf{x}}) > y\_j (\widehat{\mathbf{x}})$ and $\widehat{\mathbf{x}}$ lies in the convex region $R\_k$.

### Least squares for classification

Consider a $K$-class classification problem, with 1-of-K coding scheme for the target vector $\mathbf{t}$.

The motivation to use the least squares is that the optimal solution is given by $\operatorname{E} [\mathbf{t}\|\mathbf{x}]$.

Suppose each class $C\_k$ has a linear model given by

$$
y_k (\mathbf{x}) = \mathbf{w}_k^{\mathsf{T}} \mathbf{x} + w_{k0}
$$

where $k=1, \dots, K$. Collectively, these functions can be written as

$$
\mathbf{y} (\mathbf{x}) = \widetilde{\mathbf{W}}^{\mathsf{T}} \widetilde{\mathbf{x}}
$$

in which

$$
\widetilde{\mathbf{W}}_{(D+1) \times K} = (\widetilde{\mathbf{w}}_1, \dots, \widetilde{\mathbf{w}}_K)
$$

An input $\mathbf{x}$ is then assigned to the class corresponding the largest $y\_k$.

Defining

$$
\begin{align*}
\mathbf{T}_{N \times K} &= \left( \mathbf{t}_1, \dots,  \mathbf{t}_N \right)^{\mathsf{T}} \\
\widetilde{\mathbf{X}}_{N \times (D+1)} &= \left( \widetilde{\mathbf{x}}_1, \dots, \widetilde{\mathbf{x}}_N \right)^{\mathsf{T}} \\
\end{align*}
$$

then the sum-of-squares error gives

$$
\begin{align*}
E_D(\widetilde{\mathbf{W}})
= \frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K \left( y_k(\mathbf{x}_n) - t_{nk} \right)^2
= \frac{1}{2} \operatorname{Tr} \left\{ (\widetilde{\mathbf{X}} \widetilde{\mathbf{W}} - \mathbf{T})^{\mathsf{T}} (\widetilde{\mathbf{X}} \widetilde{\mathbf{W}} - \mathbf{T}) \right\}
\end{align*}
$$

> Frobenius inner product

Setting derivative with respect to $\widetilde{\mathbf{W}}$ to zero, we obtain

$$
\widetilde{\mathbf{W}}
= (\widetilde{\mathbf{X}}^{\mathsf{T}} \widetilde{\mathbf{X}})^{-1} \widetilde{\mathbf{X}}^{\mathsf{T}} \mathbf{T}
= \widetilde{\mathbf{X}}^\dagger \mathbf{T}
\tag{4.16}
$$

**Excercise 4.2**

It can be shown that, with 1-of-$K$ binary coding scheme, a dummy input $x\_0 = 1$ and the paramter given by (4.16), the sum of elements of $\mathbf{y} (\mathbf{x})$ will be 1. 

Suppose that the target vectors satisfy some linear equation

$$
\mathbf{a}^{\mathsf{T}} \mathbf{t}_n + b = 0, \quad n=1, \dots, N
$$

Then the discriminant function under the same linear transformation gives

$$
\begin{align*}
\mathbf{a}^{\mathsf{T}} \mathbf{y} (\mathbf{x}) + b
&= \mathbf{a}^{\mathsf{T}} \mathbf{T}^{\mathsf{T}} \widetilde{\mathbf{X}} (\widetilde{\mathbf{X}}^{\mathsf{T}} \widetilde{\mathbf{X}})^{-1} \widetilde{\mathbf{x}} + b \\
&= b \left\{ - \mathbf{1}^{\mathsf{T}} \widetilde{\mathbf{X}} (\widetilde{\mathbf{X}}^{\mathsf{T}} \widetilde{\mathbf{X}})^{-1} \widetilde{\mathbf{x}} + 1 \right\} \\
\end{align*}
$$

Assume that the columns of $\widetilde{\mathbf{X}}$ are linearly independent.
Conducting QR-decomposition on $\widetilde{\mathbf{X}}$, we have

$$
\widetilde{\mathbf{X}} = Q R
$$

in which $R$ is an invertible matrix and $Q^{\mathsf{T}} Q = I$. From the orthonormalization process we also find

$$
\begin{align*}
Q_{:,1} &= \frac{1}{\sqrt{N}} \mathbf{1} \\
R_{:,1} &= (\sqrt{N}, 0, \dots)^{\mathsf{T}} = \sqrt{N} \mathbf{e}_1 \\
\end{align*}
$$

Sustituting $\widetilde{\mathbf{X}}$, we have

$$
\begin{align*}
\mathbf{1}^{\mathsf{T}} \widetilde{\mathbf{X}} (\widetilde{\mathbf{X}}^{\mathsf{T}} \widetilde{\mathbf{X}})^{-1} \widetilde{\mathbf{x}}
= \mathbf{1}^{\mathsf{T}} Q R^{-\mathsf{T}} \widetilde{\mathbf{x}}
\end{align*}
$$

Since $\mathbf{1} = \sqrt{N} Q\_{:,1}$, we have

$$
\begin{align*}
\mathbf{1}^{\mathsf{T}} Q R^{-\mathsf{T}} \widetilde{\mathbf{x}}
&= \sqrt{N} \mathbf{e}_1^{\mathsf{T}} R^{-\mathsf{T}} \widetilde{\mathbf{x}} \\
&= \sqrt{N} \widetilde{\mathbf{x}}^{\mathsf{T}} R^{-1} \mathbf{e}_1 \\
&= \sqrt{N} \widetilde{\mathbf{x}}^{\mathsf{T}} (R^{-1})_{:,1} \\
\end{align*}
$$

By $(R^{-1})\_{:,1}^{\mathsf{T}} (R^{-1})\_{:,1} = 1$, we have 
$(R^{-1})\_{:,1}^{\mathsf{T}} = \frac{1}{\sqrt{N}} \mathbf{e}\_1$ and

$$
\sqrt{N} \widetilde{\mathbf{x}}^{\mathsf{T}} (R^{-1})_{:,1} = 1
$$

Therefore

$$
\mathbf{a}^{\mathsf{T}} \mathbf{y} (\mathbf{x}) + b = 0
$$

By setting $\mathbf{a} = \mathbf{1}$ and $b = -1$, we see that $\mathbf{y} (\mathbf{x})$ sums to 1.

However, the elements of $\mathbf{y}$ are not constrained within range $[0, 1]$, which disallows the probability interpretation of $\mathbf{y}$.

The least squres approach has significant flaws:

- It lacks robustness against outiliers.
- It corresponds to the maximum likelilhood of a Gaussian conditional distribution,
which is generally not the case for binary target vectors and may result in poor performance.

### Fisher's linear discriminant

Consider a binary classification problem.

We project a D-dimensional input vector $\mathbf{x}$ into one dimension using

$$
y = \mathbf{w}^{\mathsf{T}} \mathbf{x}
$$

by which we classify $y \ge - w\_0$ as class $C\_1$, otherwise $C\_2$.

To find a $\mathbf{w}$ that maximizes the class seperation,
a simple but unreliable way is to maximize the seperation of projected class means, 
so that we maximize

$$
\mathbf{w}^{\mathsf{T}} (\mathbf{m}_2 - \mathbf{m}_1)
$$

where

$$
\mathbf{m}_1 = \frac{1}{N_1} \sum_{n \in C_1} \mathbf{x}_n, \quad 
\mathbf{m}_2 = \frac{1}{N_2} \sum_{n \in C_2} \mathbf{x}_n
$$

subject to the constraint

$$
\left\| \mathbf{w} \right\| = 1
$$

It can be solved by the Lagarange multiplier. However,
we can easily see that the maximum is obtained when $\mathbf{w}$ is parallel to $\mathbf{m}\_2 - \mathbf{m}\_1$, which gives

$$
\mathbf{w} \propto (\mathbf{m}_2 - \mathbf{m}_1)
$$

A problem of this method is, the projections of two well seperated classes can still severly overlap, as the plot illustrated. 
This results from that the spread (or the variance) of the projections of each class are not optimized.
Thus we also want a small variance of projections within classes.

![](/assets/images/prml/fig-4.6.png)

Based on this idea, the **Fisher criterion** is defined as 

$$
J (\mathbf{w}) = \frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}
$$

where

$$
\begin{align*}
m_k &= \mathbf{w}^{\mathsf{T}} \mathbf{m}_k \\
s_k^2 &= \sum_{n \in C_k} (y_n - m_k)^2 \\
y_n &= \mathbf{w}^{\mathsf{T}} \mathbf{x}_n \\
\end{align*}
$$

**Excercise 4.5**

Since

$$
\begin{align*}
(m_2 - m_1)^2 &= [\mathbf{w}^{\mathsf{T}} (\mathbf{m}_2 - \mathbf{m}_1)]^2 \\
&= \mathbf{w}^{\mathsf{T}} (\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^{\mathsf{T}} \mathbf{w}
\end{align*}
$$

and

$$
\begin{align*}
s_k^2 &= \sum_{n \in C_k} [\mathbf{w}^{\mathsf{T}} (\mathbf{x}_n - \mathbf{m}_k)]^2 \\
&= \sum_{n \in C_k} \mathbf{w}^{\mathsf{T}} (\mathbf{x}_n - \mathbf{m}_k) (\mathbf{x}_n - \mathbf{m}_k)^{\mathsf{T}} \mathbf{w}\\
\end{align*}
$$

the fisher criterion can be written as

$$
J(\mathbf{w}) = \frac{\mathbf{w}^{\mathsf{T}} \mathbf{S}_B \mathbf{w}}{\mathbf{w}^{\mathsf{T}} \mathbf{S}_W \mathbf{w}}
$$

where $\mathbf{S}\_B$ is the **between-class covariance** given by

$$
\mathbf{S}_B = (\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^{\mathsf{T}}
$$

and $\mathbf{S}\_W$ is the **within-class covariance** matrix defined by

$$
\mathbf{B}_W = \sum_{n \in C_1} (\mathbf{x}_n - \mathbf{m}_1)(\mathbf{x}_n - \mathbf{m}_1)^{\mathsf{T}}
+ \sum_{n \in C_2} (\mathbf{x}_n - \mathbf{m}_2)(\mathbf{x}_n - \mathbf{m}_2)^{\mathsf{T}}
$$

Setting direvative of $J(\mathbf{w})$ to zero, we have

$$
(\mathbf{w}^{\mathsf{T}} \mathbf{S}_B \mathbf{w}) \mathbf{S}_W \mathbf{w} = (\mathbf{w}^{\mathsf{T}} \mathbf{S}_W \mathbf{w}) \mathbf{S}_B \mathbf{w}
$$

Treating $\mathbf{S}\_B \mathbf{w}$ as a linear combination of $(\mathbf{m}\_2 - \mathbf{m}\_1)$,
we have

$$
\begin{align*}
(\mathbf{w}^{\mathsf{T}} \mathbf{S}_B \mathbf{w}) \mathbf{S}_W \mathbf{w}
&\propto (\mathbf{m}_2 - \mathbf{m}_1) \\
\mathbf{S}_W \mathbf{w} &\propto (\mathbf{m}_2 - \mathbf{m}_1) \\
\mathbf{w} &\propto \mathbf{S}_W^{-1} (\mathbf{m}_2 - \mathbf{m}_1) \\
\end{align*}
$$

which is known as the **Fisher's linear discriminant**.

Then we select a $y\_0$ such that $y(\mathbf{x}) \ge y\_0$ assigns $\mathbf{x}$ to $C\_1$, otherwise $C\_2$. 
One way to determine $y\_0$ is by first estimating the class conditional density $p(y\|C\_k)$ over projections and then using it to determine a decision boundary.
The Gaussian distribution is usually preferable since $\mathbf{w}^{\mathsf{T}} \mathbf{x}$ is a sum of random variables,
which asymptotically follows Gaussian distribution by the central limit theorem.

Note that, when $\mathbf{S}\_W$ is isotropic (proportional to the identity matrix), the solution reduces to the first case.

### Relation to least squares

This section shows that the Fisher's criterion is a special case of least squares.

Consider the two-class classification problem.

Instead of using 1-of-K coding scheme, we assign targets of $C\_1$ with value $N/N\_1$ and $C\_2$ with value $-N/N\_2$, 
where $N$ is the total number of patterns and $N\_1, N\_2$ are the number of patterns of $C\_1$ and $C\_2$ respectively.

The sum-of-sqaures error function is then given by

$$
E = \frac{1}{2} \sum_{n=1}^N (\mathbf{w}^{\mathsf{T}} \mathbf{x}_n + w_0 - t_n)^2
$$

Setting derivative with respect to $w\_0$ to zero, we obtain

$$
\sum_n (\mathbf{w}^{\mathsf{T}} \mathbf{x}_n + w_0 - t_n) = 0
$$

Using

$$
\mathbf{m} = \frac{1}{N} \sum_n \mathbf{x}_n,
\quad \sum_n t_n = N_1 \frac{N}{N_1} - N_2 \frac{N}{N_2} = 0
$$

we have

$$
w_0 = - \mathbf{w}^{\mathsf{T}} \mathbf{m}
$$

Setting derivative with respect to $\mathbf{w}$ to zero gives

$$
\sum_n (\mathbf{w}^{\mathsf{T}} \mathbf{x}_n + w_0 - t_n) \mathbf{x}_n = \mathbf{0}
$$

Substituting $w\_0$ and rearranging terms we have

$$
\begin{align*}
\sum_n \mathbf{x}_n \mathbf{x}_n^{\mathsf{T}} \mathbf{w}
+ \sum_n \mathbf{x}_n \mathbf{m}^{\mathsf{T}} \mathbf{w}
&= \sum_{n \in C_1} \frac{N}{N_1} \mathbf{x}_n 
- \sum_{n \in C_2} \frac{N}{N_2} \mathbf{x}_n  \\
\sum_n \mathbf{x}_n \mathbf{x}_n^{\mathsf{T}} \mathbf{w}
+ N \mathbf{m} \mathbf{m}^{\mathsf{T}} \mathbf{w}
&= N (\mathbf{m}_1 - \mathbf{m}_2) \\
\end{align*}
$$

The final result gives

$$
\left( \mathbf{S}_W + \frac{N_1 N_2}{N} \mathbf{S}_B \right) \mathbf{w} = N (\mathbf{m}_1 - \mathbf{m}_2) \\
$$

Since $\mathbf{S}\_B \mathbf{w}$ can be seen as a linear combination of $(\mathbf{m}\_2 - \mathbf{m}\_1)$,
the result is equivalent to the Fisher's linear discriminant:

$$
\mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{m}_2 - \mathbf{m}_1)
$$

Note that, the bias $w\_0$ is already given by $- \mathbf{w}^{\mathsf{T}} \mathbf{m}$.
Therefore $\mathbf{x}$ belongs to $C\_1$ if $y(\mathbf{x}) = \mathbf{w}^{\mathsf{T}} (\mathbf{x} - \mathbf{m}) > 0$, $C\_2$ otherwise.

### Fisher's discriminant for multiple classes

Assume that the dimensionality $D$ of input space is greater than the number K of classes.

> WHY???

We introduce a set of $D^\prime > 1$ linear functions $y\_k = \mathbf{w}\_k^{\mathsf{T}} \mathbf{x}$, where $k = 1, \dots, D^\prime$.
These functions can be collectively expressed as

$$
\mathbf{y} = \mathbf{W}^{\mathsf{T}} \mathbf{x}
$$

where $\mathbf{W}\_{D \times D^\prime} = (\mathbf{w}\_1, \dots, \mathbf{w}\_{D^\prime})$.
Note that, the bias parameter is not included.

The generalized within-class covariance matrix is given by

$$
\begin{align*}
\mathbf{S}_W &= \sum_{k=1}^K \mathbf{S}_k \\
\end{align*}
$$

where 

$$
\begin{align*}
\mathbf{S}_k &= \sum_{n \in C_k} (\mathbf{x}_n - \mathbf{m}_k)(\mathbf{x}_n - \mathbf{m}_k)^{\mathsf{T}} \\
\mathbf{m}_k &= \frac{1}{N_k} \sum_{n \in C_k} \mathbf{x}_n
\end{align*}
$$

and $N\_k$ is the number of patterns in class $C\_k$.

The total covariance matrix is give by

$$
\mathbf{S}_T = \sum_{n=1}^N (\mathbf{x}_n - \mathbf{m})(\mathbf{x}_n - \mathbf{m})^{\mathsf{T}}
$$

where

$$
\mathbf{m} = \frac{1}{N} \sum_{n=1}^N \mathbf{x}_n
$$

Decomposing $\mathbf{S}\_T$ into two matrices such that

$$
\mathbf{S}_T = \mathbf{S}_W + \mathbf{S}_B
$$

we obtain

$$
\mathbf{S}_B =  \sum_{k=1}^K N_k (\mathbf{m}_k - \mathbf{m})(\mathbf{m}_k - \mathbf{m})^{\mathsf{T}}
$$

which can be used as a measure of the between-class covariance.

Note that, when $K=2$, the between-class invariance given by

$$
\mathbf{S}_B = \frac{N_1 N_2}{N} (\mathbf{m}_1 - \mathbf{m}_2)(\mathbf{m}_1 - \mathbf{m}_2)^{\mathsf{T}}
$$

is different from the $K=2$ case mentioned in the previous sections.

The covariance matrices projected into the $D^\prime$-dimensional space of $\mathbf{y}$ are defined as

$$
\begin{align*}
\mathbf{s}_k &= \sum_{n \in C_k} (\mathbf{y}_n - \boldsymbol{\mu}_k)(\mathbf{y}_n - \boldsymbol{\mu}_k)^{\mathsf{T}} \\
\mathbf{s}_B &=  \sum_{k=1}^K N_k (\boldsymbol{\mu}_k - \boldsymbol{\mu})(\boldsymbol{\mu}_k - \boldsymbol{\mu})^{\mathsf{T}} \\
\end{align*}
$$

where

$$
\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{n \in C_k} \mathbf{y}_n, 
\quad \boldsymbol{\mu} = \frac{1}{N} \sum_{k=1}^K \mathbf{y}_n
$$

Then we try to maximize the class separation in the space of $\mathbf{y}$. One choice of criterion is given by

$$
J(\mathbf{w}) = \operatorname{Tr} (\mathbf{s}_W^{-1} \mathbf{s}_B)
$$

Another equivalent form explicitly involving $\mathbf{W}$ is written as

$$
J(\mathbf{W}) = \operatorname{Tr} \left\{ (\mathbf{W}^{\mathsf{T}} \mathbf{S}_W \mathbf{W})^{-1} \mathbf{W}^{\mathsf{T}} \mathbf{S}_B \mathbf{W} \right\}
$$

The weight values are determined by by those eigenvectors of $\mathbf{s}\_W^{-1} \mathbf{s}\_B$
that corresponds the $D^\prime$ largest eigenvalues.

TODO: Proof

### The perceptron algorithm

Consider a linear model of the form

$$
y(\mathbf{x}) = f \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}) \right)
$$

in which $\boldsymbol{\phi}(\mathbf{x})$ is a feature vector nonlinearly transformed from $\mathbf{x}$,
with $\phi\_0(\mathbf{x}) = 1$.
The nonlinear **activation function** $f$ is defined as

$$
f(a) = \begin{cases}
+1 & \text{ if } a \ge 0 \\
-1 & \text{ if } a < 0 \\
\end{cases}
$$

Then we define a target variable $t$ such that $t = +1$ belongs to class $C\_1$ and $t=-1$ to class $C\_2$.
For correctly classified patterns, we have $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}\_n) t\_n \ge 0$, and $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}\_n) t\_n < 0$ otherwise.

The **perceptron criterion** is an error function defined by

$$
E_P(\mathbf{w}) = - \sum_{n \in \mathcal{M}} \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n) t_n
$$

in which $\mathcal{M}$ is the set of misclassified patterns. 

Our goal is to minimize the error function. Applying the stochastic gradient descent algorithm, we obtain

$$
\mathbf{w}^{(\tau + 1)} = \mathbf{w}^{(\tau)} - \eta \nabla E_n (\mathbf{w}) 
= \mathbf{w}^{(\tau)} + \eta \boldsymbol{\phi}_n t_n
$$

Since the magnitude of $\mathbf{w}$ does not affect the output of the discriminant $y(\mathbf{x})$, we can take $\eta=1$ without loss of generality.

Note that, as $\mathbf{w}$ changes, the set of misclassified patterns also changes. 
The algorithm does not guarantee the total error will reduce after each iteration.

The **perceptron convergence theorem** states that if the training data can be linearly separated, 
then the perceptron is guaranteed to find an exact solution in finite number of steps.
For data sets that are not linearly separable, the algorithm diverges.

In practice, however, it is difficult to distinguish between slow convergence and divergence.

Other disadvantages of the perceptron are

- It does not generalize to $K > 2$
- It is based on fixed basis functions

## Probability generative models

In the generative approach, we first models the class-conditional distribution $p(\mathbf{x}\|C\_k)$ and the class priors $p(C\_k)$, and then obtain the posterior $p(C\_k\|\mathbf{x})$.

It's called 'generative' because we can obtain the joint distribution $p(\mathbf{x}, C\_k)$ and generate new data.

First we give an alternative form of writting the posterior probability.

Consider a two-class classification problem. The posterior for class $C\_1$ can be written as

$$
\begin{align*}
p(C_1|\mathbf{x}) &= \frac{p(\mathbf{x}|C_1) p(C_1)}{p(\mathbf{x}|C_1) p(C_1) + p(\mathbf{x}|C_2) p(C_2)} \\
&= \frac{1}{1 + \exp(-a)} = \sigma(a)
\end{align*}
$$

in which $\sigma(a)$ are the logistic sigmoid function. It then follows

$$
a = \ln \frac{p(\mathbf{x}|C_1) p(C_1)}{p(\mathbf{x}|C_2) p(C_2)}
$$

which is known as the **log odds**.

The inverse of the logistic sigmoid is given by

$$
a = \ln \left( \frac{\sigma}{1 - \sigma} \right)
$$

which is also known as the **logit** function.

For $K > 2$ classes, we have

$$
p(C_k|\mathbf{x}) = \frac{p(\mathbf{x}|C_k) p(C_k)}{\sum_j p(\mathbf{x}|C_j) p(C_j)}
= \frac{\exp(a_k)}{\sum_j \exp(a_j)}
$$

which is known as the **normalized exponential** or the **softmax** function. It is a generalization of the sigmoid function. 
Here we defined

$$
a_k = \ln p(\mathbf{x}|C_k) p(C_k) \tag{4.63}
$$

The name "softmax" is misleading. The function is not a smooth maximum (that is, a smooth approximation to the maximum function), but is rather a smooth approximation to the arg max function.

### Continuous inputs

Assume that the class-conditional distributions are Gaussians and all classes have a common covariance matrix.

The class-conditional density for $C\_k$ is given by

$$
p(\mathbf{x}|C_k) = \frac{1}{(2 \pi)^{1/2}} \frac{1}{|\boldsymbol{\Sigma}|^{1/2}}
\exp \left\{ - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k) \boldsymbol{\Sigma}^{-1}  (\mathbf{x} - \boldsymbol{\mu}_k)^{\mathsf{T}} \right\}
$$

Consider first the two-class case. By wrtting the posterior in the form

$$
p(C_1 | \mathbf{x}) = \sigma(\mathbf{w}^{\mathsf{T}} \mathbf{x} + w_0)
$$

We have

$$
\begin{align*}
\mathbf{w} &= \boldsymbol{\Sigma}^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2) \\
w_0 &= - \frac{1}{2} \boldsymbol{\mu}_1^{\mathsf{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_1
+ \frac{1}{2} \boldsymbol{\mu}_2^{\mathsf{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_2
+ \ln \frac{p(C_1)}{p(C_2)} \\
\end{align*}
$$

The quadratic terms in $\mathbf{x}$ are canceled due to the common covariance matrix.
The decision boundary thus will be linear.

As to the case of $K$ classes, we deine

$$
a_k(\mathbf{x}) = \mathbf{w}_k^{\mathsf{T}} \mathbf{x} + w_{k0}
$$

in which the quadratic terms in $\mathbf{x}$ are also canceled so that

$$
\begin{align*}
\mathbf{w}_k &= \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k \\
w_{k0} &= - \frac{1}{2} \boldsymbol{\mu}_k^{\mathsf{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k + \ln p(C_k)
\end{align*}
$$

If the conditional-class distributions have different covariance matrices, then the quadratic terms will appear, which gives a **quadratic discriminant**.

### Maximum likelihood solution

**Excercise 4.10**

Consider a generative classification model for $K$ classes.

Suppose that the prior distribution is defined by $p(C\_k) = \pi\_k$,
$\boldsymbol{\phi}$ is the feature vector, 
$p(\boldsymbol{\phi}\|C\_k)$ is a general class-conditional density, 
$\mathbf{t}\_n$ is a target vector with 1-of-K coding scheme,
and $N$ is the number of training data. 

The likelihood function is then given by

$$
p(\mathbf{T}, \mathbf{X}| \{\pi_i\}) = \prod_{n=1}^N \prod_{j=1}^K p(\mathbf{x}_n, C_j)^{t_{nj}}
= \prod_{n=1}^N \prod_{j=1}^K \big[ p(\mathbf{x}_n|C_j) p(C_j) \big]^{t_{nj}}
$$

where $\mathbf{T} = (\mathbf{t}\_1, \dots, \mathbf{t}\_N)^{\mathsf{T}}$.

Taking logarithm

$$
\ln p(\mathbf{T}| \{\pi_i\}) 
= \sum_{n=1}^N \sum_{j=1}^K t_{nj} \left[ \ln p(\mathbf{x}_n| C_j) + \ln \pi_j \right]
$$

With the constraint

$$
\sum_j \pi_j = 1
$$

we introduce a Lagrange multipler $\lambda$ so that

$$
L(\{\pi_i\}) = \ln p(\mathbf{T}|{\pi_i}) + \lambda \left( \sum_j \pi_j - 1 \right)
$$

Setting the derivative with repsect to $\pi\_k$ to zero

$$
\sum_{n=1}^N \frac{t_{nk}}{\pi_k} + \lambda = 0
$$

Multiply both sides with $\pi\_k$ and sum over $k$

$$
\begin{align*}
\sum_n \sum_k t_{nk} + \lambda \sum_k \pi_k &= 0 \\
N + \lambda &= 0
\end{align*}
$$

Back-substituting we obtain

$$
\pi_k = \frac{N_k}{N}
$$

where $N\_k$ is the number of patterns in class $C\_k$.

Now suppose the class-conditional distribution for class $C\_k$ is given by

$$
p(\boldsymbol{\phi}|C_k) = \mathcal{N} (\boldsymbol{\phi}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma})
$$

where all classes share a common covariance matrix. The log likelohood function then takes the form

$$
\ln p(\mathbf{T}, \mathbf{X}| \{\pi_i\}, \{ \boldsymbol{\mu}_k \}) 
= \sum_{n=1}^N \sum_{j=1}^K t_{nj} \left[ - \frac{D}{2} \ln 2\pi - \frac{1}{2} \ln |\boldsymbol{\Sigma}|
- \frac{1}{2} (\boldsymbol{\phi}_n - \boldsymbol{\mu}_j)^{\mathsf{T}} \boldsymbol{\Sigma}^{-1} (\boldsymbol{\phi}_n - \boldsymbol{\mu}_j) + \ln \pi_j \right]
$$

Setting derivative with respect to $\boldsymbol{\mu}\_k$ to zero,  we have

$$
\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_n t_{nk} \boldsymbol{\phi}_n
= \frac{1}{N_k} \sum_{n \in C_k} \boldsymbol{\phi}_n
$$

which is the mean of input vectors of class $C\_k$.

Taking derivative with respect to $\boldsymbol{\Sigma}$, we obtain

$$
\begin{align*}
\boldsymbol{\Sigma} &= \sum_{k=1}^K \frac{N_k}{N} \mathbf{S}_k \\
\mathbf{S}_k &= \frac{1}{N_k} \sum_{n \in C_k} (\boldsymbol{\phi}_n - \boldsymbol{\mu}_k) (\boldsymbol{\phi}_n - \boldsymbol{\mu}_k)^{\mathsf{T}}
\end{align*}
$$

Thus $\boldsymbol{\Sigma}$ is given by the weighted average of the covariances of each class.

Note that fitting Gaussian distributions to classes is not robust against outliers, since the maximum likelihood of a Gaussian is not robust.

### Discrete features

**Exercise 4.11**

Consider a $K$-class classification problem with feature vector $\boldsymbol{\phi}$ composed of $M$ independent components,
each of which takes one of $L$ possible states and uses 1-of-L binary coding scheme.

The class-conditional distribution is then given by

$$
p(\boldsymbol{\phi}|C_k) = \prod_{i=1}^M \prod_{j=1}^L \mu_{kij}^{\phi_{ij}}
$$

where $0 \le \mu\_{kij} \le 1$ represents the probability of the j-th state of the i-th component of class $C\_k$,
$\phi\_{ij} \in \{0, 1\}$ is the value of the j-th state of the i-th component, and they satisfy

$$
\begin{align*}
\sum_{j=1}^L \phi_{ij} = 1, \quad \sum_{j=1}^L \mu_{kij} = 1 \\
\end{align*}
$$

Using (4.63) we obtain

$$
\begin{align*}
a_k(\boldsymbol{\phi}) = \sum_{i=1}^M \sum_{j=1}^L \phi_{ij} \ln \mu_{kij} + \ln p(C_k)
\end{align*}
$$

### Exponential family

Consider a general form of exponential family defined by

$$
p(\mathbf{x}|\boldsymbol{\lambda}_k) 
= h(\mathbf{x}) g(\boldsymbol{\lambda}_k) \exp \left\{ \boldsymbol{\lambda}_k^{\mathsf{T}} \mathbf{u} (\mathbf{x}) \right\}
$$

Setting $\mathbf{u} (\mathbf{x}) = \mathbf{x}$ and introducing a scale parameter $s$,
we obtain a restricted form of exponential family

$$
p(\mathbf{x}|\boldsymbol{\lambda}_k, s)
= \frac{1}{s} h(\frac{1}{s} \mathbf{x}) g(\boldsymbol{\lambda}_k) \exp \left\{ \frac{1}{s} \boldsymbol{\lambda}_k \mathbf{x} \right\}
$$

in which each class has its own parameter $\boldsymbol{\lambda}\_k$ but shares a common scale parameter.

For binary class problems, we have

$$
a(\mathbf{x}) = \frac{1}{s} (\boldsymbol{\lambda}_1 - \boldsymbol{\lambda}_2)^{\mathsf{T}} \mathbf{x}
+ \ln g(\boldsymbol{\lambda}_1) - \ln g(\boldsymbol{\lambda}_2)
+ \ln p(C_1) - \ln p(C_2)
$$

And for multi-class problems, we have

$$
a_k (\mathbf{x}) = \frac{1}{s} \boldsymbol{\lambda}_k^{\mathsf{T}} \mathbf{x}
+ \ln g(\boldsymbol{\lambda}_k) + \ln p(C_k)
$$
