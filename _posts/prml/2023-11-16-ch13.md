---
layout: posts
title: 'PRML Ch13 - Sequential Data'
---
# Sequential Data

This chapter drops the assumption that observations are i.i.d and discuss the modeling of sequential data.

There're two kinds of sequntial distributions:

- Stationary model where the data evolves with time but the distribution remains unchanged.
- Nonstationary model where the distribution itself evolves with time.

## Markov models

An intuitive thought is that the recent ovservations are more informative than the historical ones. 
The **Markov models** assume that the future predictions are depedent only on the most recent observations. It can be written in the form

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N) = \prod_{n=1}^N p(\mathbf{x}_n | \mathbf{x}_1, \dots, \mathbf{x}_{n-1})
$$

which corresponds to fully connected graphical model of $N$ nodes.

If we assume that the conditional distribution depends only on the most recent observation,
we obtain the **first-order Markov chain**, which can be written as

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N) = p(\mathbf{x}_1) \prod_{n=2}^N p(\mathbf{x}_n | \mathbf{x}_{n-1})
$$

The corresponding graph is shown by

$$
\mathbf{x}_1 \rightarrow \mathbf{x}_2 \rightarrow \cdots \rightarrow \mathbf{x}_N
$$

By d-separation, we have

$$
p(\mathbf{x}_n | \mathbf{x}_1, \dots, \mathbf{x}_{n-1}) = p(\mathbf{x}_n | \mathbf{x}_{n-1})
$$

If the model $p(\mathbf{x}\_n \| \mathbf{x}\_{n-1})$ is defined to be stationary, it is called a homogenuous Markov chain.

A second order Markov chain takes the form

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N) = p(\mathbf{x}_1) p(\mathbf{x}_2 | \mathbf{x}_1) \prod_{n=3}^N p(\mathbf{x}_n | \mathbf{x}_{n-1}, \mathbf{x}_{n-2})
$$

which corresponds to the graph

![](/assets/images/prml/fig-13.4.png)

However high order Markkov chain may suffer from the large number of parameters.
Consider a $M$-th order Markov chain, in which Each varaible is discrete and has $K$ possible states.
Then by the discussion of Ch8, to model $p(\mathbf{x}\_n \| \mathbf{x}\_{n-M}, \dots, \mathbf{x}\_{n-1})$, we need $K^M (K-1)$ parameters.

For continous variables, we use linear-Gaussian condition distribution, which gives the **autoregressive** (a.k.a AR) model.

Some parametric model, such as neural network, can be used to model $p(\mathbf{x}\_n \| \mathbf{x}\_{n-M}, \dots, \mathbf{x}\_{n-1})$ and reduce the number of prameters.

We can also build a model that is not limited by the Markov assumption yet has limited number of parameters. 
For each $\mathbf{x}\_n$, we introduce a latent variable $\mathbf{z}\_n$ which can be discrete or continuous and have different dimensionality as $\mathbf{x}$.
The graphical model is shown by

![](/assets/images/prml/fig-13.5.png)

The joint distribution is written

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N, \mathbf{z}_1, \dots, \mathbf{z}_N)
= p(\mathbf{z}_1) \left[ \prod_{n=2}^N p(\mathbf{z}_n|\mathbf{z}_{n-1}) \right]
\prod_{n=1}^N p(\mathbf{x}_n | \mathbf{z}_n)
$$

This model is known as the **state space model**. It has the property that

$$
\mathbf{z}_{n+1} \perp\!\!\!\perp \mathbf{z}_{n-1} \mid \mathbf{z}_n
$$

Moreover, the path between any $\mathbf{x}\_n$ and $\mathbf{x}\_m$ will not be blocked by latent variables,
so the predictive distribution $p(\mathbf{x}\_{n+1}\|\mathbf{x}\_1, \dots, \mathbf{x}\_n)$ will depend on all previous observations.

If the latent variables are discrete, we obtain the **hidden Markov model**.
If they are continuous, we obtain the **linear dynamic system**.

## Hidden Markov models

Widely used in NPL, on-line handwriting recognition.

Let $\mathbf{z}\_n$ be a discrete random variable using 1-of-K coding scheme.
To model the distribution $p(\mathbf{z}\_n\|\mathbf{z}\_{n-1})$, we define a **transition matrix** $\mathbf{A}$ with each element given by

$$
A_{jk} = p(z_{nk} = 1 | z_{n-1, j} = 1)
$$

which is called the **transition probability**. It satisfies $0 \le A\_{jk} \le 1$ and $\sum\_{k} A\_{jk} = 1$.
The number of independent parameter in $\mathbf{A}$ is $K(K-1)$.

Then the conditional distribution $p(\mathbf{z}\_n\|\mathbf{z}\_{n-1})$ takes the form

$$
p(\mathbf{z}_n|\mathbf{z}_{n-1}, \mathbf{A})
= \prod_{k=1}^K \prod_{j=1}^K A_{jk}^{z_{n-1, j} z_{nk}}
$$

For $\mathbf{z}\_1$, we define $\pi\_k \equiv p(z\_{1k} = 1)$ and then

$$
p(\mathbf{z}_1) = \prod_{k=1}^K \pi_k^{z_{1k}}
$$

where $\sum\_k \pi\_k = 1$.

The transition matrix can be shown as a state transition diagram. In the case of 3 states, we have

![](/assets/images/prml/fig-13.6.png)

Another type of diagram, namely the **lattice** or **trellis** diagram, is obtained by expanding the transitions along time, which gives

![](/assets/images/prml/fig-13.7.png)

The distribution $p(\mathbf{x}\_n\|\mathbf{z}\_n, \boldsymbol{\phi})$, where $\boldsymbol{\phi}$ denotes the parameters, is called the **emission probability**.
It can be defined as a conditional distribution of a mixture distribution, so that

$$
p(\mathbf{x}_n|\mathbf{z}_n, \boldsymbol{\phi})
= \prod_{k=1}^K p(\mathbf{x}_n | \boldsymbol{\phi}_k)^{z_{nk}}
$$

where $\boldsymbol{\phi}\_k$ is the parameters corresponding to $z\_{nk} = 1$.

If we assume that $p(\mathbf{z}\_n\|\mathbf{z}\_{n-1})$ shares a transition matrix for $n=2,\dots, N$ and $p(\mathbf{x}\_n\|\boldsymbol{\phi}\_k)$ shares the same parameter $\boldsymbol{\phi}$ for $n=1,\dots, N$,
the joint distribution is then given by

$$
p(\mathbf{Z}, \mathbf{X}|\boldsymbol{\theta}) 
= p(\mathbf{z}_1|\boldsymbol{\pi}) \left[ \prod_{n=2}^N p(\mathbf{z}_n|\mathbf{z}_{n-1}, \mathbf{A}) \right]
\prod_{m=1}^N p(\mathbf{x}_m|\mathbf{z}_m, \boldsymbol{\phi})
$$

where $\mathbf{X} = \{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$, $\mathbf{Z} = \{ \mathbf{z}\_1, \dots, \mathbf{z}\_N \}$ and $\boldsymbol{\theta} = \{ \boldsymbol{\pi}, \mathbf{A}, \boldsymbol{\phi} \}$.

The emission probability can also be modeled as linear regression or neural network.

A restricted form of HMM, called the **left-to-right** HMM, is obtained by setting $A\_{jk} = 0$ for $k < j$, so that the state transitions are constrained to non-decreasing.
$p(\mathbf{z}\_1)$ is typically defined as $p(z\_{11} = 1) = 1$ and $p(\mathbf{z}\_{1j}=1) = 0$ for $j \ne 1$, so that the sequence always starts from state 1.

A 3-state left-to-right HMM:

![](/assets/images/prml/fig-13.9.png)

Further constraint is to set $A\_{jk} = 0$ for $k > j+\Delta$. This disallows drastic changes in the state transition.

![](/assets/images/prml/fig-13.10.png)

Many applications such as speech recognition or on-line handwritting recognition make use of the left-to-right HMM.

### Maximum likelihood for the HMM

The likelihood of observation set $\mathbf{X} = \{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$ is obtained from

$$
p(\mathbf{X} | \mathbf{\theta}) = \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} | \mathbf{\theta})
$$

First difficulty of direct optimization is the summation over $\mathbf{Z}$, which takes a total of $K^N$ possible values, each of which corresponding to a path through the lattic diagram.

Therefore we consider the EM algorithm. In the E step, we evaluate the posterior $p(\mathbf{Z} \| \mathbf{X}, \boldsymbol{\theta}^{\text{old}})$.
Then the expectation is given by

$$
Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})
= \sum_{\mathbf{Z}} p(\mathbf{Z} | \mathbf{X}, \boldsymbol{\theta}^{\text{old}})
\ln p(\mathbf{X}, \mathbf{Z} | \mathbf{\theta})
$$

The complete-data log likelihood takes the form

$$
\begin{align*}
\ln p(\mathbf{X}, \mathbf{Z} | \mathbf{\theta})
&= \ln p(\mathbf{z}_1|\boldsymbol{\pi}) + \sum_{n=2}^N \ln p(\mathbf{z}_n|\mathbf{z}_{n-1}, \mathbf{A}) 
+ \sum_{m=1}^N \ln p(\mathbf{x}_m|\mathbf{z}_m, \boldsymbol{\phi}) \\
&= \sum_{k=1}^K z_{1k} \ln \pi_k + \sum_{n=2}^N \sum_{j=1}^K \sum_{k=1}^K z_{n-1, j} z_{nk} \ln A_{jk} \\
&\quad + \sum_{n=1}^N \sum_{k=1}^K z_{nk} \ln p(\mathbf{x}_n | \boldsymbol{\phi}_k)
\end{align*}
$$

Denoting the posterior distributions by

$$
\begin{align*}
\gamma (\mathbf{z}_n) &= p(\mathbf{z}_n | \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \\
\xi (\mathbf{z}_{n-1}, \mathbf{z}_n) &= p(\mathbf{z}_{n-1}, \mathbf{z}_n | \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \\
\end{align*}
$$

where $\gamma (\mathbf{z}\_n)$ is specified by $K$ probability values and $\xi (\mathbf{z}\_{n-1}, \mathbf{z}\_n)$ is specified by $K \times K$ probability values. 
We may also define

$$
\begin{align*}
\gamma (z_{nk}) &= p(z_{nk} = 1 | \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \\
\xi (z_{n-1, j}, z_{nk}) &= p(z_{n-1, j} = 1,  z_{nk} =1 | \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \\
\end{align*}
$$

Then we obtain the expectations under the $p(\mathbf{Z} \| \mathbf{X}, \boldsymbol{\theta}^{\text{old}})$

$$
\begin{align*}
\operatorname{E} [z_{nk}] &= \sum_{\mathbf{z}_n} \gamma (\mathbf{z}_n) z_{nk} = \gamma (z_{nk}) \\
\operatorname{E} [z_{n-1, j}, z_{nk}] &= \sum_{z_{n-1, j}} \sum_{z_{nk}} \xi (\mathbf{z}_{n-1}, \mathbf{z}_n) z_{n-1, j} z_{nk}
= \xi (z_{n-1, j}, z_{nk})
\end{align*}
$$

Then we have

$$
\begin{align*}
Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})
&= \sum_{k=1}^K \gamma (z_{1k}) \ln \pi_k + \sum_{n=2}^N \sum_{j=1}^K \sum_{k=1}^K \xi (z_{n-1, j}, z_{nk}) \ln A_{jk} \\
&\quad + \sum_{n=1}^N \sum_{k=1}^K \gamma (z_{nk}) \ln p(\mathbf{x}_n | \boldsymbol{\phi}_k)
\end{align*}
$$

The evaluation of $\gamma (z\_{nk})$ and $\xi (z\_{n-1, j}, z\_{nk})$ will be discussed shortly.

In the following M step, we maximize $Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})$ with respect to $\boldsymbol{\theta}$. 

Maximization with respect to $\boldsymbol{\pi}$ and $\mathbf{A}$, under the summation constraint, can be done using Lagrange multiplier and gives

$$
\begin{align*}
\pi_k &= \frac{\gamma (z_{1k})}{\sum_{j=1}^K \gamma (z_{1j})} \\
A_{jk} &= \frac{\sum_{n=2}^N \xi(z_{n-1, j}, z_{nk})}{\sum_{n=2}^N \sum_{l=1}^K \xi(z_{n-1, j}, z_{nl})}
\end{align*}
$$

**Excercise 13.5**

Note that any $\pi\_k$ or $A\_{jk}$ that is initialized to zero will remain zero in subsequent optimization.
For $\pi\_k = 0$, we have $\gamma(z\_{1k}) = \pi\_k = 0$. For $A\_{jk} = 0$, we have $\xi(z\_{n-1, j}, z\_{nk}) = 0$ for $n=2, \dots, N$.

As a result, to build a left-to-right model, the only modification required is to choose some elements of $\mathbf{A}$ to be zero.

Maximization with respect to $\boldsymbol{\phi}\_k$ is straightforward, which is achived by

$$
\sum_{n=1}^N \sum_{k=1}^K \gamma (z_{nk}) \frac{\partial}{\partial \boldsymbol{\phi}_k} \ln p(\mathbf{x}_n | \boldsymbol{\phi}_k) = 0
$$

In the case of a Gaussian distribution

$$
p(\mathbf{x} | \boldsymbol{\phi}_k) = \mathcal{N} (\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$

we have

$$
\begin{align*}
\boldsymbol{\mu}_k &= \frac{\sum_{n=1}^N \gamma (z_{nk}) \mathbf{x}_n}{\sum_{n=1}^N \gamma (z_{nk})} \\
\boldsymbol{\Sigma}_k &= \frac{1}{\sum_{n=1}^N \gamma (z_{nk})}
\sum_{n=1}^N \gamma (z_{nk}) (\mathbf{x}_n -  \boldsymbol{\mu}_k) (\mathbf{x}_n -  \boldsymbol{\mu}_k)^{\mathsf{T}}
\end{align*}
$$

In the case of a multinomial distribution

$$
p(\mathbf{x} | \boldsymbol{\phi}_k) = \prod_{i=1}^D \mu_{ki}^{x_i}
$$

where $\sum\_i \mu\_{ki} = 1$. Using Lagrange multiplier, we have

$$
\mu_{ki} = \frac{\sum_{n=1}^N \gamma (z_{nk}) x_{ni}}{\sum_{n=1}^N \gamma (z_{nk})} \\
$$

A good approach to initialize the parameters of emission distribution is first to treat the data as i.i.d, and then use the maximum likelihood to obtain the initial values.

### The forward-backward algorithm

