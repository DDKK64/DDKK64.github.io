---
layout: posts
title: 'PRML Ch2 - Probability distributions - Part II'
---
- [Guassian distribution](#guassian-distribution)
  - [Marginal Gaussian distribution](#marginal-gaussian-distribution)
  - [Conditional Gaussian distribution](#conditional-gaussian-distribution)
  - [Bayes' theorem for Gaussian distribution](#bayes-theorem-for-gaussian-distribution)
  - [Maximum likelihood for Gaussian](#maximum-likelihood-for-gaussian)
  - [Sequential estimation](#sequential-estimation)
  - [Bayesian inference for Gaussian distribution](#bayesian-inference-for-gaussian-distribution)
  - [Student's t-distribution](#students-t-distribution)
  - [Periodic variables](#periodic-variables)
  - [Mixture of Gaussians](#mixture-of-gaussians)

## Guassian distribution

A.k.a. normal distribution.

A univariate Gaussian distribution takes the form

$$
\mathcal{N} (\mu ,\sigma ^{2}) = \frac{1}{(2 \pi \sigma ^{2})^{1/2}} \exp \left \{ - \frac{1}{2 \sigma ^{2}} (x-\mu)^2 \right \}
$$

where $\mu$ is the mean and $\sigma ^{2}$ is the variance.

A $D$-dimensional multivariate Guassian distribution takes the form

$$
\mathcal{N} (\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma})
= \frac{1}{(2\pi)^{D/2}} \frac{1}{| \boldsymbol{\Sigma} | ^{1/2}} \exp \left \{ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right \}
$$

where 

- $\mathbf{x}$ is a $D$-dimensional vector
- $\boldsymbol{\mu}$ is the mean vector
- $\boldsymbol{\Sigma}$ is a $D \times D$ covariance matrix
- $\| \boldsymbol{\Sigma} \|$ is the determinant of $\boldsymbol{\Sigma}$

Gaussian distribution can be motivated from many perspectives and also arises in various contexts.
Recall that, the Gaussian distribution maximizes the entropy under bounded mean and variance.
As a example of central limit theorem, the binomial distribution, which can be seen as a sum of Bernoulli variables, will tend to a Guassian as $N \rightarrow \infty$.

The quadratic form in the exponent

$$
\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})
$$

is called the **Mahalanobis distance** from $\mathbf{x}$ to $\boldsymbol{\mu}$.

$\Delta$ degrades to  Euclidean distance when $\boldsymbol{\Sigma}$ is the identity matrix.

Without loss of generality, $\boldsymbol{\Sigma}$ can always be taken to be symmetric.
If not, let $\Lambda$ denote $\boldsymbol{\Sigma}^{-1}$, 
then by substituting $\Lambda$ with 

$$
\Lambda = \frac{\Lambda + \Lambda^{\mathsf {T}}}{2} + \frac{\Lambda - \Lambda^{\mathsf {T}}}{2}
$$

we have

$$
\begin{align*}
\Delta^2 &= (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \frac{\Lambda + \Lambda^{\mathsf {T}}}{2} (\mathbf{x} - \boldsymbol{\mu}) \\
&\quad + \frac{1}{2} \left[ (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda (\mathbf{x} - \boldsymbol{\mu}) - (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda^{\mathsf {T}} (\mathbf{x} - \boldsymbol{\mu}) \right]
\end{align*}
$$

Since

$$
(\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda (\mathbf{x} - \boldsymbol{\mu})
= \left[ (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda (\mathbf{x} - \boldsymbol{\mu}) \right]^{\mathsf {T}}
= (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \Lambda^{\mathsf {T}} (\mathbf{x} - \boldsymbol{\mu})
$$

we obtain

$$
\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \frac{\Lambda + \Lambda^{\mathsf {T}}}{2} (\mathbf{x} - \boldsymbol{\mu})
$$

in which $\frac{\Lambda + \Lambda^{\mathsf {T}}}{2}$ is symmetric.
Basically, what we've done here is to transform terms like $\displaystyle a\_{ij} x\_i x\_j + a\_{ji} x\_j x\_i$ into
$b\_{ij} x\_i x\_j + b\_{ji} x\_j x\_i$ where $b\_{ij} = b\_{ji} = (a\_{ij} + a\_{ji}) /2$.

Also note that if $\boldsymbol{\Sigma}$ (or $\boldsymbol{\Sigma}^{-1}$ vice versa) is symmetric, $\boldsymbol{\Sigma}^{-1}$ is also symmetric,
since $(\boldsymbol{\Sigma}^{\mathsf{T}})^{-1} = (\boldsymbol{\Sigma}^{-1})^{\mathsf{T}}$.

As from now, we shall suppose $\boldsymbol{\Sigma}$ is a real, symmetric matrix.
Then diagonalizing $\boldsymbol{\Sigma}$ gives

$$
U \boldsymbol{\Sigma} U^{\mathsf {T}} = \operatorname{diag} (\lambda_1, \dots, \lambda_D)
$$

where

$$
U = (\mathbf{u_1}, \dots, \mathbf{u_D})^{\mathsf {T}}
$$

and $\mathbf{u\_i}$ is the eigenvector of $\boldsymbol{\Sigma}$ corresponding to eigenvalue $\lambda\_i$.

Taking the inverse, we have

$$
(U \boldsymbol{\Sigma} U^{\mathsf {T}})^{-1}
= U \boldsymbol{\Sigma}^{-1} U^{\mathsf {T}}
= \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D})
$$

The inverse of $\boldsymbol{\Sigma}$ can be expressed as

$$
\boldsymbol{\Sigma}^{-1} = U^{\mathsf {T}} \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D}) U
$$

Then we have

$$
\begin{align*}
\Delta^2 
&= (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}}
U^{\mathsf {T}} \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D}) U
(\mathbf{x} - \boldsymbol{\mu}) \\
&= \mathbf{y}^{\mathsf {T}} \operatorname{diag} (\frac{1}{\lambda_1}, \dots, \frac{1}{\lambda_D}) \mathbf{y} \\
&= \sum_{i=1}^D \frac{y_i^2}{\lambda_i}
\end{align*}
$$

in which $\mathbf{y} = U (\mathbf{x} - \boldsymbol{\mu})$.

This can be interpreted as a sequence of shifting (by $\boldsymbol{\mu}$), 
rotating and reflecting (by $U$) operations that transform $\mathbf{x}$ to $\mathbf{y}$.

Setting $\Delta^2$ to constant, we obtain a hyperellipsoid spanned by $\mathbf{x}$, on which the probability density will be constant.
The center are defined by $\boldsymbol{\mu}$ and the length of axes is determined by the eigenvalues of the covariance matrix.

![](/assets/images/prml/2.7.png)

For a Guassian distribution to be well defined, the covariance matrix must be postive definite. I.e. all eigenvalues of the covariance matrix should be positve.

We shall see that a non-postive eigenvalue causes some part of Gaussian integral to diverge,
so the distribution cannot be normalized properly.
Particularly, a zero eigenvalue indicates 
some components of $\mathbf{x}$ are linearly dependent, and thus $\mathbf{x}$ does not span $\mathbb{R}^D$.

In fact, any covariance matrix is positive semi-definite. 
Since for any $\mathbf{a} \in \mathbb{R}^n$ that $\mathbf{a} \ne \mathbf{0}$, we have

$$
\begin{align*}
\mathbf{a}^{\mathsf {T}} \operatorname{cov}(\mathbf{x}, \mathbf{x}) \mathbf{a}
&= \mathbf{a}^{\mathsf {T}} \operatorname{E} \left[ (\mathbf{x}-\operatorname{E}[\mathbf{x}])(\mathbf{x} - \operatorname{E}[\mathbf{x}])^{\mathsf {T}} \right] \mathbf{a} \\
&= \operatorname{E} \left[ \mathbf{a}^{\mathsf {T}} (\mathbf{x}-\operatorname{E}[\mathbf{x}])(\mathbf{x} - \operatorname{E}[\mathbf{x}])^{\mathsf {T}} \mathbf{a} \right] \\
&= \operatorname{E} \left[ \langle \mathbf{a}, \mathbf{x}-\operatorname{E}[\mathbf{x}] \rangle ^2 \right] \ge 0
\end{align*}
$$

\
Using

$$
| \boldsymbol{\Sigma} | = \prod_{i=1}^D \lambda_i
$$

the guassian distribution in terms of $\mathbf{y}$ can also be written as

$$
p(\mathbf{y}) = \prod_{i=1}^{D} \frac{1}{(2 \pi \lambda_i)^{1/2}} \exp \left\{ -\frac{y_i^2}{2 \lambda_i}  \right\}
$$

which is the product of D independent univariate Guassian distributions, and it has the following properties:

$$
\begin{align*}
\int p(\mathbf{y}) \,d\mathbf{y} &= 1 \\
\operatorname{E} [\mathbf{y}] &=  0 \\
\operatorname{cov} (\mathbf{y}, \mathbf{y}) &= \operatorname{diag} (\lambda_1, \dots, \lambda_D)
\end{align*}
$$

The normalization of $\mathbf{y}$ is shown by

$$
\begin{align*}
\int p(\mathbf{y}) \,d\mathbf{y}
&= \int \cdots \int \prod_{i=1}^{D} \frac{1}{(2 \pi \lambda_i)^{1/2}} \exp \left\{ -\frac{y_i^2}{2 \lambda_i}  \right\} \,dy_1 \dots dy_D \\
&= \prod_{i=1}^{D} \int \frac{1}{(2 \pi \lambda_i)^{1/2}} \exp \left\{ -\frac{y_i^2}{2 \lambda_i}  \right\} \,dy_i \\
&= 1
\end{align*}
$$

Since the components of $\mathbf{y}$ are independent, the mean and covariance of $\mathbf{y}$ can be derived from

$$
\begin{align*}
\operatorname{E} [y_i] &= 0 \\
\operatorname{cov} (y_i, y_j) &= 
\begin{cases}
0 &\text{ if } i \ne j \\
\operatorname{var}(y_i) = \lambda_i &\text{ if } i=j 
\end{cases} \\
\end{align*}
$$

Now back to $p(\mathbf{x})$.

To show that $\int p(\mathbf{x}) \,d\mathbf{x} = 1$, we substitute $\mathbf{x}$ with

$$
\mathbf{x} = U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu}
$$

which gives

$$
\begin{align*}
\int p_{\mathbf{x}}(\mathbf{x}) \,d\mathbf{x} 
&= \int p_{\mathbf{x}}(U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu}) \left| \frac{\partial \mathbf{x}}{\partial \mathbf{y}} \right| \,d\mathbf{y} \\
&= \int p_{\mathbf{x}}(U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu}) \left| \mathbf{U}^{\mathsf{T}} \right| \,d\mathbf{y} \\
&= \int p_{\mathbf{y}}(\mathbf{y}) \,d\mathbf{y} \\
&= 1
\end{align*}
$$

where $p\_{\mathbf{x}}$, $p\_{\mathbf{y}}$ denotes the density function about $\mathbf{x}$, $\mathbf{y}$ respectively.
This confirms the multivariate Guassian distribution is normalized.

The expectation of $\mathbf{x}$ is given by

$$
\begin{align*}
\operatorname{E} [\mathbf{x}]
&= \operatorname{E} \left[ U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu} \right] \\
&= U^{\mathsf {T}} \operatorname{E} [\mathbf{y}] + \boldsymbol{\mu} \\
&= \boldsymbol{\mu}
\end{align*}
$$

The second order moment of $\mathbf{x}$ is given by

$$
\begin{align*}
\operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} \right]
&= \operatorname{E} \left[ (U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu}) (U^{\mathsf {T}} \mathbf{y} + \boldsymbol{\mu})^{\mathsf {T}} \right] \\
&= U^{\mathsf {T}} \operatorname{E} \left[ \mathbf{y} \mathbf{y}^{\mathsf {T}} \right] U
+ \boldsymbol{\mu} \operatorname{E}[\mathbf{y}^{\mathsf {T}}] U
+ U^{\mathsf {T}} \operatorname{E}[\mathbf{y}] \boldsymbol{\mu}^{\mathsf {T}}
+ \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= U^{\mathsf {T}} \operatorname{E} \left[ (\mathbf{y}-\mathbf{0}) (\mathbf{y}^{\mathsf {T}}-\mathbf{0}) \right] U
+ \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= U^{\mathsf {T}} \operatorname{diag} (\lambda_1, \dots, \lambda_D) U
+ \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= \boldsymbol{\Sigma} + \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \tag{2.62}
\end{align*}
$$

So the covariance matrix of $\mathbf{x}$ is

$$
\begin{align*}
\operatorname{cov} [\mathbf{x}]
&= \operatorname{E} \left[ (\mathbf{x}-\operatorname{E}[\mathbf{x}])(\mathbf{x} - \operatorname{E}[\mathbf{x}])^{\mathsf {T}} \right] \\
&= \operatorname{E} \left[ \mathbf{x} \mathbf{x}^{\mathsf {T}} \right] - \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}} \\
&= \boldsymbol{\Sigma}
\end{align*}
$$

However, there are some limitations of the Gaussian distribution.

One limitation is that the number of independent parameters grows quadraticly, which incurs significant computation cost. There are $D(D+1)/2$ parameters in $\boldsymbol{\Sigma}$ and $D$ parameters in $\boldsymbol{\mu}$.
By restricting $\boldsymbol{\Sigma}$ to be diagonal, the number of parameters is reduced to $2D$. 
By further restricting $\boldsymbol{\Sigma}$ to be isotropic so that $\boldsymbol{\Sigma} = \sigma^2 I$, the number is reduced to $D+1$.
However, the capability of simplified distribution is also restricted.

Another limitation is that the Gaussian distribution is unimodal.

### Marginal Gaussian distribution

Suppose $\mathbf{x}$ is a $D$-dimensional vector with Gaussian distribution $\mathcal{N} (\mathbf{x} \| \boldsymbol{\mu}, \boldsymbol{\Sigma})$, 
and $\mathbf{x}\_a$, $\mathbf{x}\_b$ are two disjoint subsets obtained by partitioning $\mathbf{x}$.
Without loss of generality, we can write $\mathbf{x}$ as

$$
\mathbf{x} =
\begin{pmatrix}
\mathbf{x}_a \\
\mathbf{x}_b \\
\end{pmatrix}
$$

Correspondingly, the mean $\boldsymbol{\mu}$ takes the form

$$
\boldsymbol{\mu} =
\begin{pmatrix}
\boldsymbol{\mu}_a \\
\boldsymbol{\mu}_b \\
\end{pmatrix}
$$

and the covariance matrix $\boldsymbol{\Sigma}$ takes the form

$$
\boldsymbol{\Sigma} =
\begin{pmatrix}
\boldsymbol{\Sigma}_{aa} & \boldsymbol{\Sigma}_{ab} \\
\boldsymbol{\Sigma}_{ba} & \boldsymbol{\Sigma}_{bb} \\
\end{pmatrix}
$$

Define the **precision matrix** $\Lambda$ as $\Lambda \equiv \boldsymbol{\Sigma}^{-1}$.
Then the partitioned form of the precision matrix is given by

$$
\boldsymbol{\Lambda} =
\begin{pmatrix}
\boldsymbol{\Lambda}_{aa} & \boldsymbol{\Lambda}_{ab} \\
\boldsymbol{\Lambda}_{ba} & \boldsymbol{\Lambda}_{bb} \\
\end{pmatrix}
$$

The relationship between the blocks of $\Lambda$ and that of $\boldsymbol{\Sigma}$ can be derived using

$$
{\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}
={\begin{bmatrix}\mathbf{M}&-\mathbf{M}\mathbf {BD} ^{-1}\\-\mathbf {D} ^{-1}\mathbf {C} \mathbf{M}&\quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} \mathbf{M}\mathbf {BD} ^{-1}\end{bmatrix}} \tag{2.76}
$$

where $\mathbf{M} = \left(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} \right)^{-1}$. This identity can be obtained by performing row operations on the partitioned matrix. 

By definition, the marginal distribution of $\mathbf{x}\_a$ is given by

$$
p(\mathbf{x}_a) = \int p(\mathbf{x}_a, \mathbf{x}_b) \,d\mathbf{x}_b
$$

Expanding the exponent of $p(\mathbf{x})$ using partitioned matrix, we have

$$
\begin{align*}
&-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) = \\
& -\frac{1}{2} (\mathbf{x}_a - \boldsymbol{\mu}_a)^{\mathsf {T}} \boldsymbol{\Lambda}_{aa}(\mathbf{x}_a - \boldsymbol{\mu}_a)
-\frac{1}{2} (\mathbf{x}_a - \boldsymbol{\mu}_a)^{\mathsf {T}} \boldsymbol{\Lambda}_{ab}(\mathbf{x}_b - \boldsymbol{\mu}_b) \\
& -\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\mu}_b)^{\mathsf {T}} \boldsymbol{\Lambda}_{ba}(\mathbf{x}_a - \boldsymbol{\mu}_a)
-\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\mu}_b)^{\mathsf {T}} \boldsymbol{\Lambda}_{bb}(\mathbf{x}_b - \boldsymbol{\mu}_b) \tag{2.70}
\end{align*}
$$

Grouping terms involving $\mathbf{x}\_b$ and completing the squre, we have

$$
-\frac{1}{2} \mathbf{x}_b^{\mathsf {T}} \boldsymbol{\Lambda}_{bb} \mathbf{x}_b
+ \mathbf{x}_b^{\mathsf {T}} \mathbf{m}
= -\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m})^{\mathsf {T}} \boldsymbol{\Lambda}_{bb} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m})
+ \frac{1}{2} \mathbf{m}^{\mathsf {T}} \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m}
\tag{2.84}
$$

in which $\mathbf{m} = \boldsymbol{\Lambda}\_{bb} \boldsymbol{\mu}\_b -\boldsymbol{\Lambda}\_{ba} (\mathbf{x}\_a - \boldsymbol{\mu}\_a)$ is independent of $\mathbf{x}\_b$ but dependent of  $\mathbf{x}\_a$.

The integral over $\mathbf{x}\_b$ is then given by

$$
\int \exp \left\{ -\frac{1}{2} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m})^{\mathsf {T}} \boldsymbol{\Lambda}_{bb} (\mathbf{x}_b - \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m}) \right\} \,d\mathbf{x}_b
$$

which is a Gaussian integral, and the result is the normalization constant of a Gaussian distribution, which will be independent of $\mathbf{x}\_a$.
Therefore the exact result is not required since we can derive the distribution by normalizing with respect to $\mathbf{x}\_a$

Grouping remaining terms in (2.70) and the second term on r.h.s of (2.84), we have

$$
\frac{1}{2} \mathbf{m}^{\mathsf {T}} \boldsymbol{\Lambda}_{bb}^{-1} \mathbf{m}
- \frac{1}{2} \mathbf{x}_a^{\mathsf {T}} \boldsymbol{\Lambda}_{aa} \mathbf{x}_a
+ \mathbf{x}_a^{\mathsf {T}} ( \boldsymbol{\Lambda}_{aa} \boldsymbol{\mu}_a +  \boldsymbol{\Lambda}_{ab} \boldsymbol{\mu}_b) + \text{const} \\
= - \frac{1}{2} \mathbf{x}_a^{\mathsf {T}} (\boldsymbol{\Lambda}_{aa} - \boldsymbol{\Lambda}_{ab} \boldsymbol{\Lambda}_{bb}^{-1} \boldsymbol{\Lambda}_{ba}) \mathbf{x}_a
+ \mathbf{x}_a^{\mathsf {T}} (\boldsymbol{\Lambda}_{aa} - \boldsymbol{\Lambda}_{ab} \boldsymbol{\Lambda}_{bb}^{-1} \boldsymbol{\Lambda}_{ba}) \boldsymbol{\mu}_a + \textrm{const}
$$

where the $\textrm{const}$ concludes terms independent of $\mathbf{x}\_a$.

Since the exponent is a quadratic form in $\mathbf{x}\_a$, the marginal distribution $p(\mathbf{x}\_a)$ will also be a Gaussian.
Instead of integrating over $\mathbf{x}\_a$ to obtain the complete form of $p(\mathbf{x}\_a)$, we inspect the exponent to get 

$$
\operatorname{cov} [\mathbf{x}_a]
= (\boldsymbol{\Lambda}_{aa} - \boldsymbol{\Lambda}_{ab} \boldsymbol{\Lambda}_{bb}^{-1} \boldsymbol{\Lambda}_{ba})^{-1}
$$

Using (2.76), we can also write

$$
\operatorname{cov} [\mathbf{x}_a] = \boldsymbol{\Sigma}_{aa}
$$

The mean is given by 

$$
\operatorname{E} [\mathbf{x}_a] = \boldsymbol{\mu}_a
$$

Therefore, the marginal distribution of $\mathbf{x}\_a$ is given by

$$
p(\mathbf{x}_a) = \mathcal{N} (\mathbf{x}_a | \boldsymbol{\mu}_a, \boldsymbol{\Sigma}_{aa})
$$

### Conditional Gaussian distribution

By definition, the probability distribution of $\mathbf{x}\_a$ conditioned on $\mathbf{x}\_b$ is given by

$$
p(\mathbf{x}_a | \mathbf{x}_b)
= \frac{p(\mathbf{x}_a, \mathbf{x}_b)}{p(\mathbf{x}_b)}
= \frac{p(\mathbf{x})}{p(\mathbf{x}_b)}
$$

One way to evaluate the $p(\mathbf{x}\_a \| \mathbf{x}\_b)$ is to obtain the marginal distribution $p(\mathbf{x}\_b)$ first 
and divede it with $p(\mathbf{x})$. However it is more efficient to obtain the result by normalizing the function about $\mathbf{x}\_a$.

By fixing $\mathbf{x}\_b$, $p(\mathbf{x}\_b)$ will be constant and becomes a part of the normalization constant,
and the exponent of $p(\mathbf{x}\_a, \mathbf{x}\_b)$ will be a quadratic form of $\mathbf{x}\_a$, which indicates the conditional distribution is also a Gaussian.

Grouping terms involving $\mathbf{x}\_a$ in the exponent, we have

$$
-\frac{1}{2} \mathbf{x}_a^{\mathsf {T}} \boldsymbol{\Lambda}_{aa} \mathbf{x}_a
+ \mathbf{x}_a^{\mathsf {T}} \left[ \boldsymbol{\Lambda}_{aa} \boldsymbol{\mu}_a -\boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \right]
$$

By inspection, the covariance of $p(\mathbf{x}\_a \| \mathbf{x}\_b)$ is given by

$$
\boldsymbol{\Sigma}_{a | b} = \boldsymbol{\Lambda}_{aa}^{-1}
$$

and the mean is given by

$$
\begin{align*}
\boldsymbol{\mu}_{a | b} 
&= \boldsymbol{\Sigma}_{a | b} \left[ \boldsymbol{\Lambda}_{aa} \boldsymbol{\mu}_a -\boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \right] \\
&= \boldsymbol{\mu}_a - \boldsymbol{\Lambda}_{aa}^{-1} \boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \\
\end{align*}
$$

Substituting precision matrices with covariance matrices, they can be written

$$
\begin{align*}
\boldsymbol{\mu}_{a | b} &= \boldsymbol{\mu}_a + \boldsymbol{\Sigma}_{ab} \boldsymbol{\Sigma}_{bb}^{-1} (\mathbf{x}_b - \boldsymbol{\mu}_b) \\
\boldsymbol{\Sigma}_{a | b} &= \boldsymbol{\Sigma}_{aa} - \boldsymbol{\Sigma}_{ab} \boldsymbol{\Sigma}_{bb}^{-1} \boldsymbol{\Sigma}_{ba} \\
\end{align*}
$$

which gives a more complex form.

Thus the distribution of $\mathbf{x}\_a$ conditioned on $\mathbf{x}\_b$ is given by

$$
\begin{align*}
p(\mathbf{x}_a | \mathbf{x}_b) &= \mathcal{N} (\mathbf{x}_a | \boldsymbol{\mu}_{a | b}, \boldsymbol{\Lambda}_{aa}^{-1}) \\
\boldsymbol{\mu}_{a | b} &= \boldsymbol{\mu}_a - \boldsymbol{\Lambda}_{aa}^{-1} \boldsymbol{\Lambda}_{ab} (\mathbf{x}_b - \boldsymbol{\mu}_b) \\
\end{align*}
$$

### Bayes' theorem for Gaussian distribution

A very common problem we will encounter is that given $p(\mathbf{x})$ and $p(\mathbf{y} \| \mathbf{x})$, find $p(\mathbf{y})$ and $p(\mathbf{x} \| \mathbf{y})$.

Suppose

$$
\begin{align*}
p(\mathbf{x}) &= \mathcal{N} (\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}) \\
p(\mathbf{y} | \mathbf{x}) &= \mathcal{N} (\mathbf{y} | A \mathbf{x} + \mathbf{b}, \mathbf{L}^{-1}) \\
\end{align*}
$$

where $\boldsymbol{\Lambda}$ and $\mathbf{L}$ are precision matrices, $\mathbf{x}$ and $\mathbf{y}$ have demensionality M and D respectively, and $A$ is a $D \times M$ matrix.

Let

$$
\mathbf{z} =
\begin{pmatrix}
\mathbf{x} \\
\mathbf{y} \\
\end{pmatrix}
$$

The log joint distribution is then given by

$$
\begin{align*}
\ln p(\mathbf{z}) &= \ln p(\mathbf{y} | \mathbf{x}) + \ln p(\mathbf{x}) \\
&= -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Lambda} (\mathbf{x} - \boldsymbol{\mu}) 
-\frac{1}{2} (\mathbf{y} - A \mathbf{x} - \mathbf{b})^{\mathsf {T}} \mathbf{L} (\mathbf{y} - A \mathbf{x} - \mathbf{b}) + \mathrm{const} \\
\end{align*}
$$

where 'const' concludes terms independent of $\mathbf{x}$ and $\mathbf{y}$.

Grouping the second order terms gives

$$
-\frac{1}{2} \mathbf{x}^{\mathsf {T}} (\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A) \mathbf{x}
-\frac{1}{2} \mathbf{y}^{\mathsf {T}} \mathbf{L} \mathbf{y}
+\frac{1}{2} \mathbf{y}^{\mathsf {T}} \mathbf{L} A \mathbf{x}
+\frac{1}{2} \mathbf{x}^{\mathsf {T}} A^{\mathsf {T}} \mathbf{L} \mathbf{y} \\
= -\frac{1}{2}
\begin{pmatrix} \mathbf{x} \\ \mathbf{y} \\ \end{pmatrix} ^{\mathsf {T}}
\begin{pmatrix}
\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A & - A^{\mathsf {T}} \mathbf{L} \\
- \mathbf{L} A & \mathbf{L} \\
\end{pmatrix}
\begin{pmatrix} \mathbf{x} \\ \mathbf{y} \\ \end{pmatrix}
= \mathbf{z}^{\mathsf {T}} \mathbf{R} \mathbf{z}
$$

Thus the precision matrix of $\mathbf{z}$ is obtained

$$
\mathbf{R} =
\begin{pmatrix}
\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A & - A^{\mathsf {T}} \mathbf{L} \\
- \mathbf{L} A & \mathbf{L} \\
\end{pmatrix}
$$

Using (2.76), the corresponding covariance matrix is given by

$$
\operatorname{cov}[\mathbf{z}] = \mathbf{R}^{-1}
= \begin{pmatrix}
\boldsymbol{\Lambda}^{-1} & \boldsymbol{\Lambda}^{-1} A^{\mathsf {T}} \\
A \boldsymbol{\Lambda}^{-1} & \mathbf{L}^{-1} + A \boldsymbol{\Lambda}^{-1} A^{\mathsf {T}}\\
\end{pmatrix}
$$

Grouping the linear terms, we have

$$
\mathbf{x}^{\mathsf {T}} \boldsymbol{\Lambda} \boldsymbol{\mu}
- \mathbf{x}^{\mathsf {T}} A^{\mathsf {T}} \mathbf{L} \mathbf{b}
+ \mathbf{y}^{\mathsf {T}} \mathbf{L} \mathbf{b}
= \begin{pmatrix} \mathbf{x} \\ \mathbf{y} \\ \end{pmatrix} ^{\mathsf {T}}
\begin{pmatrix} \boldsymbol{\Lambda} \boldsymbol{\mu} - A^{\mathsf {T}} \mathbf{L} \mathbf{b} \\ \mathbf{L} \mathbf{b} \\ \end{pmatrix}
$$

The mean of $\mathbf{z}$ is then given by

$$
\operatorname{E}[\mathbf{z}] = \mathbf{R}^{-1} 
\begin{pmatrix} \boldsymbol{\Lambda} \boldsymbol{\mu} - A^{\mathsf {T}} \mathbf{L} \mathbf{b} \\ \mathbf{L} \mathbf{b} \\ \end{pmatrix}
= \begin{pmatrix} \boldsymbol{\mu} \\ A \boldsymbol{\mu} + \mathbf{b} \\ \end{pmatrix}
$$

From the joint distribution of $\mathbf{z}$, we obtain the marginal distribution $p(\mathbf{y})$

$$
\begin{align*}
\operatorname{E}[\mathbf{y}] &= A \boldsymbol{\mu} + \mathbf{b} \\
\operatorname{cov}[\mathbf{y}] &= \mathbf{L}^{-1} + A \boldsymbol{\Lambda}^{-1} A^{\mathsf {T}} \\
\end{align*}
$$

and the conditional distribution $p(\mathbf{x} \| \mathbf{y})$

$$
\begin{align*}
\operatorname{E}[\mathbf{x} | \mathbf{y}] 
&= (\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A) ^{-1} 
\left[ A^{\mathsf {T}} \mathbf{L} (\mathbf{y} - \mathbf{b}) + \boldsymbol{\Lambda} \boldsymbol{\mu} \right] \\
\operatorname{cov}[\mathbf{x} | \mathbf{y}] &= (\boldsymbol{\Lambda} + A^{\mathsf {T}} \mathbf{L} A) ^{-1} \\
\end{align*}
$$

### Maximum likelihood for Gaussian

Given a data set $\mathbf{X} = (\mathbf{x}\_1, \dots, \mathbf{x}\_N)^{\mathsf{T}}$ with each observation drawn independently from a Gaussian distribution,
the log likelihood function is given by

$$
\ln p(\mathbf{X} | \boldsymbol{\mu}, \boldsymbol{\Sigma})
= - \frac{ND}{2} \ln (2 \pi) - \frac{N}{2} \ln |\boldsymbol{\Sigma}|
- \frac{1}{2} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Sigma}^{-1} (\mathbf{x}_n - \boldsymbol{\mu})
$$

Setting derivative with respect to $\boldsymbol{\mu}$ to zero

$$
\frac{\partial }{\partial \boldsymbol{\mu}} \ln p(\mathbf{X} | \boldsymbol{\mu}, \boldsymbol{\Sigma})
= \sum_{n=1}^N \boldsymbol{\Sigma}^{-1} (\mathbf{x}_n - \boldsymbol{\mu}) = 0
$$

Then we obtain

$$
\boldsymbol{\mu}_{\textrm{ML}} = \frac{1}{N} \sum_{n=1}^N \mathbf{x}_n
\tag{1.121}
$$

Setting derivative with respect to $\boldsymbol{\Sigma}$ to zero, we have

$$
- \frac{N}{2} \boldsymbol{\Sigma}^{-\mathsf{T}}
+ \frac{1}{2} \sum_n \boldsymbol{\Sigma}^{-\mathsf{T}} (\mathbf{x}_n - \boldsymbol{\mu})(\mathbf{x}_n - \boldsymbol{\mu})^{\mathsf{T}} \boldsymbol{\Sigma}^{-\mathsf{T}}
= \mathbf{0}
$$

Solving for $\boldsymbol{\Sigma}$, we obtain

$$
\boldsymbol{\Sigma}_{\textrm{ML}} = \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}}) (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}})^{\mathsf {T}}
$$

Thus we see the sufficient statistics for the Gassuain distribution are given by $\sum\_n \mathbf{x}\_n$ and $\sum\_n \mathbf{x}\_n \mathbf{x}\_n^{\mathsf {T}}$.

**Excercise 2.35**

Evaluating the expectation of estimated mean gives

$$
\operatorname{E} [\boldsymbol{\mu}_{\textrm{ML}}] = \frac{1}{N} \sum_{n=1}^N \operatorname{E} [\mathbf{x}_n] = \boldsymbol{\mu}
$$

And the expectation of the covariance matrix is evaluated by

$$
\begin{align*}
\operatorname{E} [\boldsymbol{\Sigma}_{\textrm{ML}}]
&= \frac{1}{N} \sum_{n=1}^N \operatorname{E} \left[ (\mathbf{x}_n - \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i) (\mathbf{x}_n - \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i)^{\mathsf {T}} \right] \\
&= \frac{1}{N} \sum_{n=1}^N \operatorname{E} \left[ \mathbf{x}_n \mathbf{x}_n^{\mathsf {T}} - \frac{2}{N} \mathbf{x}_n \sum_{i=1}^N \mathbf{x}_i^{\mathsf {T}}
+ \frac{1}{N^2} \sum_{i=1}^N \mathbf{x}_i \sum_{i=1}^N \mathbf{x}_i^{\mathsf {T}} \right]
\end{align*}
$$

Using (2.62), for $i = j$ we have 

$$
\operatorname{E} [\mathbf{x}_i \mathbf{x}_j^{\mathsf {T}} ] 
= \operatorname{E} [\mathbf{x}_i \mathbf{x}_i^{\mathsf {T}} ] 
= \boldsymbol{\Sigma} + \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}}
$$

For $i \ne j$, since $\mathbf{x}\_i$ and $\mathbf{x}\_j$ is independent, we have

$$
\operatorname{E} [\mathbf{x}_i \mathbf{x}_j^{\mathsf {T}} ]
= \operatorname{E} [\mathbf{x}_i] \operatorname{E} [ \mathbf{x}_j^{\mathsf {T}} ]
= \boldsymbol{\mu} \boldsymbol{\mu}^{\mathsf {T}}
$$

By substitution, we have

$$
\operatorname{E} [\boldsymbol{\Sigma}_{\textrm{ML}}] = \frac{N-1}{N} \boldsymbol{\Sigma}
$$

which means $\boldsymbol{\Sigma}\_{\textrm{ML}}$ is a biased estimator for the covariance.

The bias can be corrected by using the estimator

$$
\tilde{\boldsymbol{\Sigma}} = \frac{1}{N-1} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}}) (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}})^{\mathsf {T}}
$$

### Sequential estimation

Sequential methods are useful for on-line applications or large data sets.

Consider the estimator $\boldsymbol{\mu}\_{\textrm{ML}}$ given by (1.121). It can be written in the form

$$
\begin{align*}
\boldsymbol{\mu}_{\textrm{ML}}^{(N)}
&= \frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_n \\
&= \frac{1}{N} \mathbf{x}_N + \frac{1}{N} \sum_{n=1}^{N-1} \mathbf{x}_n \\
&= \frac{1}{N} \mathbf{x}_N + \frac{N-1}{N} \boldsymbol{\mu}_{\textrm{ML}}^{(N-1)} \\
&= \boldsymbol{\mu}_{\textrm{ML}}^{(N-1)} + \frac{1}{N} (\mathbf{x}_n - \boldsymbol{\mu}_{\textrm{ML}}^{(N-1)}) \tag{2.126} \\
\end{align*}
$$

So the estimation based on $N$ obsevations can be obtained directly from the point $\mathbf{x}\_N$ and the estimation based on previous $N-1$ observations.

A more general form of sequential algorithm is given by the **Robbins-Monro algorithm**, an stochastic appximation method.
Let $\mathbf{z}$ be a random variable governed by a parameter $\theta$.
The expectation of $z$ is then a function of $\theta$, so that

$$
f(\theta) = \operatorname{E} [z|\theta]
$$

This is kwons as a **regression function**.

Our goal is to find a root $\theta^\ast$ at which $f(\theta^\ast) = 0$. Provided that

- The variance of $z$ is bounded for any choice of $\theta$
- $f(\theta) < 0$ for $\theta < \theta^\ast$ and $f(\theta) > 0$ for $\theta > \theta^\ast$

and with a sequence $\{ a\_N\}$ that satisfies

$$
\begin{align*}
a_N &\ge 0 \\
\sum_{N=1}^{\infty} a_N &= \infty \tag{2.131} \\
\sum_{N=1}^{\infty} a_N &< \infty \tag{2.132} \\
\end{align*}
$$

the iteration given by

$$
\theta^{(N)} = \theta^{(N - 1)} - a_{N-1} z(\theta^{(N-1)})
$$

will converge to the root $\theta^\ast$, where $z(\theta^{N-1})$ is an observation of $z$ with parameter $\theta = \theta^{(N-1)}$.
A example of such sequence $\{ a\_N \}$ is $a\_N = 1/N$.

A loose argument for correctness is as follows. The bounded variance ensures values of $z$ have a strong enough tendency to its mean.
Then if we take the value of $z$ to be its mean $f(\theta)$, we can see the iteration will drive $\theta$ towards $\theta^\ast$.
(2.131) ensures that the iteration does not converge too early to reach the root.
(2.132) ensures that the variance of $z$ does not break convergence.

Now we will see how it applies to the maximum likelihood problem.

Setting the derivative of log likelihood function with repsect to $\theta$ to zero, we have

$$
\frac{\partial}{\partial \theta} \left[ \frac{1}{N} \sum_n \ln p(x_n|\theta) \right] = 0
$$

Taking the limit $N \rightarrow \infty$

$$
\begin{align*}
\lim_{N \rightarrow \infty} \frac{\partial}{\partial \theta} \left[ \frac{1}{N} \sum_n \ln p(x_n|\theta) \right] 
&= \lim_{N \rightarrow \infty} \frac{1}{N} \sum_n \frac{\partial}{\partial \theta} \ln p(x_n|\theta) \\
&= \operatorname{E}_{x} \left[ \frac{\partial}{\partial \theta} \ln p(x|\theta) \right]
\end{align*}
$$

Identifying $z(\theta) = \frac{\partial}{\partial \theta} \ln p(x\|\theta)$,
we see that the maximum likelihood solution is given by the root of a regression function.
If the conditions are met, we can apply the Robbins-Monro algorithm to give

$$
\theta^{(N)} = \theta^{(N - 1)} - a_{N-1} \frac{\partial}{\partial \theta} \ln p(x|\theta)
$$

As a pariticular example, consider a gaussian random variable $x$ of which we want to estimate $\mu\_{\text{ML}}$.
The random variable $z$ is given by

$$
z(\mu_{\text{ML}}) = \frac{\partial}{\partial \mu_{\text{ML}}} \ln p(x|\mu_{\text{ML}})
= - \frac{1}{\sigma^2} (x - \mu_{\text{ML}})
$$

The regression function is given by

$$
f(\mu_{\text{ML}}) = - \frac{1}{\sigma^2} ( \mu - \mu_{\text{ML}})
$$

from which we verity that the root is the estimator of the mean of $x$.

Choosing $a\_N = \sigma^2 / N$, we obtain the univaraite form of (2.126).

For further generalization to the numtivariate case, see Blum, Julius R. “Multidimensional Stochastic Approximation Methods.”

### Bayesian inference for Gaussian distribution

Consider a univariate gaussian distribution with known $\sigma^2$ and unknown $\mu$. 

Given a set of observations $\mathbf{X} = \{ x\_1, \dots, x\_N \}$, the likelihood $p(\mathbf{X} \| \mu)$ is

$$
p(\mathbf{X} | \mu) = \prod_{n=1}^N p(x_n | \mu)
= \frac{1}{(2\pi \sigma^2)^{N/2}} \exp \left\{ - \frac{1}{2 \sigma^2} \sum_{n=1}^N (x_n - \mu)^2 \right\}
$$

For $\mu$ , we choose a conjugate prior

$$
p(\mu) = \mathcal{N} (\mu | \mu_0, \sigma_0^2)
$$

By Bayes' theorem, the posterior distribution satisfies

$$
p(\mu|\mathbf{X}) \propto p(\mu) p(\mathbf{X}|\mu)
$$

Since the exponent of $p(\mu) p(\mathbf{X}\|\mu)$ takes the form

$$
- \frac{1}{2} \mu^2 \left( \frac{1}{\sigma_0^2} + \frac{N}{\sigma^2} \right)
+ \mu \left( \frac{1}{\sigma_0^2} \mu_0 + \frac{1}{\sigma^2} \sum_{n=1}^N x_n \right)
+ \textrm{constant}
$$

we have

$$
p(\mu|\mathbf{X}) =  \mathcal{N} (\mu | \mu_N, \sigma_N^2)
$$

where

$$
\begin{align*}
\frac{1}{\sigma_N^2} &= \frac{1}{\sigma_0^2} + \frac{N}{\sigma^2} \\
\mu_N &= \frac{\sigma^2}{N \sigma_0^2 + \sigma^2} \mu_0 + \frac{N \sigma_0^2}{N \sigma_0^2 + \sigma^2} \mu_{\mathrm{ML}} \\
\mu_{\textrm{ML}} &= \frac{1}{N} \sum_{n=1}^N x_n \\
\end{align*}
$$

Note that

- When $N=0$ the posterior reduces to the prior.
- $N \rightarrow \infty$, we have $\mu\_N \rightarrow \mu\_{\mathrm{ML}}$ and $\sigma\_N^2 \rightarrow 0$
- As the number of observations grows, the precision of the posterior increases
- In the limit $\sigma\_0^2 \rightarrow \infty$, the effect of the prior vanishes, since $\mu\_N \rightarrow \mu\_{\mathrm{ML}}$ and $\displaystyle \frac{1}{\sigma\_N^2} \rightarrow \frac{N}{\sigma^2}$. 

Another approach to evaluate the posterior is by using the results in Section 2.3.3, with following change of variables

$$
\begin{align*}
\mathbf{x} &\rightarrow \mu,
& \boldsymbol{\mu} &\rightarrow \mu_0,
& \boldsymbol{\Lambda}^{-1} &\rightarrow \sigma_0^2 I \\
\mathbf{y} &\rightarrow \mathbf{X},
&A &\rightarrow \mathbf{1},
& b &\rightarrow \mathbf{0},
&\mathbf{L}^{-1} &\rightarrow \sigma^{2}I \\
\end{align*}
$$

As a general rule, we should note that Bayesian method provides a natural way of sequential learning. This can be shown by

$$
p(\mu|\mathbf{X}) \propto \left[ p(\mu) \prod_{n=1}^{N-1} p(x_n|\mu) \right]  p(x_N|\mu)
$$

in which the posterior distribution of the previous $N-1$ observations is also a prior for subsequent observation $x\_n$.

\
Now suppose the mean is known and the variance is unknown. For convenience, define the precision $\lambda \equiv 1/{\sigma^2}$.

Then the conjugate prior can be constructed proportional to

$$
\lambda^{a-1} \exp (-b \lambda)
$$

**Excercise 2.41**

The normalization starts from the gamma function

$$
\Gamma (a) = \int _{0}^{\infty }t^{a-1}e^{-t}\,dt
$$

By substituting $t$ with $t = b \lambda$, we have

$$
\Gamma (a) = \int _{0}^{\infty } (b \lambda)^{a-1}e^{- b \lambda} b \,d\lambda
= b^a\int _{0}^{\infty } \lambda^{a-1}e^{- b \lambda} \,d\lambda
$$

Thereby we obtain the **gamma distribution**

$$
\mathrm{Gam}(\lambda|a,b) = \frac{1}{\Gamma(a)} b^a \lambda^{a-1} \exp (- b \lambda)
$$

where $a, b>0$. When $a \ge 1$, the density values are bounded.

Using the similar approach, the following properties are obtained

$$
\begin{align*}
\operatorname{E} [\lambda] &= \frac{a}{b} \\
\operatorname{var} [\lambda] &= \frac{a}{b^2} \\
\operatorname{mode} [\lambda] &= \frac{a-1}{b} \quad \text{ for } a \ge 1 \\
\end{align*}
$$

Define a prior over $\lambda$ such that

$$
p(\lambda) = \mathrm{Gam}(\lambda|a_0,b_0)
$$

Then the posterior is given by

$$
p(\lambda|\mathbf{X}) \propto \lambda^{a_0 - 1} \lambda^{N/2} \exp \left\{ -b_0 \lambda - \frac{\lambda}{2} \sum_{n=1}^N (x_n - \mu)^2 \right\}
$$

which takes the form of a gamma distribution. By inspection, we obtain

$$
p(\lambda|\mathbf{X}) = \mathrm{Gam}(\lambda|a_N,b_N)
$$

where we defined

$$
\begin{align*}
a_N &= a_0 + \frac{N}{2} \\
b_N &= b_0 + \frac{N}{2} \sigma_{\textrm{ML}}^2 \\
\sigma_{\textrm{ML}}^2 &= \frac{1}{N} \sum_n (x_n - \mu)^2 \\
\end{align*}
$$

Here, $2a\_0$ can be interpreted as the number of fictitious observations.
By writing $b\_0$ as $\displaystyle \frac{2 a\_0}{2} \frac{b\_0}{a\_0}$,
we see $\displaystyle \frac{b\_0}{a\_0}$ can be interpreted as the variance of fictitious data.

Instead of working with the precision, the conjugate prior over the variance is known as the inverse-gamma distribution.

\
Now suppose the mean and the variance are both unknown.

The likelihood function can be written as

$$
\begin{align*}
p(\mathbf{X} | \mu, \lambda)
&= \left( \frac{\lambda}{2\pi} \right)^{N/2} \exp \left\{ - \frac{\lambda}{2} \sum_{n=1}^N (x_n - \mu)^2 \right\} \\
&\propto \lambda^{N/2} \exp \left\{ - \frac{N \lambda}{2} \left( \mu^2 - 2 \mu \frac{\sum_n x_n}{N} + \frac{\sum_n x_n^2}{N} \right) \right\}
\end{align*}
$$

Defining $\displaystyle c=\frac{\sum\_n x\_n}{N}, d=\frac{\sum\_n x\_n^2}{N}$ and rearranging terms, we have

$$
\begin{align*}
p(\mathbf{X} | \mu, \lambda)
&\propto \lambda^{N/2} \exp \left\{ - \frac{N \lambda}{2} \left[ (\mu - c)^2 + d - c^2 \right] \right\} \\
&= \lambda^{1/2} \exp \left\{ - \frac{N \lambda}{2} \left( \mu - c \right)^2 \right\}
\lambda^{(N-1)/2} \exp \left\{ - \frac{N}{2} (d - c^2) \lambda \right\}
\end{align*}
$$

which is a product of a gamma distribution over $\lambda$ and a Gaussian distribution conditioned on $\lambda$. 

Therefore, the prior distribution $p(\mu, \lambda)$ can take the form

$$
p(\mu, \lambda) = \mathcal{N} (\mu | \mu_0, (\beta \lambda)^{-1}) \mathrm{Gam}(\lambda|a,b)
$$

which is called the **normal-gamma distribution** or gaussian-gamma distribution. 
Note that, $\mu$ is conditioned on $\lambda$.

\
Now consider a multivariate Gaussian $\mathcal{N} (\mathbf{x} \| \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})$ 
with a set of observations $\mathbf{X} = \{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$. The likelihood function is given by

$$
p(\mathbf{X}|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})
= \prod_{n=1}^N p(\mathbf{x}_n|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})
= \frac{1}{(2 \pi)^{DN/2}} |\boldsymbol{\Lambda}|^{N/2} \exp \left\{ - \frac{1}{2} \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Lambda} (\mathbf{x}_n - \boldsymbol{\mu}) \right\}
$$

**Excercise 2.40**

Suppose the precision $\boldsymbol{\Lambda}$ is known and the mean $\boldsymbol{\mu}$ is known.
We choose a prior of $\boldsymbol{\mu}$ as

$$
p(\boldsymbol{\mu}) = \mathcal{N} (\boldsymbol{\mu} | \boldsymbol{\mu}_0, \boldsymbol{\Lambda}_0^{-1})
$$

By Bayes' theorem

$$
p(\boldsymbol{\mu} | \mathbf{X}) 
\propto p(\boldsymbol{\mu}) p(\mathbf{X}|\boldsymbol{\mu})
$$

Inspecting the exponent

$$
- \frac{1}{2} \boldsymbol{\mu}^{\mathsf {T}} (\boldsymbol{\Lambda}_0 + N \boldsymbol{\Lambda}) \boldsymbol{\mu}
+ \boldsymbol{\mu}^{\mathsf {T}} \left( \boldsymbol{\Lambda} \boldsymbol{\mu}_0 + \boldsymbol{\Lambda} \sum_n \mathbf{x}_n \right)
+ \textrm{const}
$$

where the $\text{const}$ concludes terms independent of $\boldsymbol{\mu}$.
Thereby the posterior is a Gaussian distribution defined by

$$
p(\boldsymbol{\mu} | \mathbf{X}) = \mathcal{N} (\boldsymbol{\mu} | \boldsymbol{\mu}_N, \boldsymbol{\Lambda}_N^{-1})
$$

where

$$
\begin{align*}
\boldsymbol{\Lambda}_N &= \boldsymbol{\Lambda}_0 + N\boldsymbol{\Lambda} \\
\boldsymbol{\mu}_N &= \boldsymbol{\Lambda}_N^{-1} (\boldsymbol{\Lambda}_0 \boldsymbol{\mu}_0 + N \boldsymbol{\Lambda} \boldsymbol{\mu}_{\mathrm{ML}}) \\
&= (\boldsymbol{\Lambda}_0 + N\boldsymbol{\Lambda})^{-1} (\boldsymbol{\Lambda}_0 \boldsymbol{\mu}_0 + N \boldsymbol{\Lambda} \boldsymbol{\mu}_{\mathrm{ML}}) \\
\boldsymbol{\mu}_{\mathrm{ML}} &= \frac{1}{N} \sum_n \mathbf{x}_n \\
\end{align*}
$$


If the mean $\boldsymbol{\mu}$ is known and the precision $\boldsymbol{\Lambda}$ is unknown. The conjugate prior for the precision is the **Wishart distribution** given by

$$
\mathcal{W} (\boldsymbol{\Lambda} | \mathbf{W}, \nu) = B |\boldsymbol{\Lambda}| ^{(\nu - D - 1)/2} \exp \left\{ - \frac{1}{2} \operatorname{Tr} (\mathbf{W}^{-1} \boldsymbol{\Lambda}) \right\}
$$

where $\nu$ is degree of freedom, $\mathbf{W}$ is symmetric $D \times D$ matrix.

**Excercise 2.45**

TODO: Show conjugacy

The normalization constatnt $B$ is given by

$$
B(\mathbf{W}, \nu) = |\mathbf{W}|^{-\nu/2} \left[ 2^{\nu D / 2} \pi ^{D(D-1)/4}
\prod_{i=1}^D \Gamma \left( \frac{\nu+1-i}{2} \right) \right]^{-1}
$$

If both the mean and the precision is unkown, the conjugate prior is defined by the **normal-Wishart** distribution

$$
p(\boldsymbol{\mu},\boldsymbol{\Lambda} |\boldsymbol{\mu}_0, \beta, \mathbf{W}, \nu)
= \mathcal{N} (\boldsymbol{\mu}|\boldsymbol{\mu}_0, (\beta \boldsymbol{\Lambda})^{-1})
\mathcal{W} (\boldsymbol{\Lambda}|\mathbf{W}, \nu)
$$

### Student's t-distribution

**Excercise 2.46**

Beginning with the univariate case, we integrate out the precision from the normal-gamma prior distribution

$$
\begin{align*}
p(x | \mu, a, b)
&= \int_0^\infty \mathcal{N} (x|\mu, \tau^{-1}) \mathrm{Gam} (\tau | a, b) \\
&= \int_0^\infty \frac{b^a \tau^{a-1} e^{-b \tau}}{\Gamma(a)} \left( \frac{\tau}{2 \pi} \right)^{1/2} \exp \left\{ - \frac{\tau}{2} (x - \mu)^2 \right\} \,d\tau \\
&= \frac{b^a}{(2\pi)^{1/2} \Gamma(a)} \int_0^\infty \tau^{a - \frac{1}{2}} \exp \left\{ -\left[ \frac{1}{2} (x-\mu)^2 + b \right] \tau \right\} \,d\tau \\
\end{align*}
$$

Subtituting $\tau$ with $t = \left[ \frac{1}{2} (x-\mu)^2 + b \right] \tau$, we have

$$
p(x | \mu, a, b)
= \frac{b^a \Gamma(a+\frac{1}{2})}{(2\pi)^{1/2} \Gamma(a)}  \left[ \frac{1}{2} (x-\mu)^2 + b \right]^{-a-\frac{1}{2}}
$$

Making parameter transformations $\nu = 2a$ and $\lambda = a/b$, we have

$$
\mathrm{St} (x | \mu, \nu, \lambda)
= \frac{\Gamma(\frac{\nu + 1}{2})}{\Gamma(\frac{\nu}{2})} \left( \frac{\lambda}{\pi\nu} \right)^{1/2} \left[ 1 + \frac{\lambda}{\nu} (x-\mu)^2 \right]^{-(\nu+1)/2}
$$

which is known as **(localtion-scale) t-distribution**. $\nu$ is called the **degrees of freedom**.

As a particular case of $\nu=1$, the distribution reduces to **cauchy** distribution, 
which has an unbounded variance.

**Excercise 2.49**

As $\nu \rightarrow \infty$, the t-distribution converges to $\mathcal{N} (\mu, \lambda^{-1})$. This can be shown by taking the limit of the term

$$
\begin{align*}
&\lim_{\nu \rightarrow \infty} \left[ 1 + \frac{\lambda}{\nu} (x-\mu)^2 \right]^{-(\nu+1)/2} \\
=& \lim_{t \rightarrow \infty} \left[ \left( 1 + \frac{1}{t} \right)^{t} \right]^{- \lambda (x-\mu)^{2}/2} \cdot \left( 1 + \frac{1}{t}\right)^{-1/2} \\
=& \exp \left\{ - \frac{\lambda}{2} (x-\mu)^2 \right\}
\end{align*}
$$

where we made the substitution $\nu = \lambda (x-\mu)^2 t$.  Since the remaining terms are independent of $x$,
the normalization constant can be easily obtained by inspecting the exponent,
instead of taking the limit of the normalization constant of t-stribution.
However, we can derive the normalilzation constant directly from the t-distribution,
by taking limit of $\Gamma(z)$ with the Stirling's approximation.

t-distribution can be interpreted as a sum of infinite number of Gaussian distributions that have identical mean and various precisions.
It has heavier tails than the normal distribution and exhibits robustness, which means it is less sensitive to the presence of outiliers.

TODO: plot illustration of robustness

A generalization to the multivariate t-distribution can be obtained by

$$
\mathrm{St}(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)
= \int_0^\infty \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta
$$

Using the same technique as the univariate case, we obtain

$$
\mathrm{St}(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)
= \frac{\Gamma(\frac{D+\nu}{2})}{\Gamma(\frac{\nu}{2})} \frac{|\boldsymbol{\Lambda}|^{1/2}}{(\pi \nu)^{D/2}} \left( 1 + \frac{\Delta^2}{\nu} \right)^{-(D+\nu)/2}
$$

in which

$$
\Delta^2 = (\mathbf{x} - \boldsymbol{\mu})^{\mathsf {T}} \boldsymbol{\Lambda} (\mathbf{x} - \boldsymbol{\mu})
$$

and $D$ is the demensionality of $\mathbf{x}$.

It has following properties

$$
\begin{align*}
\operatorname{E}[\mathbf{x}] &= \boldsymbol{\mu} &\textrm{if} \;\; \nu > 1 \\
\operatorname{cov} [\mathbf{x}] &= \frac{\nu}{\nu-2} \boldsymbol{\Lambda}^{-1} &\textrm{if} \;\; \nu > 2 \\
\operatorname{mode}[\mathbf{x}] &= \boldsymbol{\mu} & \\
\end{align*}
$$

which also apply to the univariate case.

**Excercise 2.49**

The mean is derived from the integral

$$
\begin{align*}
& \int_{\mathbb{R}_D} \int_0^\infty \mathbf{x} \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta d\mathbf{x} \\
=& \int_0^\infty \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta \int_{\mathbb{R}_D} \mathbf{x} \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \,d\mathbf{x} \\
=& \boldsymbol{\mu}
\end{align*}
$$

The covariance is shown by

$$
\begin{align*}
& \int_0^\infty \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta
\int_{\mathbb{R}_D} ( \mathbf{x} - \boldsymbol{\mu} )^2 \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, (\eta \boldsymbol{\Lambda})^{-1}) \,d\mathbf{x} \\
=&\int_0^\infty (\eta \boldsymbol{\Lambda})^{-1} \mathrm{Gam}(\eta|\nu/2, \nu/2) \,d\eta \\
=& \boldsymbol{\Lambda}^{-1} \frac{(\nu/2)^{\nu/2}}{\Gamma(\nu/2)} \int_0^\infty \eta^{\nu/2 - 2} \exp \left( - \frac{\nu}{2} \eta \right) \,d\eta \\
=& \boldsymbol{\Lambda}^{-1} \frac{(\nu/2)^{\nu/2}}{\Gamma(\nu/2)} \frac{\Gamma(\nu/2 - 1)}{(\nu/2)^{\nu/2-1}} \\
=&  \frac{\nu}{\nu-2} \boldsymbol{\Lambda}^{-1} \\
\end{align*}
$$

Since $\mathrm{Gam}(\eta\|\nu/2, \nu/2)$ is non-negtive and independent of $\mathbf{x}$,
the mode of t-distribution will correspond to that of normal distribution given by $\boldsymbol{\mu}$.

> The valid range of $\nu$ is unobvious in the derivation above. Though, by integrating the t-distribution directly we can observe the valid range for $\nu$. 
> My guess is that for some $\nu$, the improper integral diverges in some different order of integration.

### Periodic variables

This section discuss a class of random variables that depict circular data, e.g. day time, direction.

In the context of circular data, an angle $\theta$ has infinitely many equivelent representations.
For instance, $1^\circ, 361^\circ, -359^\circ$ are all the same angle.
A straightforward way to align the representation is by

$$
\theta \operatorname{mod} 2 \pi
$$

However it has the disadvantage of discontinuity. A better representation for an angle is 
a point on the unit circle, so that

$$
x_1 = \cos \theta, \quad x_2 = \sin \theta
$$

TODO: elaborate on this

By convention, the distribution for periodic variables satisfies:

$$
\begin{align*}
p(\theta) &\ge 0 \\
\int_0^{2\pi} p(\theta) \,d\theta &= 1 \\
p(\theta + 2 \pi) &= p(\theta) \\
\end{align*}
$$

Consider a bivariate Gaussian distribution defined over $x\_1, x\_2$, with mean $\mu\_1, \mu\_2$ and covariance $\boldsymbol{\Sigma} = \sigma^2 I$

$$
p(x_1, x_2) = \frac{1}{2 \pi \sigma^2} \exp \left\{ - \frac{1}{2\sigma^2} \left[ (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right] \right\}
$$

By substituting variables with 

$$
\begin{align*}
x_1 &= \cos \theta ,& x_2 &= \sin \theta \\
\mu_1 &= r_0 \cos \theta_0 ,& \mu_2 &= r_0 \sin \theta_0 \\
\end{align*}
$$

we try to construct a distribution over a unit circle in polar system.

![](/assets/images/prml/2.18.png)

The exponent can be written

$$
\begin{align*}
&- \frac{1}{2\sigma^2} \left[ (\cos \theta - r_0 \cos \theta_0)^2 + (\sin \theta - r_0 \sin \theta_0)^2 \right] \\
&= - \frac{1}{2\sigma^2} \left[ 1 + r_0^2 - 2 r_0 \cos \theta \cos \theta_0 - 2 r_0 \sin \theta \sin \theta_0 \right] \\
&= \frac{r_0}{\sigma^2} \cos (\theta - \theta_0) + \textrm{const} \\
\end{align*}
$$

where the 'const' summarizes terms independent of $\theta$.

Thereby we obtain the **von Mises** distribution or the **circular normal** distribution

$$
p(\theta | \theta_0, m) = \frac{1}{2 \pi I_0(m)} \exp \left\{ m \cos (\theta - \theta_0) \right\}
$$

Here $\theta\_0$ corresponds to the mean, $m$ is called the **concentration parameter** and is analogous to the precision. 

$I\_0(m)$ is the zeroth-order **modified Bessel function** of the first kind defined by

$$
I_0(m) = \frac{1}{2 \pi} \int_0^{2 \pi} \exp \left\{ m \cos \theta \right\} \,d\theta
$$

**Excercise 2.52**

For large $m$, the distribution approximates the Gaussian. This can be seen by using Taylor expansion

$$
\begin{align*}
\exp \left\{ m \cos (\theta-\theta_0) \right\}
&\approx \exp \left\{ m \left[ 1 - \frac{(\theta-\theta_0)^2}{2} \right]  \right\} \\
&= e^m \exp \left\{ - \frac{m (\theta - \theta_0)^2}{2} \right\}
\end{align*}
$$

In small neighborhood of $\theta\_0$ it approximates well.

Now consider the maximum likelihood for von Mises distribution. The log likelihood function is given by

$$
p(D|\theta_0, m) = -N \ln (2\pi) - N \ln I_0(m) + m\sum_{n=1}^N \cos(\theta_n - \theta_0)
$$

Setting derivative to 0 with repect to $\theta\_0$, we have

$$
\sum_{n=1}^N \sin (\theta_n - \theta_0) = 0
$$

The estimator for $\theta\_0$ is given by

$$
\theta_0^{\mathrm{ML}} = \arctan \left( \frac{\sum_n \sin \theta_n}{\sum_n \cos \theta_n} \right)
$$

Setting derivative to 0 with respect to $m$,  we have

$$
\begin{align*}
A(m_{\mathrm{ML}})
&= \frac{1}{N} \sum_{n=1}^N \cos(\theta_n - \theta_0^{\mathrm{ML}} ) \\
&= \left( \frac{1}{N} \sum_n \cos \theta_n \right) \cos \theta_0^{\mathrm{ML}} + \left( \frac{1}{N} \sum_n \sin \theta_n \right) \sin \theta_0^{\mathrm{ML}}
\end{align*}
$$

in which

$$
\begin{align*}
I_1(m) &= I_0^\prime (m) \\
A(m) &= \frac{I_1(m)}{I_0(m)} \\
\end{align*}
$$

and $m$ can be numerically solved from $A(m)$.

One limitation of von Mises distribution is that it is unimodal. This can be addressed by introducing mixture of von Mises distributions. 
In fact, any distribution over the real axis can be wrapped around a unit circle.

### Mixture of Gaussians

A mixture distribution is formed by linear combination of multiple basic distributions.

A superposition of $K$ Gaussian distributions, which takes the form 

$$
p(\mathbf{x}) = \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$

is called a **mixture of Gaussians**. Each $\mathcal{N} (\mathbf{x}\|\boldsymbol{\mu}\_k, \boldsymbol{\Sigma}\_k)$ is called a **component** of the mixture.

$\pi\_k$ are **mixing coefficients** that satisfy $0 \le \pi\_k \le 1$ and $\sum\_{k=1}^K \pi\_k = 1$.
It can be viewed as a prior distribution $p(k)$ of the k-th component.
Defining $N(\mathbf{x}\|\boldsymbol{\mu}\_k, \boldsymbol{\Sigma}\_k) = p(\mathbf{x}\|k)$, we see
$p(\mathbf{x})$ is the marginal distribution of $p(\mathbf{x}, k)$

Using Bayes' theorem, the posterior $p(k\|\mathbf{x})$, which is also known as the **responsibilities**, is given by

$$
\begin{align*}
\gamma_k(\mathbf{x}) &\equiv p(k|\mathbf{x}) \\
&= \frac{p(k) p(\mathbf{x}|k)}{\sum_l p(l) p(\mathbf{x}|l)} \\
&= \frac{\pi_k \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
{\sum_l \pi_l \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_l, \boldsymbol{\Sigma}_l)} \\
\end{align*}
$$

The log likelihood function for mixture of Gaussians is given by

$$
\ln p(\mathbf{X}|\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma})
= \sum_{n=1}^N \ln \left\{ \sum_{k=1}^K \pi_k \mathcal{N} (\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right\}
$$

Due to the summation inside logarithm, the maximum likelihood no longer has an analytical solution.
Instead, the gradient-based optmization algorithm or the EM algorithm is often applied.
