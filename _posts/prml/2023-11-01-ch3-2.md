---
layout: posts
title: 'PRML Ch2 - Linear Models for Regression - Part II'
---
- [3.4. Bayesian model comparison](#34-bayesian-model-comparison)
- [3.5. Evidence appximation](#35-evidence-appximation)
  - [3.5.1. Evaluation of evidence function](#351-evaluation-of-evidence-function)
  - [3.5.2. Maximizing the evidence function](#352-maximizing-the-evidence-function)
  - [3.5.3. Effective number of parameters](#353-effective-number-of-parameters)
- [3.6. Limitations of fixed basis functions](#36-limitations-of-fixed-basis-functions)

## 3.4. Bayesian model comparison

Consider a set of models $M\_i, i = 1, \dots, L$, where a model represents a distribution.

Given a data set $D$, we can compare the model with the posterior

$$
p(M_i|D) \propto p(M_i) p(D|M_i)
$$

The prior $p(M\_i)$ indicates our preference for models.
The **model evidence** $p(D\|M\_i)$, which is also known as the **marginal likelihood**, indicates preference of the data for models.
The 'marginal' comes from marginalization of parameters in the likelihood function of the model.

The ratio $p(D\|M\_i) / p(D\|M\_j)$ is called a **Bayes factor**.

Once the model posterior is obtained, the predictive distribution is then a mixture distribution given by

$$
p (t|\mathbf{x}, D) = \sum_{i=1}^L p(t | \mathbf{x}, M_i, D) p(M_i | D)
$$

in which $p(t\|\mathbf{x}, M\_i, D)$ is the predictive distribution of $M\_i$.

When we use the most probable model alone to make decision, the problem is called the **model selection**.

For a model governed by parameter $\mathbf{w}$, the model evidence is given by

$$
p(D|M_i) = \int p(D|\mathbf{w}, M_i) p(\mathbf{w}|M_i) \,d\mathbf{w}
$$

To obtain some insight into model evidence, consider a model with a single parameter $w$.

We first assume that the posterior of $w$ is sharply peaked around $w\_{\textrm{MAP}}$ with width $\Delta w\_{\text{post}}$, which is often the case when we have large number of data.
We also assume that the prior is flat with width $\Delta w\_{\text{prior}}$ so that $p(w) = 1/\Delta w\_{\text{prior}}$.

![](/assets/images/prml/fig-3.12.png)

Then the model evidence can be approximated by

$$
\begin{align*}
p(D) &= \int p(D|w) p(w) \,dw \\
&\simeq p(D|w_{\text{MAP}}) \frac{\Delta w_{\text{post}}}{\Delta w_{\text{prior}}} \\
\end{align*}
$$

The conditioning on the model $M\_i$ is omitted here. If we further assume that there are $M$ such independent parameters with equal $\Delta w\_{\text{post}} / \Delta w\_{\text{prior}}$,
we have

$$
\begin{align*}
p(D) \simeq p(D|\mathbf{w}_{\text{MAP}}) \left(\frac{\Delta w_{\text{post}}}{\Delta w_{\text{prior}}} \right)^M
\end{align*}
$$

Taking logarithm gives

$$
\ln p(D) \simeq \ln p(D|\mathbf{w}_{\text{MAP}}) + M \ln \left( \frac{\Delta w_{\text{post}}}{\Delta w_{\text{prior}}} \right) 
$$

The first term represents the fitness to the data, which generally increase during the learning process.

The second term panelizes such fitness. Since $\Delta w\_{\text{post}} < \Delta w\_{\text{prior}}$, it is negative.
With larger $M$ or smaller $\Delta w\_{\text{post}}$, the penelty becomes more severe.

Thereby the model evidence framework tries to find an optimal point of the trade-off.

## 3.5. Evidence appximation

In fully Bayesian inference, we would introduce prior distributions for hyperparameters 
and marginalizing over the parameters and hyperparameters to obtain the predictive distribution.

In the context of linear regression, the integral takes the form

$$
p(t|\mathbf{t}) = \iiint p(t|\mathbf{w}, \beta) p(\mathbf{w}|\mathbf{t}, \alpha, \beta) p(\alpha, \beta | \mathbf{t}) \,d\mathbf{w} d\alpha d\beta
$$

where the conditioning on input variable $\mathbf{x}$ has been omitted to keep notations uncluttered.
Distributions in the integrand are defined respectively by

$$
\begin{align*}
p(t|\mathbf{w}, \beta) &= \mathcal{N} (t|\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}), \beta^{-1}) \\
p(\mathbf{w}|\mathbf{t}, \alpha, \beta) &= \mathcal{N} (\mathbf{w}|\mathbf{m}_N, \mathbf{S}_N) \\
\end{align*}
$$

However, integrating out all parameters and hyperparameters is analytically intractable.
A workaround is to assume that the hyperprior $p(\alpha, \beta\|\mathbf{t})$ sharply peaks around some $\widehat{\alpha}, \widehat{\beta}$,
then the predictive distribution can be approximated by

$$
p(t|\mathbf{t}) \simeq p(t|\mathbf{t}, \widehat{\alpha}, \widehat{\beta}) = \int p(t|\mathbf{w}, \widehat{\beta}) p(\mathbf{w}|\mathbf{t}, \widehat{\alpha}, \widehat{\beta}) \,d\mathbf{w}
$$

Then our goal is to determine proper values of $\widehat{\alpha}$ and $\widehat{\beta}$.
From Bayes' theorem, we know

$$
p(\alpha, \beta|\mathbf{t}) \propto p(\mathbf{t}|\alpha, \beta) p(\alpha, \beta)
$$

Assuming the prior $p(\alpha, \beta)$ is noninformative, then the mode of $p(\alpha, \beta\|\mathbf{t})$ will correspond to that of $p(\mathbf{t}\|\alpha, \beta)$.
The values of $\widehat{\alpha}, \widehat{\beta}$ can be found by maximizing the marginal likelihood function $p(\mathbf{t}\|\alpha, \beta)$, 
which is obtained by integrating out $\mathbf{w}$.

This framework is called the **evidence approximation** in machine learning literature, or **empirical Bayes**, **type 2 maximum likelihood**, or **generalized maximum likelihood** in statistics.

The maximization of evidence can be done either by taking derivatives, which is dicussed shortly, or by the EM algorithm.

### 3.5.1. Evaluation of evidence function

The marginal likelihood function $p(\mathbf{t}\|\alpha, \beta)$ is obtained from

$$
p(\mathbf{t}|\alpha, \beta) = \int p(\mathbf{t}|\mathbf{w}, \beta) p(\mathbf{w}|\alpha) \,d\mathbf{w}
$$

where we take

$$
\begin{align*}
p(\mathbf{t}|\mathbf{w}, \beta) &= \prod_{n=1}^N \mathcal{N} (t_n|\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}_n), \beta^{-1}) \\
p(\mathbf{w}|\alpha) &= \mathcal{N} (\mathbf{w}|\mathbf{0}, \alpha^{-1}I)
\end{align*}
$$

Rearranging terms, the evidence function is written

$$
p(\mathbf{t}|\alpha, \beta) = \left( \frac{\beta}{2 \pi} \right)^{N/2} 
\left( \frac{\alpha}{2 \pi} \right)^{M/2}
\int \exp \left\{ - E(\mathbf{w}) \right\} \,d\mathbf{w}
$$

in which

$$
\begin{align*}
E(\mathbf{w}) &= \beta E_D(\mathbf{w}) + \alpha E_W(\mathbf{w}) \\
&= \frac{\beta}{2} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{w} \right\|^2 +
\frac{\alpha}{2} \mathbf{w}^{\mathsf{T}} \mathbf{w}
\end{align*}
$$

Expanding and rearranging terms

$$
\begin{align*}
E(\mathbf{w}) &= \mathbf{w}^{\mathsf{T}} \left( \frac{\beta}{2} \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} + \frac{\alpha}{2} I \right) \mathbf{w}
- \mathbf{w}^{\mathsf{T}} \beta \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}
+ \frac{\beta}{2} \mathbf{t}^{\mathsf{T}} \mathbf{t} \\
\end{align*}
$$

Define

$$
\begin{align*}
\mathbf{A} &= \alpha I +  \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \\
\mathbf{m}_N &= \beta \mathbf{A}^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}
\end{align*}
$$

Completing the square over $\mathbf{w}$, we obtain

$$
\begin{align*}
E(\mathbf{w}) &= \frac{1}{2} (\mathbf{w} - \mathbf{m}_N)^{\mathsf{T}} \mathbf{A} (\mathbf{w} - \mathbf{m}_N)
- \frac{1}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{A} \mathbf{m}_N
+ \frac{\beta}{2} \mathbf{t}^{\mathsf{T}} \mathbf{t} \\
\end{align*}
$$

Focusing on the last two terms on the r.h.s

$$
\begin{align*}
- \frac{1}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{A} \mathbf{m}_N
+ \frac{\beta}{2} \mathbf{t}^{\mathsf{T}} \mathbf{t} 
&= - \frac{1}{2} \mathbf{m}_N^{\mathsf{T}} (\alpha I + \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) \mathbf{m}_N
+ \frac{\beta}{2} \mathbf{t}^{\mathsf{T}} \mathbf{t}  \\
&= \frac{\beta}{2} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \right\|^2
+ \beta \mathbf{m}_N^{\mathsf{T}} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}
- \beta \mathbf{m}_N^{\mathsf{T}} \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{m}_N
- \frac{\alpha}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N \\
&= \frac{\beta}{2} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \right\|^2
+ \frac{\alpha}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N
\end{align*}
$$

where we used $\beta \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} = \mathbf{A} \mathbf{m}\_N = (\alpha I +  \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) \mathbf{m}\_N$.

Back-substituting, we have

$$
E(\mathbf{w}) = E(\mathbf{m}_N)
+ \frac{1}{2} (\mathbf{w} - \mathbf{m}_N)^{\mathsf{T}} \mathbf{A} (\mathbf{w} - \mathbf{m}_N)
$$

where

$$
\begin{align*}
E(\mathbf{m}_N) 
&= \beta E_D(\mathbf{m}_N) + \alpha E_W(\mathbf{m}_N) \\
&= \frac{\beta}{2} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \right\|^2
+ \frac{\alpha}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N \tag{3.82} \\
\end{align*}
$$

Now the integral over $\mathbf{w}$

$$
\begin{align*}
&\int \exp \left\{ - E(\mathbf{w}) \right\} \,d\mathbf{w} \\
&\quad = \exp \left\{ - E(\mathbf{m}_N) \right\} 
\int \exp \left\{ - \frac{1}{2} (\mathbf{w} - \mathbf{m}_N)^{\mathsf{T}} \mathbf{A} (\mathbf{w} - \mathbf{m}_N) \right\} \,d\mathbf{w} \\
&\quad = \exp \left\{ - E(\mathbf{m}_N) \right\} 
(2 \pi)^{M/2} |\mathbf{A}|^{-1/2}
\end{align*}
$$

The log marginal likelihood or the evidence function is then given by

$$
\ln p(\mathbf{t}|\alpha, \beta) 
= \frac{M}{2} \ln \alpha + \frac{N}{2} \ln \beta
- E(\mathbf{m}_N) - \frac{1}{2} \ln |\mathbf{A}| - \frac{N}{2} \ln (2 \pi)
$$

**Excercise 3.16**

Another approach to evaluate the integral is by using the convolution formula in seciton 2.3.3.

Making following change of variables

$$
\begin{align*}
\mathbf{y} &\rightarrow \mathbf{t}, & \mathbf{x} &\rightarrow \mathbf{w} , & \mathbf{A} &\rightarrow \boldsymbol{\Phi},& \mathbf{b} &\rightarrow \mathbf{0} \\
\mathbf{L}^{-1} &\rightarrow \beta^{-1} I, & \boldsymbol{\mu} &\rightarrow \mathbf{0}, & \boldsymbol{\Lambda}^{-1} &\rightarrow \alpha^{-1} I
\end{align*}
$$

we obtain the marginal likelihood function

$$
p(\mathbf{t} | \alpha, \beta) = \mathcal{N} (\mathbf{t} | \mathbf{0}, \beta^{-1} I + \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}})
$$

Taking logarithm of the marginal likelihood we obtain

$$
\ln p(\mathbf{t} | \alpha, \beta) = - \frac{N}{2} \ln 2\pi - \frac{1}{2} \ln \left| \beta^{-1} I + \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} \right|
- \frac{1}{2} \mathbf{t}^{\mathsf{T}} (\beta^{-1} I + \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}})^{-1}  \mathbf{t}
$$

First we show that

$$
\begin{align*}
\left| \beta^{-1} I + \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} \right|
&= \beta^{-N} \left| I + \beta \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} \right| \\
&= \beta^{-N} \left| I + \beta \alpha^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \right| \\
&= \beta^{-N} \alpha^{-M} \left| \alpha I + \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \right| \\
&= \beta^{-N} \alpha^{-M} |\mathbf{A}| \\
\end{align*}
$$

where we used the identity

$$
\left| I_n + AB \right| = \left| I_m + BA \right| 
$$

By Woodbury identity

$$
(\beta^{-1} I + \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}})^{-1}
= \beta I - \beta^2 \boldsymbol{\Phi} \mathbf{A}^{-1} \boldsymbol{\Phi}^{\mathsf{T}}
$$

Bakc substituting, we have

$$
\begin{align*}
- \frac{1}{2} \mathbf{t}^{\mathsf{T}} (\beta^{-1} I + \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}})^{-1}  \mathbf{t}
&= \beta \mathbf{t}^{\mathsf{T}} \mathbf{t} - \beta^2 \mathbf{t}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{A}^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
&= \beta \mathbf{t}^{\mathsf{T}} \mathbf{t} - \mathbf{m}_N^{\mathsf{T}} \mathbf{A} \mathbf{m}_N
\end{align*}
$$

The remaining steps are the same as the completing square method.

### 3.5.2. Maximizing the evidence function

First consider the maximization of $p(\mathbf{t}\|\alpha, \beta)$ with respect to $\alpha$.

Suppose $\{\lambda\_i\}$ are eigenvalues of $\beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}$ such that

$$
(\beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) \mathbf{u}_i = \lambda_i \mathbf{u}_i
\tag{3.87}
$$

By

$$
\mathbf{A} \mathbf{u}_i
= (\alpha I + \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) \mathbf{u}_i 
= (\alpha + \lambda_i) \mathbf{u}_i
$$

we see that $\mathbf{A}$ has eigenvalues $\alpha+\lambda\_i$.
Hence the determinant of $\mathbf{A}$ can be expressed as

$$
|\mathbf{A}| = \prod_{i=1}^M (\alpha + \lambda_i)
$$

Taking derivative of $\ln \|\mathbf{A}\|$ with respect to $\alpha$, we have

$$
\begin{align*}
\frac{\partial}{\partial \alpha} \ln |\mathbf{A}| 
= \frac{\partial}{\partial \alpha} \sum_i \ln (\alpha + \lambda_i)
= \sum_i \frac{1}{\alpha + \lambda_i} 
\end{align*}
$$

**Excercise 3.20**

Taking derivative of (3.82) with respect to $\alpha$, the first term gives

$$
\begin{align*}
\partial \left\{ \frac{\beta}{2} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \right\|^2 \right\} 
&= \beta (\boldsymbol{\Phi} \mathbf{m}_N - \mathbf{t})^{\mathsf{T}} \boldsymbol{\Phi} \,\partial \mathbf{m}_N \\
\end{align*}
$$

and the second term gives

$$
\begin{align*}
\partial \left\{ \frac{\alpha}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N \right\}
&= \frac{1}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N \,\partial \alpha
+ \alpha \mathbf{m}_N^{\mathsf{T}} \,\partial \mathbf{m}_N \\
\end{align*}
$$

Combining both terms

$$
\begin{align*}
\partial E(\mathbf{m}_N)
&= \partial \left\{ \frac{\beta}{2} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \right\|^2 \right\}
+ \partial \left\{ \frac{\alpha}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N \right\} \\
&= \frac{1}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N \,\partial \alpha
+ \left[ \beta (\boldsymbol{\Phi} \mathbf{m}_N - \mathbf{t})^{\mathsf{T}} \boldsymbol{\Phi} + \alpha \mathbf{m}_N^{\mathsf{T}} \right] \,\partial \mathbf{m}_N \\
\end{align*}
$$

By

$$
\begin{align*}
\beta (\boldsymbol{\Phi} \mathbf{m}_N - \mathbf{t})^{\mathsf{T}} \boldsymbol{\Phi} + \alpha \mathbf{m}_N^{\mathsf{T}}
&= \mathbf{m}_N^{\mathsf{T}} (\alpha I + \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) - \beta \mathbf{t}^{\mathsf{T}} \boldsymbol{\Phi} \\
&= \mathbf{m}_N^{\mathsf{T}} \mathbf{A} - \mathbf{m}_N^{\mathsf{T}} \mathbf{A}
= \mathbf{0}
\end{align*}
$$

we have

$$
\frac{\partial}{\partial \alpha} E(\mathbf{m}_N) = \frac{1}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N
$$

Setting derivative of the log evidence function to zero, we obtain

$$
\frac{\partial}{\partial \alpha} \ln p(\mathbf{t} | \alpha, \beta)
= \frac{M}{2 \alpha} - \frac{1}{2} \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N -\frac{1}{2} \sum_i \frac{1}{\alpha + \lambda_i} = 0
$$

Rearranging terms

$$
\alpha \mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N = M - \alpha \sum_i \frac{1}{\alpha + \lambda_i} = \sum_i \frac{\lambda_i}{\alpha + \lambda_i}
\tag{3.90}
$$

Defining the quantity

$$
\gamma = \sum_i \frac{\lambda_i}{\alpha + \lambda_i}
$$

we have

$$
\alpha = \frac{\gamma}{\mathbf{m}_N^{\mathsf{T}} \mathbf{m}_N}
$$

This is an implicit solution for $\alpha$, since both $\gamma$ and $\mathbf{m}\_N$ depend on $\alpha$.

An iterative procedure can be adopted to compute the value of $\alpha$:

1. Select an intial value of $\alpha$
2. Evaluate $\mathbf{m}\_N$ and $\gamma$, obtain a new value for $\alpha$
3. Reapeat step 2 until convergence.

Now consider the maximazation with respect to $\beta$.

By (3.87), we see $\lambda\_i$ is proportional to $\beta$, therefore $\partial \lambda\_i / \partial \beta = \lambda\_i / \beta$ and

$$
\frac{\partial}{\partial \beta} \ln |\mathbf{A}| = \sum_i \frac{1}{\alpha + \lambda_i} \frac{\partial \lambda_i}{\partial \beta} 
= \frac{1}{\beta} \sum_i \frac{\lambda_i}{\alpha + \lambda_i}
= \frac{\gamma}{\beta}
$$

Setting the derivative to zero gives

$$
\frac{\partial}{\partial \beta} \ln p(\mathbf{t} | \alpha, \beta)
= \frac{N}{2 \beta} - \frac{1}{2} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \right\|^2 - \frac{\gamma}{2 \beta} = 0
$$

Rearraging terms

$$
\frac{1}{\beta} = \frac{1}{N - \gamma} \left\| \mathbf{t} - \boldsymbol{\Phi} \mathbf{m}_N \right\|^2
$$

which is also a implicit solution. We use the same approach to calculate $\beta$ as we do for $\alpha$.

Note that, the optimization of these hyperparameters only involves the training data, without the requirement for additional data sets.

### 3.5.3. Effective number of parameters

If we treat the likelihood function $p(\mathbf{t}\|\mathbf{w}, \beta)$ as a function over $\mathbf{w}$, we can obtain its contours by fixing the exponent so that

$$
\beta \| \boldsymbol{\Phi} \mathbf{w} - \mathbf{t} \|^2 = C
$$

Rearranging terms, we obtain the expression of the contour

$$
(\mathbf{w} - \mathbf{w}_{\mathrm{ML}})^{\mathsf{T}} \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} (\mathbf{w} - \mathbf{w}_{\mathrm{ML}}) = \mathrm{const}
$$

in which $\mathrm{const}$ concludes terms independent of $\mathbf{w}$, and $\mathbf{w}\_{\mathrm{ML}}$ is the maximum likelihood solution given by

$$
\mathbf{w}_{\mathrm{ML}} = (\boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}
$$

Without loss of generality, we may assume that $\beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}$ is diagonal.
Otherwise we can perform diagonalization and the following analysis similarly holds.

As a result, the elliptical contours are now axis-aligned to the coordinate.
In the case of 2 parameters, it is illustrated as

![](/assets/images/prml/3.15.jpeg)

The red curve is a contour of the likelihood function $p(\mathbf{t}\|\mathbf{w}, \beta)$ and the green one corresponds to the prior $p(\mathbf{w}\| \alpha)$.

Using

$$
\begin{align*}
\beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} &= \mathrm{diag} \left( \lambda_i \right) \\
\mathbf{A}^{-1} &= \mathrm{diag} \left( \frac{1}{\alpha + \lambda_i} \right) \\
\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} &= \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{w}_{\mathrm{ML}} \\
\end{align*}
$$

we obtain

$$
\begin{align*}
\mathbf{m}_N &= \beta \mathbf{A}^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}
= \mathrm{diag} (\frac{\lambda_i}{\alpha + \lambda_i}) \mathbf{w}_{\mathrm{ML}}
\end{align*}
$$

Note that the maximum posterior solution $\mathbf{w}\_{\mathrm{MAP}}$ corresponds to $\mathbf{m}\_N$, thus we have

$$
\mathbf{w}_{\mathrm{MAP}} = \mathrm{diag} (\frac{\lambda_i}{\alpha + \lambda_i}) \mathbf{w}_{\mathrm{ML}} \\
$$

When $\lambda\_i \gg \alpha$, ${\lambda\_i}/(\alpha + \lambda\_i)$ is close to 1 and $w\_i^{\textrm{MAP}}$ will be close to $w\_i^{\textrm{ML}}$.
Such parameters are called **well determined** since they are finely tuned to the data.

When $\lambda\_i \ll \alpha$, ${\lambda\_i}/(\alpha + \lambda\_i)$ converges to zero,
which drives $w\_i^{\textrm{MAP}}$ to zero.

Since $\beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}$ is postive semi-definite,
we have $0 \le \lambda\_i / (\alpha + \lambda\_i) \le 1$ and consequently $0 \le \gamma \le M$.
Hence $\gamma$ can be measurement of the number of well determined parameters.

\
As $N$ increases, the magnitude of elements in $\boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}$ tends to grow since it is the sum of second order monents of data points,
and thus the eigenvalues also increase.
In the limit $N \gg M$, $\gamma$ converges to $M$, and the estimation equations for $\alpha$ and $\beta$ can be simply given

> the increase of sum does not depend on basis. How to prove?

$$
\begin{align*}
\alpha &= \frac{M}{2E_W(\mathbf{m}_N)} \\
\beta &= \frac{N}{2E_D(\mathbf{m}_N)} \\
\end{align*}
$$

which are easier to compute since calculation of eigenvalues is not required.

## 3.6. Limitations of fixed basis functions

The basis functions are fixed before the training data is observed. The number of basis functions will grow rapidly as the dimensionality of input increases.

Two properties of real data will help to find the remediation:

- Input data usually lie in a manifold with smaller dimensionality
- Target variables depends only on a subset of dimesions