# Sequential Data

This chapter drops the assumption that observations are i.i.d and discuss the modeling of sequential data.

There're two kinds of sequntial distributions:

- Stationary model where the data evolves with time but the distribution remains unchanged.
- Nonstationary model where the distribution itself evolves with time.

## Markov models

An intuitive thought is that the recent ovservations are more informative than the historical ones. 
The **Markov models** assume that the future predictions are depedent only on the most recent observations. It can be written in the form

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N) = \prod_{n=1}^N p(\mathbf{x}_n | \mathbf{x}_1, \dots, \mathbf{x}_{n-1})
$$

which corresponds to fully connected graphical model of $N$ nodes.

If we assume that the conditional distribution depends only on the most recent observation,
we obtain the **first-order Markov chain**, which can be written as

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N) = p(\mathbf{x}_1) \prod_{n=2}^N p(\mathbf{x}_n | \mathbf{x}_{n-1})
$$

The corresponding graph is shown by

$$
\mathbf{x}_1 \rightarrow \mathbf{x}_2 \rightarrow \cdots \rightarrow \mathbf{x}_N
$$

By d-separation, we have

$$
p(\mathbf{x}_n | \mathbf{x}_1, \dots, \mathbf{x}_{n-1}) = p(\mathbf{x}_n | \mathbf{x}_{n-1})
$$

If the model $p(\mathbf{x}\_n \| \mathbf{x}\_{n-1})$ is defined to be stationary, it is called a homogenuous Markov chain.

A second order Markov chain takes the form

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N) = p(\mathbf{x}_1) p(\mathbf{x}_2 | \mathbf{x}_1) \prod_{n=3}^N p(\mathbf{x}_n | \mathbf{x}_{n-1}, \mathbf{x}_{n-2})
$$

which corresponds to the graph

![](/assets/images/prml/fig-13.4.png)

However high order Markkov chain may suffer from the large number of parameters.
Consider a $M$-th order Markov chain, in which Each varaible is discrete and has $K$ possible states.
Then by the discussion of Ch8, to model $p(\mathbf{x}\_n \| \mathbf{x}\_{n-M}, \dots, \mathbf{x}\_{n-1})$, we need $K^M (K-1)$ parameters.

For continous variables, we use linear-Gaussian condition distribution, which gives the **autoregressive** (a.k.a AR) model.

Some parametric model, such as neural network, can be used to model $p(\mathbf{x}\_n \| \mathbf{x}\_{n-M}, \dots, \mathbf{x}\_{n-1})$ and reduce the number of prameters.

We can also build a model that is not limited by the Markov assumption yet has limited number of parameters. 
For each $\mathbf{x}\_n$, we introduce a latent variable $\mathbf{z}\_n$ which can be discrete or continuous and have different dimensionality as $\mathbf{x}$.
The graphical model is shown by

![](/assets/images/prml/fig-13.5.png)

The joint distribution is written

$$
p(\mathbf{x}_1, \dots, \mathbf{x}_N, \mathbf{z}_1, \dots, \mathbf{z}_N)
= p(\mathbf{z}_1) \left[ \prod_{n=2}^N p(\mathbf{z}_n|\mathbf{z}_{n-1}) \right]
\prod_{n=1}^N p(\mathbf{x}_n | \mathbf{z}_n)
$$

This model is known as the **state space model**. It has the property that

$$
\mathbf{z}_{n+1} \perp\!\!\!\perp \mathbf{z}_{n-1} \mid \mathbf{z}_n
$$

Moreover, the path between any $\mathbf{x}\_n$ and $\mathbf{x}\_m$ will not be blocked by latent variables,
so the predictive distribution $p(\mathbf{x}\_{n+1}\|\mathbf{x}\_1, \dots, \mathbf{x}\_n)$ will depend on all previous observations.

If the latent variables are discrete, we obtain the **hidden Markov model**.
If they are continuous, we obtain the **linear dynamic system**.

## Hidden Markov models

Widely used in NPL, on-line handwriting recognition.

Let $\mathbf{z}\_n$ be a discrete random variable using 1-of-K coding scheme.
To model the distribution $p(\mathbf{z}\_n\|\mathbf{z}\_{n-1})$, we define a **transition matrix** $\mathbf{A}$ with each element given by

$$
A_{jk} = p(z_{nk} = 1 | z_{n-1, j} = 1)
$$

which is called the **transition probability**. It satisfies $0 \le A\_{jk} \le 1$ and $\sum\_{k} A\_{jk} = 1$.
The number of independent parameter in $\mathbf{A}$ is $K(K-1)$.

Then the conditional distribution $p(\mathbf{z}\_n\|\mathbf{z}\_{n-1})$ takes the form

$$
p(\mathbf{z}_n|\mathbf{z}_{n-1}, \mathbf{A})
= \prod_{k=1}^K \prod_{j=1}^K A_{jk}^{z_{n-1, j} z_{nk}}
$$

For $\mathbf{z}\_1$, we define $\pi\_k \equiv p(z\_{1k} = 1)$ and then

$$
p(\mathbf{z}_1) = \prod_{k=1}^K \pi_k^{z_{1k}}
$$

where $\sum\_k \pi\_k = 1$.

The transition matrix can be shown as a state transition diagram. In the case of 3 states, we have

![](/assets/images/prml/fig-13.6.png)

Another type of diagram, namely the **lattice** or **trellis** diagram, is obtained by expanding the transitions along time, which gives

![](/assets/images/prml/fig-13.7.png)

The distribution $p(\mathbf{x}\_n\|\mathbf{z}\_n, \boldsymbol{\phi})$, where $\boldsymbol{\phi}$ denotes the parameters, is called the **emission probability**.
It can be defined as components of a mixture distribution such that

$$
p(\mathbf{x}_n|\mathbf{z}_n, \boldsymbol{\phi})
= \prod_{k=1}^K p(\mathbf{x}_n | \boldsymbol{\phi}_k)^{z_{nk}}
$$

where $\boldsymbol{\phi}\_k$ is the parameters corresponding to $z\_{nk} = 1$.

If we assume that $p(\mathbf{z}\_n\|\mathbf{z}\_{n-1})$ shares a transition matrix for $n=2,\dots, N$ and $p(\mathbf{x}\_n\|\mathbf{z}\_n)$ shares the same parameter $\boldsymbol{\phi}$ for $n=1,\dots, N$,
the joint distribution is then given by

$$
p(\mathbf{Z}, \mathbf{X}|\boldsymbol{\theta}) 
= p(\mathbf{z}_1|\boldsymbol{\pi}) \left[ \sum_{n=2}^N p(\mathbf{z}_n|\mathbf{z}_{n-1}, \mathbf{A}) \right]
\sum_{m=1}^N p(\mathbf{x}_m|\mathbf{z}_m, \boldsymbol{\phi})
$$

where $\mathbf{X} = \{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$, $\mathbf{Z} = \{ \mathbf{z}\_1, \dots, \mathbf{z}\_N \}$ and $\boldsymbol{\theta} = \{ \boldsymbol{\pi}, \mathbf{A}, \boldsymbol{\phi} \}$.

The emission probability can also be modeled as linear regression or neural network.

A restricted form of HMM, called the **left-to-right** HMM, is obtained by setting $A\_{jk} = 0$ for $k < j$, so that the state transitions are constrained to non-decreasing.
$p(\mathbf{z}\_1)$ is typically defined as $p(z\_{11} = 1) = 1$ and $p(\mathbf{z}\_{1j}=1) = 0$ for $j \ne 1$, so that the sequence always starts from state 1.

A 3-state left-to-right HMM:

![](/assets/images/prml/fig-13.9.png)

Further constraint is to set $A\_{jk} = 0$ for $k > j+\Delta$. This disallows drastic changes in the state transition.

![](/assets/images/prml/fig-13.10.png)

Many applications such as speech recognition or on-line handwritting recognition make use of the left-to-right HMM.

### Maximum likelihood for the HMM

