---
layout: posts
title: 'Neural networks - Part II'
---

- [Regularization in neural networks](#regularization-in-neural-networks)
  - [Consistent Gaussian priors](#consistent-gaussian-priors)
  - [Early stopping](#early-stopping)
  - [Invariances](#invariances)
  - [Tangent propagation](#tangent-propagation)
  - [Training with transformed data](#training-with-transformed-data)
  - [Convelutional networkds](#convelutional-networkds)
  - [Soft weight sharing](#soft-weight-sharing)
- [Mixture density networks](#mixture-density-networks)
- [Bayesian neural networks](#bayesian-neural-networks)
  - [Posterior parameter distribution](#posterior-parameter-distribution)
  - [Hyperparameter optimization](#hyperparameter-optimization)
  - [Bayesian neural networks for classfication](#bayesian-neural-networks-for-classfication)

## Regularization in neural networks

To avoid over-fitting, it is intuitive to introduce a regularization term:

$$
\widetilde{E}(\mathbf{w}) = E(\mathbf{w}) + \frac{\lambda}{2} \mathbf{w}^{\mathsf{T}} \mathbf{w}
\tag{5.112}
$$

This qudratic regularizer is also called the weight decay.

### Consistent Gaussian priors

Consider a two-layer network with identity output function, where the hidden units takes the form

$$
z_j = h \left( \sum_i w_{ji} x_i + w_{j0} \right)
$$

and the output units

$$
y_k = \sum_j w_{kj} z_j + w_{k0}
$$

Suppose a linear transformation is conducted on the input so that

$$
x_i \rightarrow \widetilde{x}_i  = a x_i + b
$$

which gives the hidden units

$$
z_j = h \left( \sum_i a w_{ji} x_i + \sum_i b w_{ji} + w_{j0} \right)
$$

By performing the following linear transformations on parameters, the output of network remains unchanged

$$
\begin{align*}
w_{ji} &\rightarrow \widetilde{w}_{ji} = \frac{1}{a} w_{ji} \\
w_{j0} &\rightarrow \widetilde{w}_{j0} = w_{j0} - \frac{b}{a} \sum_i w_{ji} 
\end{align*}
$$

Similarly, the linear transformation on the output

$$
y_k \rightarrow \widetilde{y}_k = c y_k + d
$$

can be achived by a linear transformation on parameters of the second layer such that

$$
\begin{align*}
w_{kj} &\rightarrow \widetilde{w}_{kj} = c w_{kj} \\ 
w_{k0} &\rightarrow \widetilde{w}_{k0} = c w_{k_0} + d \\
\end{align*}
$$

When we train a network based on linearly transformed inputs or targets,
consistency requires that resulting networks should only differ by the linear transformtion of weights and biases.
However, the quadratic regularizer in (5.112) will break such consistency.

A regularizer that is invariant under scaling of weights takes the form

$$
\frac{\lambda_1}{2}\sum_{w \in \mathcal{W}_1} w^2
+ \frac{\lambda_2}{2}\sum_{w \in \mathcal{W}_2} w^2
\tag{5.121}
$$

where $\mathcal{W}\_1$ is the set of weights in the first layer and $\mathcal{W}\_2$ are weights in the second layer, with biases excluded. The invariance can be achieved by transformations

$$
\begin{align*}
\lambda_1 &\rightarrow \widetilde{\lambda}_1 = a^{1/2} \lambda_1 \\
\lambda_2 &\rightarrow \widetilde{\lambda}_2 = c^{-1/2} \lambda_2 \\
\end{align*}
$$

The corresponding prior is given by

$$
p(\mathbf{w} | \alpha_1, \alpha_2) \propto \exp \left( - \frac{\alpha_1}{2}\sum_{w \in \mathcal{W}_1} w^2 - \frac{\alpha_2}{2}\sum_{w \in \mathcal{W}_2} w^2 \right)
$$

which is impoper since there's no constrains on the biases ($\int dw\_{j0} = \infty$). 

Introducing a seperate prior on the biases can achive the properness, but will braek the shift invariance.

More general prior can be given by dividing weights into arbitrary groups so that

$$
p(\mathbf{w}) \propto \exp \left( - \frac{1}{2} \sum_k \alpha_k \|\mathbf{w}\|_k^2 \right)
$$

in which

$$
\| \mathbf{w} \|^2 = \sum_{w \in \mathcal{W}_k} w^2
$$

### Early stopping

An alternative way to control the model complexity is the early stopping.
For optimization algorithms that have non-increasing error over the iteration index, traning
can be stopped when the smallest error is obtained on the validation data set.

**Excercise 5.25**

Consider a error function given by

$$
E(\mathbf{w}) = E(\mathbf{w}^\ast) + \frac{1}{2} (\mathbf{w} - \mathbf{w}^\ast)^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast)
$$

where $\mathbf{w}^\ast$ is the minima, $\mathbf{H}$ is a postive definite constant Hessian matrix.
It can be seen as an approximated of the error function around $\mathbf{w}^\ast$.

Takging derivative with respect to $\mathbf{w}$, we obtain

$$
\nabla E = \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast)
$$

Suppose the intial vector $\mathbf{w}^{(0)}$ is the origin, and the optimation algorithm is given by

$$
\mathbf{w}^{(\tau)} = \mathbf{w}^{(\tau-1)} - \rho \nabla E
$$

Substituting $\nabla E$

$$
\begin{align*}
\mathbf{w}^{(\tau)} - \mathbf{w}^\ast 
&= (I - \rho \mathbf{H}) (\mathbf{w}^{(\tau-1)} - \mathbf{w}^\ast) \\
&= (I - \rho \mathbf{H})^{\tau} (\mathbf{w}^{(0)} - \mathbf{w}^\ast)\\
&= - (I - \rho \mathbf{H})^{\tau} \mathbf{w}^\ast \\
\end{align*}
$$

Suppose $\{ \mathbf{u}\_j \}$ are orthonormal eigenvectors of $\mathbf{H}$ with corresponding eigenvalues $\{ \eta\_j \}$.
We can write

$$
(I - \rho \mathbf{H})^{\tau} \mathbf{u}_j 
= (I - \rho \mathbf{H})^{\tau - 1} (1 - \rho \eta_j) \mathbf{u}_j 
= (1 - \rho \eta_j)^{\tau} \mathbf{u}_j 
$$

Therefore

$$
w_j^{(\tau)} = \left[ 1 - (1 - \rho \eta_j)^\tau \right] w_j^\ast
$$

where we defined $w\_j = \mathbf{u}\_j^{\mathsf{T}} \mathbf{w}$.

Provided $\|1 - \rho \eta\_j\| < 1$, we have

$$
\lim_{\tau \rightarrow \infty} w_j^{(\tau)} = w_j^\ast
$$

Suppose the training stops after $\tau$ iterations. When $\eta\_j \ll (\rho \tau)^{-1}$, $w\_j^{(\tau)}$ converges to the minima $w\_j^\ast$; when $\eta\_j \gg (\rho \tau)^{-1}$, $w\_j^{(\tau)}$, $w\_j^{(\tau)}$ is close to its initial value 0.

Thereby we see, the quantity $(\rho \tau)^{-1}$ has the effect similar to the regularization coefficient $\lambda$. The regularization can be achived by early stopping.

### Invariances

In many applications, the predictions should remain unchanged, or in other words stay invariant,
under some transformation on the inputs.
E.g. handwritten digits recognition should be robust against small elastic deformation.

If there are sufficiently large number of ovservations that includes the effects of various transformations, the model can learn the invariance from the data.
However, this is not practical for real-life data sets.

There are four general approches to train an adaptive model with invariace:

1. Augment the training set by adding new data transformed from the original data.
2. Use regularization to penalize variances of the output to changes of the input.
3. Build invariance during pre-processing by extracting features that are invariant under specific transformations.
4. Build the invariance into the network iteself.

### Tangent propagation

Tangent propagation is an example of using regularization to build invariance.

Consider a continuous transformation governed by a parameter $\xi$, which transforms D-dimensional vector $\mathbf{x}$ into another vector $\mathbf{s} (\mathbf{x}, \xi)$ such that $\mathbf{s} (\mathbf{x}, 0) = 0$.
For instance, rotation and shifting are both continuous transformations, however reflection is not.

The tagent vector at $\mathbf{x}\_n$ is given by

$$
\boldsymbol{\tau}_n = \frac{\partial \mathbf{s} (\mathbf{x}_n, \xi)}{\partial \xi} \bigg|_{\xi = 0}
$$

Taking the vector $\mathbf{s} (\mathbf{x}, \xi)$ as the network input, the derivative of $y\_k$ with repsect to $\xi$ gives

$$
\frac{\partial y_k }{\partial \xi} 
= \sum_{i=1}^D \frac{\partial y_k}{\partial s_i} \frac{\partial s_i}{\partial \xi}
= \sum_{i=1}^D J_{ki} \tau_{i}
$$

where $J\_{ki}$ represents the element $(k, i)$ in Jacobian matrix, and $\tau\_i$ is the i-th component of the tangent vector at $\mathbf{x}$.

Defining a regularizer

$$
\Omega  = \frac{1}{2} \sum_{n, k} \left( \frac{\partial y_{nk}}{\partial \xi} \right)^2 \bigg|_{\xi = 0}
= \frac{1}{2} \sum_{n,k} \left( \sum_{i=1}^D J_{nki} \tau_{ni}  \right)^2
\tag{5.128}
$$

the regularized error function is then given by

$$
\widetilde{E} = E + \lambda \Omega
$$

where $\lambda$ is the regularization coefficient.
$\Omega$ will be zero if the model is invariant under the transofrmation $\mathbf{s}$.
And $\lambda$ determines the balance between fitting to data and learning the invariance.

Depending on the form of transformation, $\boldsymbol{\tau}\_n$ can be calculated either analytically or by finite difference.

**Exercise 5.26**

A backpropagation algorithm for evaluating derivatives of $\Omega$ is as follows.

To evalueate $\partial \Omega\_n / \partial w\_{rs}$, we first show that

$$
\begin{align*}
\frac{\partial J_{ki}}{\partial w_{rs}} &= \frac{\partial^2 y_k}{\partial s_i \partial w_{rs}} \\
&= \frac{\partial}{\partial s_i} \left( \frac{\partial y_k}{\partial a_r} z_s \right) \\
&= \frac{\partial^2 y_k}{\partial a_r \partial s_i} z_s + \delta_{kr} \frac{\partial z_s}{\partial s_i} \\
&= \phi_{kri} z_s + \delta_{kr} \frac{\partial z_s}{\partial s_i}
\end{align*}
$$

where we defined

$$
\delta_{kr} = \frac{\partial y_k}{\partial a_r},
\quad \phi_{kri} = \frac{\partial \delta_{kr}}{\partial s_i}
$$

$\delta\_{kr}$ can be evaluated by using backpropagation

$$
\delta_{kr} = h^\prime (a_r) \sum_l w_{lr} \delta_{kl}
$$

where units $r$ connects to $l$.

Taking derivative of $\delta\_{kr}$ with respect to $s\_i$, we obtain the backpropagation formula for $\phi\_{kri}$

$$
\phi_{kri} = h^{\prime\prime} (a_r) \frac{\partial a_r}{\partial s_i} \sum_l w_{lr} \delta_{kl}
+ h^\prime (a_r) \sum_l w_{lr} \phi_{kli}
$$

Recall that in excersice 5.15, forward propagation for evaluating Jacobian matrix is given by

$$
\beta_k = \frac{\partial a_k}{\partial x_i} = \sum_j w_{kj} \frac{\partial z_j}{\partial x_i},
\quad \alpha_j = \frac{\partial z_j}{\partial x_i} = h^\prime (a_j) \frac{\partial a_j}{\partial x_i}
$$

in which units $j$ connect to unts $k$. We can use this formula to evaluate $\partial z\_s / \partial x\_i$ and $\partial a\_r / \partial x\_i$.

Defining the operator

$$
\mathcal{G} = \sum_{i=1}^D \tau_i \frac{\partial}{\partial x_i}
$$

the derivative of $\Omega\_n$ with respect to $w\_{rs}$ is written

$$
\begin{align*}
\frac{\partial \Omega_n}{\partial w_{rs}}
= \sum_k \mathcal{G} y_k \left( z_s \mathcal{G} \delta_{kr} + \delta_{kr} \mathcal{G} z_s \right)
\end{align*}
$$

If the transformation is governed by multiple parameters, then there will be one regularization term for each parameter, each of which is defined by (5.128).
The overrall regularization is obtained by summing them up.

### Training with transformed data

Here we show that the tagent propagation is closely related to the technique of augmenting the data set with various transformations.

Consider a network with a single output, with the sum-of-squares error of untransformed inputs given by

$$
E = \frac{1}{2} \iint \left[ y(\mathbf{x}) - t \right]^2 p(t|\mathbf{x}) p(x) \,d\mathbf{x}dt
$$

Define a transformation $\mathbf{s} (\mathbf{x}, \xi)$, with $\xi$ drawn from a distribution $p(\xi)$.
Then the error function gives

$$
\widetilde{E} = \frac{1}{2} \iiint \left[ y(s(\mathbf{x}, \xi)) - t \right]^2
p(t|\mathbf{x}) p(\mathbf{x}) p(\xi) \,d\mathbf{x}dtd\xi
$$

Assume that $\xi$ has zero mean and small variance. Expand $y(\mathbf{s}(\mathbf{x}, \xi))$ at $\xi = 0$

$$
y(\mathbf{s}(\mathbf{x}, \xi))
= y(\mathbf{x}) + \xi \boldsymbol{\tau}^{\mathsf{T}} \nabla y(\mathbf{x})
+ \frac{\xi^2}{2} \left[ (\boldsymbol{\tau}^\prime)^{\mathsf{T}} \nabla y(\mathbf{x})
+ \boldsymbol{\tau}^{\mathsf{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\tau} \right]
+ O(\xi^3)
$$

where we defined

$$
\boldsymbol{\tau} = \frac{\partial }{\partial \xi} \mathbf{s} (\mathbf{x}, \xi) \bigg|_{\xi = 0},
\quad \boldsymbol{\tau}^\prime = \frac{\partial^2 }{\partial \xi^2} \mathbf{s} (\mathbf{x}, \xi) \bigg|_{\xi = 0}
$$

Substituting into the error function we obtain

$$
\begin{align*}
\widetilde{E} = & \frac{1}{2} \iint \left[ y(\mathbf{x}) - t \right]^2 p(\mathbf{x}, t) \,d\mathbf{x}dt\\
&+ \operatorname{E} [\xi] \iint \left[ y(\mathbf{x}) -t \right] \boldsymbol{\tau}^{\mathsf{T}} \nabla y(\mathbf{x}) p(\mathbf{x}, t) \,d\mathbf{x}dt \\
&+ \operatorname{E} [\xi^2] \frac{1}{2} \iint \bigg\{
\left[ y(\mathbf{x}) -t \right] \left[ (\boldsymbol{\tau}^\prime)^{\mathsf{T}} \nabla y(\mathbf{x})
+ \boldsymbol{\tau}^{\mathsf{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\tau} \right] \\
&\qquad \qquad \qquad + \left[ \boldsymbol{\tau}^{\mathsf{T}} \nabla y(\mathbf{x})\right]^2 \bigg\} p(\mathbf{x}, t) \,d\mathbf{x} dt
+ O(\xi^3)
\end{align*}
$$

Since $\xi$ has zero mean, the second term vanishes. Denoting $\operatorname{E} [\xi^2]$ by $\lambda$, the error funciton can be written

$$
\widetilde{E} = E + \lambda \Omega \tag{5.131}
$$

where $\Omega$ is defined as

$$
\begin{align*}
\Omega = \frac{1}{2} \int \bigg\{&
\left[ y(\mathbf{x}) - \operatorname{E} [t|\mathbf{x}] \right] \left[ (\boldsymbol{\tau}^\prime)^{\mathsf{T}} \nabla y(\mathbf{x})
+ \boldsymbol{\tau}^{\mathsf{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\tau} \right] \\
&+ \left[ \boldsymbol{\tau}^{\mathsf{T}} \nabla y(\mathbf{x})\right]^2 \bigg\} p(\mathbf{x}) \,d\mathbf{x}
\end{align*}
$$

> How to get $y(\mathbf{x}) = \operatorname{E} [t\|\mathbf{x}] + O(\xi^2)$ ?????

Substitute $y(\mathbf{x})$ with the unregularized optimal solution $E[t\|\mathbf{x}]$, we have

$$
\Omega = \frac{1}{2} \int \left[ \boldsymbol{\tau}^{\mathsf{T}} \nabla y(\mathbf{x})\right]^2 p(\mathbf{x}) \,d\mathbf{x}
$$

which is equivalent to the regularizer (5.128). Note that, though it seems that (5.128) requires a factor $1/N$ to achive the equivalence,
the same factor required by the unregularized error cancels the effect of averaging over $n$.
Thereby in the limit $n \rightarrow \infty$, both regularizers are equivalent.

In particular, if the transformation is an additive random noise $\boldsymbol{\xi}$ on the input so that $\mathbf{x} \rightarrow \mathbf{x} + \boldsymbol{\xi}$, the regularizer takes the form

$$
\Omega = \frac{1}{2} \int \| \nabla y(\mathbf{x}) \|^2 p(\mathbf{x}) \,d\mathbf{x}
$$

which is known as Tikhonov regularization.

### Convelutional networkds

Convelutional neural network builds the invariance into the structure of the network and has been widely applied to images.

### Soft weight sharing

One way to reduce model complexity of a network with large number of parameters is to
share weights within certain groups.

**Soft weight sharing** uses regularization to encourage weights within groups to have similar values.
The division of groups, the mean weights of each group, and the value spread within each group are determined during learning process.

Consider a prior over parameters

$$
p(\mathbf{w}) = \prod_i p(w_i)
$$

where each component follows mixture Gaussians

$$
p(w_i) = \sum_{j=1}^M \pi_j \mathcal{N} (w_i | \mu_j, \sigma_j^2)
$$

Taking the negtive logarithm, we obtain a regularization term

$$
\Omega(\mathbf{w}) = - \sum_i \ln \left( \sum_j \pi_j \mathcal{N} (w_i | \mu_j, \sigma_j^2) \right)
$$

And the total error is given by

$$
\widetilde{E}(\mathbf{w}) = E(\mathbf{w}) + \lambda \Omega(\mathbf{w})
$$

The error function is minizied with repspect to $\mathbf{w}, \mu\_j, \sigma\_j^2$ and $\pi\_j$.
A joint optimization is required.

To keep notations uncluttered, we introduce the posterior probability of a component (a.k.a responsibility)
to be denoted by

$$
\gamma_j (w) = \frac{\pi_j \mathcal{N} (w|\mu_j, \sigma_j^2)}{\sum_k \pi_k \mathcal{N} (w|\mu_k, \sigma_k^2)}
$$

Taking derivative of the error function with respect to weights gives

$$
\frac{\partial \widetilde{E}}{\partial w_i}
= \frac{\partial E}{\partial w_i} + \lambda \sum_j \gamma_j(w_i) \frac{w_i - \mu_j}{\sigma_j^2}
$$

Taking derivative with respect to $\mu\_j$ gives

$$
\frac{\partial \widetilde{E}}{\partial \mu_j}
= \lambda \sum_i \gamma_j(w_i) \frac{\mu_j - w_i}{\sigma_j^2}
$$

Taking derivative with respect to $\sigma\_j$ gives

$$
\frac{\partial \widetilde{E}}{\partial \sigma_j}
= \lambda \sum_i \gamma_j(w_i) \left( \frac{1}{\sigma_j} - \frac{(w_i - \mu_j)^2}{\sigma_j^3} \right)
$$

In practice, to ensure $\sigma\_j$ remains postive, new variables $\eta\_j$ is introduced such that

$$
\sigma_j^2 = \exp ({\eta_j})
$$

The derivative with respect to $\eta\_j$ is then given by

$$
\begin{align*}
\frac{\partial \widetilde{E}}{\partial \eta_j} =
\frac{\partial \widetilde{E}}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial \eta_j}
\end{align*}
$$

For mixing coefficients $\pi\_j$, we have constraints

$$
\sum_j \pi_j = 1, \quad 0 \le \pi_j \le 1
$$

this can be achieved by using the softmax function

$$
\pi_j = \frac{\exp (\eta_j)}{\sum_k \exp (\eta_j)}
$$

**Excercise 5.32**

The derivative with repsect to $\eta\_j$ is then given by

$$
\begin{align*}
\frac{\partial \widetilde{E}}{\partial \eta_j}
&= \sum_k \frac{\partial \widetilde{E}}{\partial \pi_k} \frac{\partial \pi_k}{\partial \eta_j} \\
&= \sum_k \left\{ - \sum_i \gamma_k (w_i) (I_{kj} - \pi_j) \right\} \\
&= \sum_i \sum_k \gamma_k (w_i) (\pi_j - I_{kj} ) \\
&= \sum_i \left( \gamma_j (w_i) - \pi_j \right) \\
\end{align*}
$$

## Mixture density networks

Practical machine learning problems often have non-Gaussian distributions, to which
the Guassian assumption will not fit well.

Consider a forward problem that calculates the effects from the causes.
If it involves one-to-many mapping, the inverse problem, which infer causes from the effects,
will have multiple solutions or be multimodal.
For example, if the forward problem is to list a set of symptoms given a particular disease,
the inverse will be infer the dicease from the symptoms.

To model such non-Gaussian problem, we use a mixture model in which the mixing coefficients
and the component densities are functions of input $\mathbf{x}$.
With this method, we build the **mixture density network**.

As a special case, consider a mixture model with Gaussian components such that

$$
p(\mathbf{t} | \mathbf{x}) = \sum_{k=1}^K \pi_k(\mathbf{x})
\mathcal{N} (\mathbf{t}|\boldsymbol{\mu}_k (\mathbf{x}), \sigma_k^2(\mathbf{x})\mathbf{I})
$$

The parameters then become the output of a neural network. Suppose $\mathbf{t}$ has $D$ components,
then there will be $K$ output activations denoted by $a\_k^\pi$ for mixing coefficients $\pi\_k(\mathbf{x})$,
$K$ output activations denoted by $a\_k^\sigma$ for variances $\sigma\_k^2(\mathbf{x})$, and
$D \times K$ output activations denoted by $a\_{kj}^\mu$ for componets $\mu\_{kj}(\mathbf{x})$ of $\boldsymbol{\mu}\_{k}(\mathbf{x})$.

The mixing coeffiecients are constrained by

$$
\sum_k \pi_k = 1, \quad, 0 \le \pi_k \le 1
$$

So we can use a softmax output function

$$
\pi_k = \frac{\exp (a_k^\pi)}{\sum_l \exp(a_l^\pi)}
$$

To satisfy $\sigma\_k \ge 0$, we choose the output function

$$
\sigma_k = \exp (a_k^\sigma)
$$

For the means $\boldsymbol{\mu}\_k$ we simply use

$$
\mu_{kj} = a_{kj}^\mu
$$

Taking the negative logarithm of the likelihood function, the error function is given by

$$
E(\mathbf{w}) = - \sum_{n=1}^N \ln \left\{
\sum_{k=1}^K \pi_k(\mathbf{x}_n, \mathbf{w})
\mathcal{N} (\mathbf{t}_n | \boldsymbol{\mu}_k (\mathbf{x}_n, \mathbf{w}), \sigma_k^2(\mathbf{x}_n, \mathbf{w})\mathbf{I})
\right\}
$$

For convenience, we introduce the posterior distribution

$$
\gamma_{nk} = \gamma_k (\mathbf{t}_n | \mathbf{x}_n)
= \frac{\pi_k \mathcal{N}_{nk}}{\sum_l \pi_l \mathcal{N}_{nl}}
$$

where we defined $\mathcal{N}\_{nk} \equiv \mathcal{N} (\mathbf{t}\_n \| \boldsymbol{\mu}\_k (\mathbf{x}\_n), \sigma\_k^2(\mathbf{x}\_n)\mathbf{I})$

Taking derivative with respect to $a\_k^\pi$ gives

$$
\begin{align*}
\frac{\partial E_n}{\partial a_k^\pi}
= \sum_j \gamma_{nj} (\pi_k - I_{kj})
= \pi_k - \gamma_{nk}
\end{align*}
$$

Given

$$
\begin{align*}
\frac{\partial \mathcal{N}_{nk}}{\partial a_{kl}^\mu}
= \frac{\partial \mathcal{N}_{nk}}{\partial \mu_{kl}}
\frac{\partial \mu_{kl}}{\partial a_{kl}^\mu}
= - \frac{\mu_{kl} - t_{nl}}{\sigma_k^2} \mathcal{N}_{nk}
\end{align*}
$$

derivatives with respect to $a\_{kl}^\mu$ give

$$
\frac{\partial E_n}{\partial a_{kl}^\mu} = \gamma_{nk} \frac{\mu_{kl} - t_{nl}}{\sigma_k^2}
$$

Given

$$
\begin{align*}
\frac{\partial \mathcal{N}_{nk}}{\partial a_k^\sigma}
&= \frac{\partial \mathcal{N}_{nk}}{\partial \sigma_k}
\frac{\partial \sigma_k}{\partial a_k^\sigma} \\
&= - D (2\pi)^{-D/2} \sigma_k^{-D-1} \exp ({\cdots}) \\
&\quad + \sigma_k^{-3} \| \mathbf{t}_n - \boldsymbol{\mu}_k \|^2 (2\pi)^{-D/2} \sigma_k^{-D} \exp ({\cdots}) \\
&= \left( \sigma_k^{-3} \| \mathbf{t}_n - \boldsymbol{\mu}_k \|^2 - D \sigma_k^{-1} \right) \mathcal{N}_{nk}
\end{align*}
$$

derivatives with respect to $a\_k^\sigma$ gives

$$
\frac{\partial E_n}{\partial a_k^\sigma}
= \gamma_{nk} \left( D - \frac{\| \mathbf{t}_n - \boldsymbol{\mu}_k \|^2}{\sigma_k^2} \right)
$$

Knowing the derivatives of error function with respect to the output unit activations, 
we can evaluate the derivatives with respect to $\mathbf{w}$ using backpropagation.

We can use the predictive distribution to obtain the conditional mean of target

$$
\operatorname{E} [\mathbf{t}|\mathbf{x}]
= \int \mathbf{t} p(\mathbf{t}|\mathbf{x}) \,d\mathbf{t}
= \sum_{k=1}^K \pi_k (\mathbf{x}) \boldsymbol{\mu}_k (\mathbf{x})
$$

which corresponds to the solution of a standard network trained with least squares.
However for multimodal distributions, the conditional means have limited value.
The conditional mode may be of more value, which may require numerical computation.
A simple alternative is to select the mean of the most probable component.

We can also obtain the variance given by

$$
\begin{align*}
s^2(\mathbf{x}) &= \operatorname{E} \big[ \| \mathbf{t} - \operatorname{E} [\mathbf{t}|\mathbf{x}] \|^2 | \mathbf{x} \big] \\
&= \sum_{k=1}^K \pi_k(\mathbf{x}) \left\{ D \sigma_k^2(\mathbf{x})
+ \left\| \boldsymbol{\mu}_k(\mathbf{x}) - \sum_{l=1}^K \pi_l(\mathbf{x}) \boldsymbol{\mu}_l(\mathbf{x})  \right\|^2 \right\}
\end{align*}
$$

which can be derived by following steps. To keep notations uncluttered, the condition on $\mathbf{x}$ will be omitted.

**Excercise 5.37**

First we show the covariance

$$
\begin{align*}
\operatorname{cov} [\mathbf{t}]
&= \operatorname{E} [\mathbf{t} \mathbf{t}^{\mathsf{T}}] - 
\operatorname{E} [\mathbf{t}] \operatorname{E} [\mathbf{t}]^{\mathsf{T}} \\
&= \sum_k \pi_k \left( \sigma_k^2 \mathbf{I} + \boldsymbol{\mu}_k \boldsymbol{\mu}_k^{\mathsf{T}} \right)
- \operatorname{E} [\mathbf{t}] \operatorname{E} [\mathbf{t}]^{\mathsf{T}}
\end{align*}
$$

Here we used

$$
\operatorname{E} [\mathbf{x} \mathbf{x}^{\mathsf{T}}]
= \operatorname{cov} [\mathbf{x}] + \operatorname{E} [\mathbf{x}] \operatorname{E} [\mathbf{x}]^{\mathsf{T}}
$$

Taking trace of both sides

$$
\begin{align*}
s^2(\mathbf{x}) = \operatorname{Tr} (\operatorname{cov}[\mathbf{t}])
&= \sum_k \pi_k \left( D\sigma_k^2 + \boldsymbol{\mu}_k^{\mathsf{T}} \boldsymbol{\mu}_k \right)
- \operatorname{E} [\mathbf{t}]^{\mathsf{T}} \operatorname{E} [\mathbf{t}] \\
\end{align*}
$$

Note that

$$
\begin{align*}
\sum_k \pi_k \boldsymbol{\mu}_k^{\mathsf{T}} \boldsymbol{\mu}_k
- \operatorname{E} [\mathbf{t}]^{\mathsf{T}} \operatorname{E} [\mathbf{t}] 
&= \sum_k \pi_k \boldsymbol{\mu}_k^{\mathsf{T}} \boldsymbol{\mu}_k
- 2\operatorname{E} [\mathbf{t}]^{\mathsf{T}} \operatorname{E} [\mathbf{t}] 
+ \operatorname{E} [\mathbf{t}]^{\mathsf{T}} \operatorname{E} [\mathbf{t}]  \\
&= \sum_k \pi_k \boldsymbol{\mu}_k^{\mathsf{T}} \boldsymbol{\mu}_k
- 2 \sum_k \pi_k \boldsymbol{\mu}_k^{\mathsf{T}} \operatorname{E} [\mathbf{t}]
+ \sum_k \pi_k \operatorname{E} [\mathbf{t}]^{\mathsf{T}} \operatorname{E} [\mathbf{t}] \\
&= \sum_k \pi_k \left\| \boldsymbol{\mu}_k - \operatorname{E} [\mathbf{t}] \right\|^2
\end{align*}
$$

## Bayesian neural networks

### Posterior parameter distribution

Consider the problem of predicting a univariate continuous target $t$. The target distribution is given by

$$
p(t|\mathbf{x}, \mathbf{w}, \beta) = \mathcal{N} (t|y(\mathbf{x},\mathbf{w}), \beta^{-1})
$$

The prior is chosen to give

$$
p(\mathbf{w}|\alpha) = \mathcal{N} (\mathbf{w}|\mathbf{0}, \alpha^{-1} \mathbf{I})
$$

Given a set of inputs ${\mathbf{x}\_i}$ and corresponding targets $\mathcal{D} = \{t1\_, \dots, t\_N\}$, the likelihood function is given by

$$
p(\mathcal{D} | \mathbf{w}, \beta) = \prod_{n=1}^N \mathcal{N} (t_n | y(\mathbf{x}_n, \mathbf{w}), \beta^{-1})
$$

By

$$
p(\mathbf{w}|\mathcal{D}, \alpha, \beta) \propto p(\mathcal{D} | \mathbf{w}, \beta) p(\mathbf{w}|\alpha)
$$

and assuming that $\alpha$ and $\beta$ are known constant, the log posterior can be written as

$$
p(\mathbf{w}|\mathcal{D})
= - \frac{\alpha}{2} \mathbf{w}^{\mathsf{T}} \mathbf{w}
- \frac{\beta}{2} \sum_n \left[ y(\mathbf{x}_n, \mathbf{w}) - t_n \right]^2 + \textrm{const}
$$

where $\textrm{const}$ concludes terms independent of $\mathbf{w}$. We can then use numerical
methods to find the mode $\mathbf{w}\_{\mathrm{MAP}}$, and give a Laplace approximation around
$\mathbf{w}\_{\mathrm{MAP}}$ 

$$
q(\mathbf{w}|\mathcal{D}) = \mathcal{N} (\mathbf{w} | \mathbf{w}_{\mathrm{MAP}}, \mathbf{A}^{-1})
$$

where $\mathbf{A}$ is given by

$$
\begin{align*}
\mathbf{A} &= - \nabla \nabla \ln p(\mathbf{w}|\mathcal{D})
= \alpha \mathbf{I} + \beta \mathbf{H} \\
\end{align*}
\tag{5.166}
$$

and $\mathbf{H}$ is the Hessian of the sum-of-squares error

$$
\frac{1}{2} \sum_n \left[ y(\mathbf{x}_n, \mathbf{w}) - t_n \right]^2
$$

The predictive distribution is then evaluated with

$$
p(t|\mathcal{D}) = \int p(t|\mathbf{x}, \mathbf{w}, \beta) q(\mathbf{w}|\mathcal{D}) \,d\mathbf{w}
$$

However, nonlinearity of the function $y(\mathbf{x}, \mathbf{w})$ makes the integral intractable.
Assume that the posterior has sufficiently small variance, $y(\mathbf{x}, \mathbf{w})$ in the integrand can be linearly approximated by

$$
y(\mathbf{x}, \mathbf{w}) \simeq y(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}})
+ \mathbf{g}^{\mathsf{T}} (\mathbf{w} - \mathbf{w}_{\mathrm{MAP}})
$$

where $\mathbf{g} = \nabla\_{\mathbf{w}} y(\mathbf{x}, \mathbf{w}\_{\mathrm{MAP}})$

$$
p(t|\mathbf{x}, \mathbf{w}, \beta)
\simeq \mathcal{N} (t | y(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}})
+ \mathbf{g}^{\mathsf{T}} (\mathbf{w} - \mathbf{w}_{\mathrm{MAP}}), \beta^{-1})
$$

With following change of variables

$$
\begin{align*}
\mathbf{y} &\rightarrow t, &\mathbf{x} &\rightarrow \mathbf{w},
&\mathbf{A} &\rightarrow \mathbf{g}^{\mathsf{T}},
& b &\rightarrow y(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}) - \mathbf{g}^{\mathsf{T}} \mathbf{w}_{\mathrm{MAP}} \\
\mathbf{L}^{-1} &\rightarrow \beta^{-1},
& \boldsymbol{\mu} &\rightarrow \mathbf{w}_{\mathrm{MAP}},
& \boldsymbol{\Lambda}^{-1} &\rightarrow \mathbf{A}^{-1}\\
\end{align*}
$$

we obtain the marginalized distribution

$$
p(t|\mathbf{x}, \mathcal{D}, \alpha, \beta)
= \mathcal{N} (t|y(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}), \sigma^2(\mathbf{x}))
$$

where

$$
\sigma^2(\mathbf{x}) = \beta^{-1} + \mathbf{g}^{\mathsf{T}} \mathbf{A}^{-1} \mathbf{g}
$$

The first term is the intrinsic noise on the target. The second term the uncertainty in $\mathbf{w}$ at a given $\mathbf{x}$.

### Hyperparameter optimization

The model evidence is given by

$$
p(\mathcal{D} | \alpha, \beta) = \int p(\mathcal{D}|\mathbf{w}, \beta) p(\mathbf{w}|\alpha) \,d\mathbf{w}
$$

in which 

$$
\begin{align*}
p(\mathcal{D} | \mathbf{w}, \beta) &= \prod_{n=1}^N \mathcal{N} (t_n | y(\mathbf{x}_n, \mathbf{w}), \beta^{-1}) \\
p(\mathbf{w}|\alpha) &= \mathcal{N} (\mathbf{w}|\mathbf{0}, \alpha^{-1} \mathbf{I}) \\
\end{align*}
$$

Using (4.137), we obtain

$$
\ln p(\mathcal{D}|\alpha, \beta)
\simeq - E(\mathbf{w}_{\mathrm{MAP}}) - \frac{1}{2} \ln \big| \mathbf{A} |_{\mathbf{w}=\mathbf{w}_{\mathrm{MAP}}} \big|
+ \frac{W}{2} \ln \alpha + \frac{N}{2} \ln \beta - \frac{N}{2} \ln 2 \pi
\tag{5.175}
$$

where $W$ is the number of parameters in $\mathbf{w}$, $\mathbf{A}$ is given by (5.166) and we defined

$$
E(\mathbf{w}_{\mathrm{MAP}}) = \frac{\beta}{2} \sum_{n=1}^N \left[ y(\mathbf{x}_n, \mathbf{w}_{\mathrm{MAP}}) -t_n \right]^2 + \frac{\alpha}{2} \mathbf{w}^{\mathsf{T}} \mathbf{w}
$$

Defining

$$
\beta \mathbf{H} \mathbf{u}_i = \lambda_i \mathbf{u}_i
$$

and following the steps in section 3.5.2, we obtain the derivatice with respect to $\alpha$ given by

$$
\alpha = \frac{\gamma}{\mathbf{w}_{\mathrm{MAP}}^{\mathsf{T}} \mathbf{w}_{\mathrm{MAP}}}
\tag{5.178}
$$

where

$$
\gamma = \sum_{i=1}^W \frac{\lambda_i}{\alpha + \lambda_i}
$$

> Varianation in $\alpha$ will cause $\mathbf{H}$ to vary???

The derivative with respect to $\beta$ is given by

$$
\frac{1}{\beta} = \frac{1}{N - \gamma} \sum_{n=1}^N \left[ y(\mathbf{x}_n, \mathbf{w}) - t_n \right]^2
$$

As with the linear model, we alternate the optimization of the hyperparameters and the posterior distribution.

Note that, due to the non-linearrity of the neural network, the solution $\mathbf{w}\_{\mathrm{MAP}}$ found will depend on the initialization of $\mathbf{w}$.
There will be equivalent modes due to symmetric property, as well as many inequivalent solutions which will yeild different optimal values for hyperparameters.

To compare different models, we first obtain the optimized hyperparameters,
and then evaluate the approximation to the model evidence given by (5.175).

When comparing models with different number of hidden units, we can take the symmetries into
account by multiplying the evidence with a factor $M! 2^M$.

> Justification??

### Bayesian neural networks for classfication

Consider a binary classification problem with a single logistic sigmoid output, the target $t \in \{0, 1\}$ and the target distribution given by

$$
p(t|\mathbf{x}, \mathbf{w}) = y(\mathbf{x}, \mathbf{w})^t \left[ 1 - y(\mathbf{x}, \mathbf{w}) \right]^{1-t}
$$

Assuming that the data are correctly labels, the role played by hyperparameter $\beta$ vanishes.

The prior is chosen as the isotropic Gaussian

$$
p(\mathbf{w})  = \mathcal{N}  (\mathbf{w} | \mathbf{0}, \alpha^{-1} \mathbf{I})
$$

The log likelihood is given 

$$
p(\mathcal{D}|\mathbf{w})
= \sum_{n=1}^N \left[ t_n \ln y_n + (1 - t_n) \ln (1- y_n) \right]
$$

where we defined $y\_n \equiv y(\mathbf{x}\_n, \mathbf{w})$

First we initialize $\alpha$, and then by maximizing the posterior we find the solution $\mathbf{w}\_{\mathrm{MAP}}$. By

$$
\ln p(\mathbf{w}|\mathcal{D}) = \ln p(\mathcal{D}|\mathbf{w}) + \ln p(\mathbf{w}) + \text{const}
$$

we see that maximizing the posterior is equivalent to minimizing the regularized regularized error

$$
E(\mathbf{w}) = - \ln p(\mathcal{D}|\mathbf{w}) + \frac{\alpha}{2} \mathbf{w}^{\mathsf{T}} \mathbf{w}
\tag{5.182}
$$

The next step is to evaluate the Hessian of $\ln p(\mathcal{D}\|\mathbf{w})$ with respect to $\mathbf{w}$, which have been discussed in previous sections.

To optimize $\alpha$, using (4.137) again, we maximize the evidence approximated by

$$
\ln p(\mathcal{D}|\alpha)
\simeq - E(\mathbf{w}_{\mathrm{MAP}}) - \frac{1}{2} \ln |\mathbf{A}| + \frac{W}{2} \ln \alpha
$$

where $E(\cdot)$ is defined by (5.186). The required iteration to optimize $\alpha$ is given by (5.178).

