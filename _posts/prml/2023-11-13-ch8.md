---
layout: posts
title: 'PRML Ch8 - Graphical Models'
---
- [Graphical models](#graphical-models)
  - [Bayesian networks](#bayesian-networks)
    - [Example: polynomial regression](#example-polynomial-regression)
    - [Generative models](#generative-models)
    - [Discrete variables](#discrete-variables)
    - [Linear-Gaussian models](#linear-gaussian-models)
  - [Conditional independnece](#conditional-independnece)
    - [Three example cases](#three-example-cases)
    - [D-Separation](#d-separation)
# Graphical models


## Bayesian networks

An arbitrary joint distribution $p(a,b,c)$ can be written in the form

$$
p(a,b,c) = p(c|a,b) p(b|a) p(a)
$$

The graph representation of the r.h.s is shown as

![](/assets/images/prml/fig-8.1.png)

A graphical model is constructed by two steps:

1. Represent each random variable by a node
2. Add directed eadges from any node a to any node b if b is conditioned on a

Given a joint distribution over $K$ variables $p(x\_1, \dots, x\_K)$, we can write

$$
p(x_1, \dots, x_K) = p(x_K | x_{K-1}, \dots, x_1) \dots p(x_2|x_1) p(x_1)
$$

The resulting graph will be such that every node sends an outbound edge to each higher numberd nodes.
It is said to be fully connected since there's a link between every pair of nodes.

Given a graph

![](/assets/images/prml/fig-8.2.png)

the distribution is uniquely defined as

$$
p(x_1) p(x_2) p(x_3) p(x_4|x_1, x_2, x_3) p(x_5|x_1, x_3) p(x_6|x_4) p(x_7|x_4, x_5)
$$

Formally for a graph with $K$ nodes, the joint distribution is given by

$$
p(\mathbf{x}) = \prod_{k=1}^K p(x_k|\operatorname{pa}_k)
$$

where $\operatorname{pa}\_k$ are the set of parents of node $k$ and $\mathbf{x} = \{ x\_1, \dots, x\_K \}$.

Note that, a valid probability graph must be a DAG.

### Example: polynomial regression

### Generative models

Since the graphical models captures the **casual** process by which the observations are generated, they are often called generative models.
An example of image generation:

![](/assets/images/prml/fig-8.8.png)


Given a joint distribution $p(x\_1, \dots, x\_K)$ and its graph, the **ancestral sampling** samples random variables one by one in a topological order.

Typically the terminal nodes correspond to the ovserved variables and the internal nodes correspond to the latent (or hidden) variables.

### Discrete variables

Consider two discrete variables $\mathbf{x}\_1$ and $\mathbf{x}\_2$, each of which has $K$ states. We denote the probability of ${x}\_{1k} = {x}\_{2l} = 1$ by $\mu\_{kl}$.
The joint probability is then given by

$$
p(\mathbf{x}_1, \mathbf{x}_2 | \boldsymbol{\mu}) = \prod_{k=1}^K \prod_{l=1}^K \mu_{kl}^{x_{1k} x_{2l}}
$$

where $\sum\_{k,l} \mu\_{kl} = 1$. This distribution is governed by $K^2 -1$ parameters.
For a distribution consisting of $M$ such discrete variables, which corresponds to a fully-connected graph, the total parameters will be $K^M - 1$.

If we write the joint distribution in the form

$$
\mathbf{x}_1 \rightarrow \mathbf{x}_2
$$

then $p(\mathbf{x}\_1)$ is governed $K-1$ parameters. And for each state $\mathbf{x}\_1$ takes,
$\mathbf{x}\_2$ is governed $K-1$ independent parameters, which gives $p(\mathbf{x}\_2\|\mathbf{x}\_1)$ a total of $K(K-1)$ parameters. 
The total number of parameters in the model is still $K^2 - 1$.

We can reduce the number of parameters in several ways at the expanse of restricted model.

The simplest way is to drop all dependences of between variables, which gives a model

$$
p(\mathbf{x}_1, \mathbf{x}_2) = p(\mathbf{x}_1) p(\mathbf{x}_2)
$$

Then a model with $M$ variables has $M(K-1)$ parameters.

A stronger connectivity is achieved by the model

![](/assets/images/prml/fig-8.10.png)

where the distribution $p(\mathbf{x}\_1)$ has $K-1$ parameters, and each $p(\mathbf{x}\_i\|\mathbf{x}\_{i-1})$ has $K(K-1)$ parameters, giving a total of $K-1 + (M-1) K (K-1)$ parameters.

An alternative way to reduce the number of parameters is by sharing (a.k.a. tying) parameters.
E.g. by making $p(\mathbf{x}\_i\|\mathbf{x}\_{i-1})$ where $i=2,\dots, M$ share the same set of $K(K-1)$ parameters, we reduce the number of total parameters to $K(K-1)$.

TODO

### Linear-Gaussian models

A multivariate Gaussian can be expressed as a directed graph of a linear-Gaussian model over the components.

Consider a $D$-dimensional Gaussian variable $\mathbf{x}$, with each component defined by a graphical model

$$
p(x_i|\operatorname{pa}_i) = \mathcal{N} (x_i | \sum_{j \in \operatorname{pa}_i} w_{ij} x_j + b_i, v_i)
$$

where the mean is a linear combination of its parents and $v\_i$ is the variance.

The log of the joint distribution is then written

$$
\begin{align*}
\ln p(\mathbf{x}) &= \sum_{i=1}^D p(x_i|\operatorname{pa}_i) \\
&= - \sum_{i=1}^D \frac{1}{2 v_i} \left( x_i - \sum_{j \in \operatorname{pa}_i} w_{ij} x_j - b_i \right)^2 + \text{const} \\
\end{align*}
$$

where the $\text{const}$ denotes terms independent of $\mathbf{x}$. We verify that the joint distribution is a Gaussian.

To determine the mean and covariance of the joint distribution, we first write

$$
x_i = \sum_{j \in \operatorname{pa}_i} w_{ij} x_j + b_i + \sqrt{v_i} \epsilon_i
$$

where $\epsilon\_i$ follows the unit Gaussian $\mathcal{N} (0, 1)$. Taking the expectation, we have

$$
\operatorname{E} [x_i] = \sum_{j \in \operatorname{pa}_i} w_{ij} \operatorname{E} [x_j] + b_i 
\tag{8.15}
$$

The covariace can be written

$$
\begin{align*}
\operatorname{cov} [x_i, x_j]
&= \operatorname{E} \left[ (x_i - \operatorname{E} [x_i]) (x_j - \operatorname{E} [x_j]) \right] \\
&= \operatorname{E} \left[ (x_i - \operatorname{E} [x_i]) \left( \sum_{k \in \operatorname{pa}_j} w_{jk} (x_k - \operatorname{E} [x_k])  + \sqrt{v_j} \epsilon_j \right) \right] \\
&= \sum_{k \in \operatorname{pa}_j} w_{jk} \operatorname{cov} [x_i, x_k] + I_{ij} v_j
\tag{8.16}
\end{align*}
$$

Here we used $\operatorname{E} [\epsilon\_i \epsilon\_j] = I\_{ij}$.

The values can then be calculated in following procedure:

1. Topologically sort the nodes. Assume that the result is $x\_1, \dots, x\_D$.
2. Starting from $i=1$, we evaluate $\operatorname{E} [x\_i]$ with (8.15). For nodes with no parents, we have $\operatorname{E} [x\_i] = b\_i$.
3. Starting from $i = 1$, we evaluate $\operatorname{cov} [x\_i, x\_j]$ with (8.16), where $j \le i$. For any node, we have $\operatorname{cov} [x\_i, x\_i] = v\_i$. Note that $\operatorname{cov} [x\_i, x\_j] = \operatorname{cov} [x\_j, x\_i]$.

In the particular case where all components are independent, i.e. the graph has no edges, we have

$$
\begin{align*}
\operatorname{E} [\mathbf{x}] &= (b_1, \dots, b_D)^{\mathsf{T}} \\
\operatorname{cov} [\mathbf{x}] &= \operatorname{diag} (v_1, \dots, v_D) \\
\end{align*}
$$

If the graph is fully connected, there will be $D(D-1)/2$ edges, which corresponds to the same number of $\{w\_{ij}\}$.
The total number of parameters in the covariance matrix is therefore $D(D+1)/2$ corresponding to a general covariance matrix.

A intermediate level of connectivity can be achived by the model

$$
x_1 \rightarrow x_2 \rightarrow \cdots \rightarrow x_D
$$

which gives $2D - 1$ parameters in the covariance matrix and a total of $3D-1$ independen parameters in the model.

The linear-Gaussian model can be extended to the case in which each node is a multivariate Gaussian variable.

$$
p(\mathbf{x}_i | \operatorname{pa}_i) = \mathcal{N} \left( \mathbf{x}_i | \sum_{j \in \operatorname{pa}_i} \mathbf{W}_{ij} \mathbf{x}_j , \boldsymbol{\Sigma}_i \right)
$$

## Conditional independnece

Given randome variables $a, b$ and $c$, if

$$
p(a|b,c) = p(a|c)
$$

we say $a$ is conditionally independent of $b$ given $c$. This can be equivalently expressed as

$$
p(a,b|c) = p(a|b,c) p(b|c) = p(a|c) p(b|c)
$$

It can be written in the notation

$$
a \perp\!\!\!\perp b \mid c
$$

Note that, the dependence must hold for all values of $c$.

### Three example cases

Consider the joint distribution defined by

$$
p(a, b, c) = p(a|c) p(b|c) p(c)
$$

which is shown by the graphical model

$$
a \leftarrow c \rightarrow b
$$

Without observing $c$, the dependence between $a$ and $b$ can be infered by marginalizing $c$, so that

$$
p(a, b) = \sum_c p(a|c) p(b|c) p(c)
$$

which is generally not equivalent to $p(a) p(b)$. So we have

$$
a \not\!\perp\!\!\!\perp b \mid \empty
$$

where $\empty$ denotes the empty set.

If $c$ is observed, we have

$$
p(a, b | c) = \frac{p(a,b,c)}{p(c)} = p(a|c) p(b|c)
$$

Thereby

$$
a \perp\!\!\!\perp b \mid c
$$

The graph is shown

![](/assets/images/prml/fig-8.16.png)

The node c is said to be **tail-to-tail** with respect to the path between $a$ and $b$ because both arrows starts from node c.

\
Consider the second case where

$$
p(a,b,c) = p(a) p(c|a) p(b|c)
$$

Corresponding graphical model is shown by

$$
a \rightarrow c \rightarrow b
$$

Without $c$ observed, the dependence between $a$ and $b$ can be seen from

$$
p(a, b) = p(a) \sum_c p(c|a) p(b|c)
$$

which is generally not equivalent to $p(a) p(b)$. So we have

$$
a \not\!\perp\!\!\!\perp b \mid \empty
$$

If $c$ is observed, then

$$
p(a, b | c)  = \frac{p(a,b,c)}{p(c)} = \frac{p(a) p(c|a) p(b|c)}{p(c)}
= p(a|c) p(b|c)
$$

which indicates

$$
a \perp\!\!\!\perp b \mid c
$$

![](/assets/images/prml/fig-8.18.png)

The node c is said to be **head-to-tail**.

\
The third case is the joint distribution defined by

$$
p(a,b,c) = p(a) p(b) p(c|a,b)
$$

Corresponding graph is shown by

$$
a \rightarrow c \leftarrow b
$$

Marginalizing out $c$ we have

$$
p(a, b) = p(a) p(b)
$$

Therefore

$$
a \perp\!\!\!\perp b \mid \empty
$$

If $c$ is observed, then

$$
p(a,b | c) = \frac{p(a,b,c)}{p(c)} = \frac{p(a) p(b) p(c|a,b)}{p(c)}
$$

which is generally not equivalent to $p(a) p(b)$. So we have

$$
a \not\!\perp\!\!\!\perp b \mid c
$$

![](/assets/images/prml/fig-8.20.png)

The node c is said to be **head to head**.

It can be shown that a head-to-head path will become blocked if either the head-to-head node or any of its descendents is observed.

### D-Separation

Let $A$, $B$ and $C$ be disjoint subsets of a graphical model. The union of the three sets does not need to includes all nodes.
The statements

$$
A \perp\!\!\!\perp B \mid C
$$


