---
title: 'PRML - Ch6 - Kernel Methods'
layout: posts
---
- [Ch6 Kernel Methods](#ch6-kernel-methods)
  - [Dual representation](#dual-representation)
  - [Constructing kernels](#constructing-kernels)
  - [Radial basis function networks](#radial-basis-function-networks)
    - [Nadaraya-Watson model](#nadaraya-watson-model)
  - [Gaussian process](#gaussian-process)
    - [Linear regression revisited](#linear-regression-revisited)
    - [Gaussian processes for regression](#gaussian-processes-for-regression)
    - [Learning the hyperparameters](#learning-the-hyperparameters)
    - [Automatic relevance determination](#automatic-relevance-determination)
    - [Gaussian process for classification](#gaussian-process-for-classification)
    - [Laplace appximation](#laplace-appximation)
    - [Connection to nerual networks](#connection-to-nerual-networks)

# Ch6 Kernel Methods

In previous chapters, we have introduced a class of models where predictions are made based
on a set of learned parameters. The training data can be discarded after the parameterers
are learned.

This chapter will discuss a class of *memory-based* methods, in which training data are stored and used to make predictions. 
These methods are typically fast to train but slow to predict.

Many linear models can be reformulated into a **dual representation**,
in which predictions take the form of a linear combination of a **kernel function** evaluated at training data points.

For models that use a set of fixed basis functions denoted by feature vector $\boldsymbol{\phi}(\mathbf{x})$, the kernel function is given

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}^\prime)
$$

A particular choice of $\boldsymbol{\phi}(\mathbf{x}) = \mathbf{x}$ gives a linear kernel
$k(\mathbf{x}, \mathbf{x}^\prime) = \mathbf{x}^{\mathsf{T}} \mathbf{x}^\prime$.

A technique called **kernel trick** or **kernel substitution**, can be applied when a model depend on the input vector only through some kernel function.
Then by replacing the original kernel with some other kernel with desired property,
we may extend the capability of the model. For instance PCA, Nearest Neighbor.

## Dual representation

Consider a regularized sum-of-sqaures error function given by

$$
J(\mathbf{w}) = \frac{1}{2} \sum_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}_n) - t_n \right)^2
+ \frac{\lambda}{2} \mathbf{w}^{\mathsf{T}} \mathbf{w}
$$

where $\lambda \ge 0$. Setting the derivative with respect to $\mathbf{w}$ to zero, we obtain

$$
\mathbf{w} = -\frac{1}{\lambda}  \sum_n \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}_n) - t_n \right) \boldsymbol{\phi} (\mathbf{x}_n)
= \sum_n a_n \boldsymbol{\phi}_n = \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{a}
$$

where we defined

$$
a_n = -\frac{1}{\lambda}  \left( \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}_n) - t_n \right) \\
\mathbf{a} = (a_1, \dots, a_N)^{\mathsf{T}}
$$

Back-substituting $\mathbf{w}$ with $\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{a}$,
we obtain the dual representation with respect to $\mathbf{a}$ given by

$$
J(\mathbf{a}) = \frac{1}{2} \| \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{a} - \mathbf{t} \|^2
+ \frac{\lambda}{2} \mathbf{a}^{\mathsf{T}} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{a}
$$

Defining the Gram matrix $\mathbf{K} = \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}}$,
whose elements are given by

$$
K_{nm} = \boldsymbol{\phi}(\mathbf{x}_n)^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_m)
= k(\mathbf{x}_n, \mathbf{x}_m)
$$

the dual function can be written as

$$
J(\mathbf{a}) = \frac{1}{2} \| \mathbf{K} \mathbf{a} - \mathbf{t} \|^2 + \frac{\lambda}{2} \mathbf{a}^{\mathsf{T}} \mathbf{K} \mathbf{a}
$$

Since the optimal value of $\mathbf{w}$ satisfies

$$
\begin{align*}
(\lambda \mathbf{I} + \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) \mathbf{w}
= \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}
\end{align*}
$$

we have

$$
\begin{align*}
(\lambda \mathbf{I} + \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{a}
&= \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
(\lambda \boldsymbol{\Phi} + \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}) \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{a}
&= \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
(\lambda \mathbf{K} + \mathbf{K}^2) \mathbf{a} &= \mathbf{K} \mathbf{t} \\
\end{align*}
$$

from which we the optimal solution for dual problem obtain

$$
\mathbf{a} = (\mathbf{K} + \lambda \mathbf{I}_N)^{-1} \mathbf{t}
$$

> The reulst is identical to that obtained by taking derivative with respect to $\mathbf{a}$. A consequence of strong duality??

Back-substituting into the regression model, we have

$$
y(\mathbf{x}) = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x})
= \mathbf{a}^{\mathsf{T}} \boldsymbol{\Phi} \boldsymbol{\phi} (\mathbf{x})
= \mathbf{k}(\mathbf{x})^{\mathsf{T}} (\mathbf{K} + \lambda \mathbf{I}_N)^{-1} \mathbf{t}
$$

where $\mathbf{k}(\mathbf{x})$ is a vector with elements $k\_n(\mathbf{x}) = k(\mathbf{x}, \mathbf{x}\_n)$.

> Singularity of $\mathbf{K}$ ??
> \
> Dual of dual??

With dual formulation, we can work directly with $k(\mathbf{x}, \mathbf{x}^\prime)$ without explicitly introuducing the feature vector $\boldsymbol{\phi}(\mathbf{x})$,
which enables the use of high-dimensional, even infinite-dimensional feature spaces.

## Constructing kernels

One method to construct the kernel function starts from choosing a feature vector $\boldsymbol{\phi}(\mathbf{x})$. The corresponding kernel function is then given by

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}^\prime)
$$

An alternative approach is to construct kernel functions directly. A kernel function $k(\mathbf{x}, \mathbf{x}^\prime)$ is valid if and only if the kernel matrix with elements given by $k(\mathbf{x}\_n, \mathbf{x}\_m)$ is postive semidefinite for all possible choices of $\{ \mathbf{x}\_n \}$.

Indeed, any matrix with the form $A^{\mathsf{T}} A$ is postive semidefinite.
Therefore, kernel functions of the form $k(\mathbf{x}, \mathbf{x}^\prime) = \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}^\prime)$ are always valid.


Given valid kernels $k\_1(\mathbf{x}, \mathbf{x}^\prime)$ and
$k\_2(\mathbf{x}, \mathbf{x}^\prime)$, the following kernels are also valid:

$$
\begin{align*}
k(\mathbf{x}, \mathbf{x}^\prime) &= c k_1(\mathbf{x}, \mathbf{x}^\prime) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= f(\mathbf{x}) k_1(\mathbf{x}, \mathbf{x}^\prime) f(\mathbf{x}^\prime) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= q\left( k_1(\mathbf{x}, \mathbf{x}^\prime) \right) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= \exp \left( k_1(\mathbf{x}, \mathbf{x}^\prime) \right) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= k_1(\mathbf{x}, \mathbf{x}^\prime) + k_2(\mathbf{x}, \mathbf{x}^\prime) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= k_1(\mathbf{x}, \mathbf{x}^\prime) k_2(\mathbf{x}, \mathbf{x}^\prime) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= k_3(\boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\phi}(\mathbf{x}^\prime)) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= \mathbf{x}^{\mathsf{T}} \mathbf{A} \mathbf{x}^\prime \\
k(\mathbf{x}, \mathbf{x}^\prime) &= k_a(\mathbf{x}_a, \mathbf{x}_a^\prime) + k_b(\mathbf{x}_b, \mathbf{x}_b^\prime) \\
k(\mathbf{x}, \mathbf{x}^\prime) &= k_a(\mathbf{x}_a, \mathbf{x}_a^\prime) k_b(\mathbf{x}_b, \mathbf{x}_b^\prime) \\
\end{align*}
$$

where $c > 0$ is a constant, $f(\cdot)$ is any function, $q(\cdot)$ is a polinomial with nonnegtive coefficients, $\boldsymbol{\phi}(\mathbf{x})$ is a function from $\mathbf{x}$ to
$\mathbb{R}^M$, $k\_3(\cdot)$ is a valid kernel in $\mathbb{R}\_M$, $\mathbf{A}$ is a symmetric postive semidefinite matrix, $\mathbf{x}\_a$ and $\mathbf{x}\_b$ are subsets of $\mathbf{x}$ such that $\mathbf{x} = (\mathbf{x}\_a, \mathbf{x}\_b)$, and $k\_a$ and $k\_b$ are corresponding valid kernels.

In practice, the construction of kernel also requires that it expresses the similarity between $\mathbf{x}$
and $\mathbf{x}^\prime$ for the intended application.
Several examples will be discussed below.

Let $\mathbf{x} = (x\_1, x\_2)^{\mathsf{T}}, \mathbf{z} = (z\_1, z\_2)^{\mathsf{T}}$. A simple kernel can be defined as

$$
\begin{align*}
k(\mathbf{x}, \mathbf{z}) &= (\mathbf{x}^{\mathsf{T}} \mathbf{z})^2 \\
&= (x_1^2 , \sqrt{2} x_1 x_2, x_2^2) (z_1^2 , \sqrt{2} z_1 z_2, z_2^2)^{\mathsf{T}} \\
&= \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{z}) \\
\end{align*}
$$

where the corresponding feature vector is given by $\boldsymbol{\phi} (\mathbf{x}) = (x\_1^2 , \sqrt{2} x\_1 x\_2, x\_2^2)$.
Though the feature space is three-dimensional, we should note that the coefficients of components of feature vector are constrained
and thus the effective dimensionality of the feature space is still limited.

A commonly used kernel is given by

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \exp \left( - \frac{\| \mathbf{x} - \mathbf{x}^\prime\|^2}{2 \sigma^2} \right)
$$

which is often called a Gaussian kernel. By writing in another form

$$
k(\mathbf{x}, \mathbf{x}^\prime)
= \exp \left( - \frac{\mathbf{x}^{\mathsf{T}} \mathbf{x}}{2 \sigma^2} \right)
\exp \left( \frac{\mathbf{x}^{\mathsf{T}} \mathbf{x}^\prime}{\sigma^2} \right)
\exp \left( - \frac{(\mathbf{x}^\prime)^{\mathsf{T}} \mathbf{x}^\prime}{2 \sigma^2} \right)
$$

we see it is a valid kernel.

Expanding the middle term as serise, we have

$$
\begin{align*}
k(\mathbf{x}, \mathbf{x}^\prime)
&= \exp \left( - \frac{\mathbf{x}^{\mathsf{T}} \mathbf{x}}{2 \sigma^2} \right)
\exp \left( - \frac{(\mathbf{x}^\prime)^{\mathsf{T}} \mathbf{x}^\prime}{2 \sigma^2} \right)
\sum_{i=0}^{\infty} \frac{1}{i!} \left( \frac{\mathbf{x}^{\mathsf{T}} \mathbf{x}^\prime}{\sigma^2} \right)^i
\end{align*}
$$

which can then be expressed as the inner product of two infinite-dimensionality feature vectors.

Gaussian kernel can be expanded beyond Euclidian distance by using a nonlinear kernel $\widetilde{k} (\mathbf{x}, \mathbf{x}^\prime)$ so that

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \exp \left\{ - \frac{1}{2 \sigma^2} 
\left[ \widetilde{k}(\mathbf{x}, \mathbf{x}) + \widetilde{k}(\mathbf{x}^\prime, \mathbf{x}^\prime) - 2 \widetilde{k}(\mathbf{x}, \mathbf{x}^\prime) \right]
\right \}
$$

Kernel methods also extends the input beyond vectors of real numbers to other objects such as
graphs, sets, strings, documents.
For instance, given finite sets $A\_1$ and $A\_2$, their similarity can be expressed by the kernel function

$$
k(A_1, A_2) = 2^{|A_1 \cap A_2|}
$$

Another particular case of the direct method starts from a generative model, and then applies kernels
in a discriminative manner.

Given a generative model $p(\mathbf{x})$, a kernel is defined by

$$
k(\mathbf{x}, \mathbf{x}^\prime) = p(\mathbf{x}) p(\mathbf{x}^\prime)
$$

where the inputs $\mathbf{x}$ and $\mathbf{x}^\prime$ are defined to be similar if they both have high probabilities.
It can be extended by introducing a latent variable so that

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \sum_i p(\mathbf{x}|i) p(\mathbf{x}^\prime|i) p(i)
$$

If the latent variable is continuous

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \int p(\mathbf{x}|\mathbf{z}) p(\mathbf{x}^\prime|\mathbf{z}) p(\mathbf{z}) \,d\mathbf{z}
$$

An alternative technique to define kernels from a generative model is the **Fisher kernel**.
Consider a generative model $p(\mathbf{x}\|\boldsymbol{\theta})$ governed by prameters $\boldsymbol{\theta}$.
As a particular case, the **Fisher score** is defined by

$$
g(\boldsymbol{\theta}, \mathbf{x}) = \nabla_{\boldsymbol{\theta}} \ln p(\mathbf{x}|\boldsymbol{\theta})
$$

by which the Fisher kernel is defined

$$
k(\mathbf{x}, \mathbf{x}^\prime) = g(\boldsymbol{\theta}, \mathbf{x})^{\mathsf{T}} \mathbf{F}^{-1} g(\boldsymbol{\theta}, \mathbf{x}^\prime)
$$

where $\mathbf{F}$ is the **Fisher information matrix** given by

$$
\mathbf{F} = \operatorname{E}_{\mathbf{x}|\boldsymbol{\theta}} \left[ g(\boldsymbol{\theta}, \mathbf{x}) g(\boldsymbol{\theta}, \mathbf{x})^{\mathsf{T}} \right]
$$

The Fisher information matrix keeps invariant of the kernel under reparameterization????
TODO

## Radial basis function networks

This section introduces a class of basis functions, namely the **radial basis functions**.
It has the property that each basis function depends on the input $\mathbf{x}$ only through the radial distance from a center
$\boldsymbol{\mu}\_j$, so that $\phi\_j(\mathbf{x}) = h(\\| \mathbf{x} - \boldsymbol{\mu}\_j \\|)$.

Radial basis function can be applied for exact function interpolation. Given a set of N data points $\{ (\mathbf{x}\_n, t\_n) \}$.
The interpolation function is given

$$
f(\mathbf{x}) = \sum_{n=1}^N w_n  h(\| \mathbf{x} - \mathbf{x}_n \|)
$$

Since the number of coeficcients $w\_n$ is equal to the number of points, the final result will fits every data point.
However, if the observations are noisy, the solution will be over-fitting.

Regularization theory ??

Consider a interpolation problem where the input is added to a noise denoted
by $\boldsymbol{\xi}$ and the noise follows the distribution $\nu(\boldsymbol{\xi})$.
The sum-of-squares error is then given

$$
E = \frac{1}{2} \sum_{n=1}^N \int \left[ y(\mathbf{x}_n + \boldsymbol{\xi}) - t_n \right]^2 \nu(\boldsymbol{\xi}) \,d\boldsymbol{\xi}
$$

**Excercise 6.17**

Changing variables, the error function is written

$$
E = \frac{1}{2} \sum_{n=1}^N \int \left[ y(\boldsymbol{\xi}) - t_n \right]^2 \nu(\boldsymbol{\xi} - \mathbf{x}_n) \,d\boldsymbol{\xi}
$$

Using variational calculus to optimize with respect to $y$, we have

$$
\frac{ \partial E}{\partial y}
= \int \sum_n \left( y(\boldsymbol{\xi}) - t_n \right) \nu(\boldsymbol{\xi} - \mathbf{x}_n) \,d\boldsymbol{\xi} = 0
$$

The solution can be taken as

$$
y(\mathbf{x}) = \sum_n t_n h(\mathbf{x} - \mathbf{x}_n)
$$

where the basis functions are defined

$$
h(\mathbf{x} - \mathbf{x}_n) = \frac{\nu(\mathbf{x} - \mathbf{x}_n)}{\sum_{i=1}^N \nu(\mathbf{x}- \mathbf{x}_i)}
\tag{6.41}
$$

This is known as the **Nadaraya-Watson** model. If the noise is isotropic, then $\nu(\boldsymbol{\xi})$ depends only on $\\| \boldsymbol{\xi} \\|$ and the basis functions are radial.

Note that, the basis functions (6.41) is normalized since $\sum\_n h(\mathbf{x}-\mathbf{x}\_n) = 1$.
Normalized basis functions have an advantage that they avoid regions where all basis functions take small values.

Because it is computationally costly to have one basis function for each data points,
we may want the number of basis functions to be smaller than $N$. Typically,
the number of basis functions and their respective centers $\boldsymbol{\mu}\_j$ should be determined by the data set alone.
A simple method is to randomly choose a subset of data points as the centers of basis functions.
More advanced methods such as the **orthogonal least squares** or clutering algorithms can also be used.

### Nadaraya-Watson model

This section derives the Nadaraya-Watson model from an alternative view. 

Suppose we have a data set $\{ \mathbf{x}\_n, t\_n \}$ for regression. 

First we use a Parzen density estimator to model the joint disitrbution $p(\mathbf{x}, t)$, so that

$$
p(\mathbf{x}, t) = \frac{1}{N} \sum_{n=1}^N f(\mathbf{x} - \mathbf{x}_n, t - t_n)
$$

where $f(\mathbf{x}, t)$ is the component density function. 

> How is $f$ determined??

With the squared loss, the regression function is given by the conditional mean, so that

$$
\begin{align*}
y(\mathbf{x}) &= \operatorname{E} [t|\mathbf{x}] = \int_{-\infty}^{\infty} t p(t|\mathbf{x}) \,dt \\
&= \frac{\int t p(\mathbf{x}, t) \,dt}{\int p(\mathbf{x}, t) \,dt} \\
&= \frac{\sum_n \int t f(\mathbf{x} - \mathbf{x}_n, t - t_n) \,dt}{\sum_n \int f(\mathbf{x} - \mathbf{x}_n, t - t_n) \,dt}
\end{align*}
$$

Assume that the component density have zero mean for any value of $\mathbf{x}$ so that

$$
\int t f(\mathbf{x}, t) \,dt = 0
$$

Changing variable with $t \rightarrow t - t\_n$, we obtain

$$
\begin{align*}
\int t f(\mathbf{x}, t) \,dt = \int (t - t_n) f(\mathbf{x}, t - t_n) \,dt &= 0 \\
\int t f(\mathbf{x}, t - t_n) \,dt &= t_n \int f(\mathbf{x}, t - t_n) \,dt = t_n \int f(\mathbf{x}, t) \,dt \\
\end{align*}
$$

Defining

$$
g(\mathbf{x}) = \int_{-\infty}^{\infty} f(\mathbf{x}, t) \,dt
$$

we have

$$
\begin{align*}
y(\mathbf{x}) &= \frac{\sum_n t_n g(\mathbf{x} - \mathbf{x}_n)}{\sum_m g(\mathbf{x} - \mathbf{x}_m)} \\
&= \sum_n k(\mathbf{x}, \mathbf{x}_n) t_n
\end{align*}
$$

The function $k$ is given by

$$
k(\mathbf{x}, \mathbf{x}_n) = \frac{g(\mathbf{x} - \mathbf{x}_n)}{\sum_m g(\mathbf{x} - \mathbf{x}_m)}
$$

and satisfies

$$
\sum_n k(\mathbf{x}, \mathbf{x}_n) = 1
$$

This is known as the **Nadaraya-Watson model** or the **kernel regression**.

Note that, the conditional distribution can also be obtained from this model

$$
p(t|\mathbf{x}) = \frac{p(t|\mathbf{x})}{\int p (t, \mathbf{x}) \,dt}
= \frac{\sum_n f(\mathbf{x} - \mathbf{x}_n, t - t_n)}{\sum_m \int f(\mathbf{x} - \mathbf{x}_m, t - t_m) \,dt}
\tag{6.48}
$$

**Excercise 6.18**

As an example, consider a single input variable $x$ and target variable $t$, with the joint distribution given by an isotropic Gassian

$$
f(x, t) = \mathcal{N} (x|0, \sigma^2) \mathcal{N}(t|0, \sigma^2)
$$

then we have

$$
f(x - x_n, t-t_n) = \mathcal{N} (x|x_n, \sigma^2) \mathcal{N}(t|t_n, \sigma^2)
$$

Using (6.48), the target distribution is given by

$$
\begin{align*}
p(t|x) &= \frac{\sum_n \mathcal{N} (x|x_n, \sigma^2) \mathcal{N}(t|t_n, \sigma^2)}{\sum_m \mathcal{N} (x|x_m, \sigma^2)} \\
&= \sum_n k(x, x_n) \mathcal{N}(t|t_n, \sigma^2) \\
\end{align*}
$$

where we defined

$$
k(x, x_n) = \frac{\mathcal{N} (x|x_n, \sigma^2) }{\sum_m \mathcal{N} (x|x_m, \sigma^2)}
$$

From the target distribution, we obtain

$$
\begin{align*}
\operatorname{E} [t|x] &= \sum_n k(x, x_n) t_n \\
\operatorname{var} [t|x] &= \sigma^2 + \sum_n k(x, x_n) t_n^2 - \left( \sum_n k(x, x_n) t_n \right)^2 \\
\end{align*}
$$

## Gaussian process

### Linear regression revisited

Consider a linear model with $M$ basis functions given by

$$
y(\mathbf{x}) = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x})
$$

The prior of $\mathbf{w}$ is defined as

$$
p(\mathbf{w}) = \mathcal{N} ( \mathbf{w} | \mathbf{0}, \alpha^{-1}\mathbf{I})
$$

Given a set of observations $\mathbf{x}\_1, \dots, \mathbf{x}\_N$. By defining a vector

$$
\mathbf{y} = \left( y(\mathbf{x}_1), \dots, y(\mathbf{x}_N) \right)^{\mathsf{T}}
$$

we have

$$
\mathbf{y} = \boldsymbol{\Phi} \mathbf{w}
$$

where $\boldsymbol{\Phi}$ is the design matrix.

Since each component of $\mathbf{y}$ is a linear combination of components of $\mathbf{w}$, 
$\mathbf{y}$ is Gaussian distributed even if the distribution of $\mathbf{w}$ is non-diagonal.
The mean and covariance of $\mathbf{y}$ are then given

$$
\begin{align*}
\operatorname{E} [\mathbf{y}] &= \boldsymbol{\Phi} \operatorname{E} [\mathbf{w}]  = \mathbf{0} \\
\operatorname{cov} [\mathbf{y}] &= \operatorname{E} [\mathbf{y} \mathbf{y}^{\mathsf{T}}]
= \boldsymbol{\Phi} \operatorname{E} [\mathbf{w} \mathbf{w}^{\mathsf{T}}] \boldsymbol{\Phi}^{\mathsf{T}}
= \frac{1}{\alpha} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}} = \mathbf{K}
\end{align*}
$$

where $\mathbf{K}$ is Gram matrix with elements

$$
K_{nm} = k(\mathbf{x}_n, \mathbf{x}_m) = \frac{1}{\alpha} \boldsymbol{\Phi} \boldsymbol{\Phi}^{\mathsf{T}}
$$

Note that the distribution of $\mathbf{y}$ can be ill-formed if $\mathbf{K}$ is not postive definite.

This is an example of the Gaussian process. The key notion of the Gaussian process is
to model the joint distirbution over $y(\mathbf{x}\_1), \dots, y(\mathbf{x}\_N)$ as a Gaussian distribution.
In the case of two-deimensional input $\mathbf{x}$, it is called a **Gaussian random field**.

The choice of zero mean is equivalent to choosing the prior $p(\mathbf{w}\|\alpha)$ with zero mean.

More generally, a **stochastic process** $y(\mathbf{x})$ is constructed by defining a joint distribution on the set $\{ y(\mathbf{x}\_n) \}$

### Gaussian processes for regression

Consider a regression problem with targets given by

$$
t_n = y_n + \epsilon_n
$$

where $y\_n \equiv y(\mathbf{x}\_n)$, and $\epsilon\_n$ is a noise term.

Suppose the noise follows a Gaussian distribution with precision $\beta$. We have

$$
p(t_n|y_n) = \mathcal{N} (t_n|y_n, \beta^{-1})
$$

Defining vectors

$$
\begin{align*}
\mathbf{t} &= (t_1, \dots, t_N)^{\mathsf{T}} \\
\mathbf{y} &= (y_1, \dots, y_N)^{\mathsf{T}} \\
\end{align*}
$$

then the joint distribution

$$
p(\mathbf{t}|\mathbf{y})  = \mathcal{N} (\mathbf{t}|\mathbf{y}, \beta^{-1}\mathbf{I}_N)
$$

With a Gaussian process, the distribution $p(\mathbf{y})$ is given

$$
p(\mathbf{y}) = \mathcal{N} (\mathbf{y}|\mathbf{0}, \mathbf{K})
$$

where the kernel function of $\mathbf{K}$ is chosen so that $y(\mathbf{x}\_n)$ and $y(\mathbf{x}\_m)$ are strongly correlated if $\mathbf{x}\_n$ and $\mathbf{x}\_m$ are similar.

Using the results of Section 3.3.3, we marginalize out $\mathbf{y}$ to obtain

$$
p(\mathbf{t}) = \mathcal{N} (\mathbf{t}|\mathbf{0}, \mathbf{C})
\tag{6.61}
$$

where

$$
\mathbf{C} = \mathbf{K} + \beta^{-1} \mathbf{I}
$$

A widely used kernel function is given by

$$
k(\mathbf{x}_n, \mathbf{x}_m) = \theta_0 \exp \left( - \frac{\theta_1}{2} \| \mathbf{x}_n - \mathbf{x}_m \|^2 \right) + \theta_2 + \theta_3 \mathbf{x}_n^{\mathsf{T}} \mathbf{x}_m
\tag{6.63}
$$

which involves the exponential of a quadratic form, a constant and a linear term.

To make predictions for new inputs, consider a new input vector given by $\mathbf{x}\_{N+1}$.
By augmenting the joint distribution of target values $\mathbf{t}$, we have

$$
p(\mathbf{t}_{N+1}) = \mathcal{N} (\mathbf{t}_{N+1}|\mathbf{0}, \mathbf{C}_{N+1})
$$

where the covariance matrix can be written in the partioned form

$$
\mathbf{C}_{N+1} = \begin{pmatrix}
\mathbf{C}_N & \mathbf{k} \\
\mathbf{k} & c \\
\end{pmatrix}
$$

The vector $\mathbf{k}$ is defined by $k\_n = k(\mathbf{x}\_n, \mathbf{x}\_{N+1})$ where $n=1,\dots, N$. And $c = k(\mathbf{x}\_{N+1}, \mathbf{x}\_{N+1}) + \beta^{-1}$.

Using the results in Section 2.3.1, we obtain the predictive distribution

$$
p(t_{N+1}|\mathbf{t}_N) = \mathcal{N} (t_{N+1} | m(\mathbf{x}_{N+1}), \sigma^2(\mathbf{x}_{N+1}) )
$$

where both the mean and the covariance are functions of the new input vector

$$
\begin{align*}
m(\mathbf{x}_{N+1}) &= \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{t}_N \\
\sigma^2(\mathbf{x}_{N+1}) &= c - \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{k} \\
\end{align*}
$$

Note that the restriction to the kernel function is that $\mathbf{K}$ should be postive semidefinite. Then the matrix $\mathbf{C}$ will be postive definite due to $\beta > 0$.

**Excercise 6.21**

We shall see that when the kernel function is defined by a finite set of basis functions,
through a Gaussian process we obtain the same result as the predictive distribution defined by (3.58).

Suppose the kernel function $k(\mathbf{x}, \mathbf{x}^\prime)$ is given by

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \alpha^{-1} \boldsymbol{\phi} (\mathbf{x}) ^{\mathsf{T}}
\boldsymbol{\phi} (\mathbf{x}^\prime)
$$

we have

$$
\begin{align*}
\mathbf{C}_N &= \mathbf{K} + \beta^{-1} \mathbf{I}
= \alpha^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} + \beta^{-1} \mathbf{I} \\
\mathbf{k} &= \alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\phi} (\mathbf{x})
\end{align*}
$$

Here we denote the new input variable by $\mathbf{x}$ instead of $\mathbf{x}\_{N+1}$ to keep
notations uncluttered.

Using Woodbury identity on $\mathbf{C}\_N$, we have

$$
\begin{align*}
\mathbf{C}_N^{-1} &= \beta \mathbf{I}
- \beta^2 \boldsymbol{\Phi} (\alpha \mathbf{I} + \beta \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \\
&= \beta \mathbf{I} - \beta^2 \boldsymbol{\Phi} \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf{T}}
\end{align*}
$$

where $\mathbf{S}\_N$ is defined by (3.54).

Then the mean can written as 

$$
\begin{align*}
m(\mathbf{x}) &= \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{t} \\
&= \alpha^{-1} \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \boldsymbol{\Phi}^{\mathsf{T}}
(\beta \mathbf{I} - \beta^2 \boldsymbol{\Phi} \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf{T}}) \mathbf{t} \\
&= \alpha^{-1} \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}}
(\beta \boldsymbol{\Phi}^{\mathsf{T}} - \beta^2 \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf{T}}) \mathbf{t} \\
&= \alpha^{-1} \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}}
\left( \beta \boldsymbol{\Phi}^{\mathsf{T}} - \beta (\mathbf{S}_N^{-1} - \alpha \mathbf{I}) \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf{T}} \right) \mathbf{t} \\
&= \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{m}_N
\end{align*}
$$

Similarly, we can verity

$$
\sigma^2(\mathbf{x}) =  c - \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{k} 
= \beta^{-1} + \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x})
$$

The computation of Gaussian process involves the inversion of an $N\times N$ matrix, which costs $O(N^3)$, and each prediction costs $O(N^2)$.
By contrast, the basis function methods costs $O(M^3)$ to invert $\mathbf{S}\_N$ and $O(M^2)$ to 
make a prediction.
For large data sets, it is more efficient to use the basis function methods. 
However, the Gaussian process has the potential to introduce infinite number of basis functions.

For large data sets, approximation methods for Gaussian process have been developed.

**Excercise 6.23**

The extension to multivariate target variables is known as co-kriging.

TODO

### Learning the hyperparameters

Hyperparameters are optimized by maximizing the likelihood function $p(\mathbf{t}\|\boldsymbol{\theta})$ with respect to hyperparameter $\boldsymbol{\theta}$,
where gradient-based algorithms can be used.

Based on (6.61), the log likelihood is given by

$$
\ln p(\mathbf{t}|\boldsymbol{\theta}) = - \frac{1}{2} \ln |\mathbf{C}_N|
- \frac{1}{2} \mathbf{t}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{t}
- \frac{N}{2} \ln 2\pi
$$

Taking derivative with respect to $\theta\_i$ gives

$$
\frac{\partial}{\partial \theta_i} \ln p(\mathbf{t}|\boldsymbol{\theta})
= - \frac{1}{2} \operatorname{Tr} \left( \mathbf{C}_N^{-1} \frac{\partial \mathbf{C}_N}{\partial \theta_i} \right)
+ \frac{1}{2} \mathbf{t}^{\mathsf{T}} \mathbf{C}_N^{-1} \frac{\partial \mathbf{C}_N}{\partial \theta_i} \mathbf{C}_N^{-1} \mathbf{t}
$$

Note that the function $\ln p(\mathbf{t}\|\boldsymbol{\theta})$ is not necessarily convex, so 
it's possible to have multiple maxima.

If a prior on $\boldsymbol{\theta}$ is introduced, we then maximize the posterior

$$
p(\boldsymbol{\theta}|\mathbf{t}) \propto p(\mathbf{t}|\boldsymbol{\theta}) p(\boldsymbol{\theta})
$$

For **heteroscedastic** problems, in which the noise iteself may depend on $\mathbf{x}$,
we can model $\beta(\mathbf{x})$ by introducing a second Gaussian process.

### Automatic relevance determination

In the context of Gaussian process, **automatic relevance determination** (or ARD) associates each input variable $x\_i$ with an adaptive parameter.
Optimization of these parameters then infers the relative importance of input variables with respect to the data.
This technique is originally formulated in neural networks.

Consider a Gaussian process with two-dimensional input $\mathbf{x} = (x\_1, x\_2)$. A
kernel ready for ARD takes the form

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \theta_0 
\exp \left\{ - \frac{1}{2} \sum_{i=1}^2 \eta_i (x_i - x_i^\prime)^2 \right\}
$$

When $\eta\_i$ is small, the function is relatively incensitive to changes of the corresponding input variable $x\_i$.
So by optimizing $\{ \eta\_i \}$, it is possible to detect irrelevent input variables.

Adapt the ARD into (6.63), we obtain a kernel function

$$
k(\mathbf{x}_n, \mathbf{x}_m) = \theta_0 \exp \left( - \frac{1}{2} \sum_{i=1}^D \eta_i \| \mathbf{x}_{ni} - \mathbf{x}_{mi} \|^2 \right) + \theta_2 + \theta_3 \mathbf{x}_n^{\mathsf{T}} \mathbf{x}_m
\tag{6.63}
$$

> No seperate parameters for linear term???

### Gaussian process for classification

In classification problems, we want to model the poterior $p(C\_k\|\mathbf{x})$.
With an appropriete activation function, we can transform the output of the Gaussian process into the valid range $[0, 1]$.

Consider a binary classification problem, with target value $t \in \{0, 1\}$.
We first perform a Gaussian process over a function $a(\mathbf{x})$, and then transform it into the target using a logstic sigmoid function, so that $y = \sigma(a)$.
This is an example of non-Gaussian stochastic process.

Let $\{ \mathbf{x}\_1, \dots, \mathbf{x}\_N \}$ be the set of observations,
and corresponding target values be $\mathbf{t}\_N = (t\_1, \dots, t\_N)^{\mathsf{T}}$.
Define a new input vector $\mathbf{x}\_{N+1}$ and its corresponding target $t\_{N+1}$.
To make predictions for new data, we need to find $p(t\_{N+1}\|\mathbf{t}\_N)$.

First we introduce a Gaussian process on $\mathbf{a}\_{N+1} = (a(\mathbf{x}\_1), \dots, a(\mathbf{x}\_{N+1}))^{\mathsf{T}}$

$$
p(\mathbf{a}_{N+1}) = \mathcal{N} (\mathbf{a}_{N+1} | \mathbf{0}, \mathbf{C}_{N+1})
$$

where the elements of $\mathbf{C}\_{N+1}$ is given by

$$
\mathbf{C}_{N+1} = \mathbf{K} + \nu \mathbf{I}
$$

where $\mathbf{K}$ is the Gram matrix defined by a valid kernel function $k(\mathbf{x}, \mathbf{x}^\prime)$ governed by parameter $\boldsymbol{\theta}$.
$\nu$ is a positive constant introduced to ensure the covariance matrix postive definite.

Note that, however, on the assumption that all training data are correctly labeled, the noise-like term $\nu$ is no longer requried.

The predictive distribution is given by

$$
\begin{align*}
p(t_{N+1}=1 | \mathbf{t}_N) &= \int p(t_{N+1}=1 | a_{N+1}) p(a_{N+1}|\mathbf{t}_N) \,d a_{N+1} \\
p(t_{N+1}=0 | \mathbf{t}_N) &= 1 - p(t_{N+1}=1 | \mathbf{t}_N) \\
\end{align*}
$$

where $p(t\_{N+1}=1 \| a\_{N+1}) = \sigma(a\_{N+1})$.

This integral is analytically intractable and some approximation method is required.
The sampling methods will be introduced in later chapters.
Here we will use the analytical approximation presented in Section 4.5.2.

Note that, the key idea of approximating the posterior with a Gaussian is a consequence of central limit theorem.
In the context of Gaussian process, the dimensionality of the variable grows with the number of data points, so the Gaussian appximation does not apply well.
However, if we increase the number of data points in a bounded region of $\mathbf{x}$ space,
then uncertainty in $a(\mathbf{x})$ in the region should decrease, which will asymptotically become a Gaussian.

Three approaches to obtain a Gaussian approximation will be considered:

- Variational inference described in chapter 10
- Expectation propagation

### Laplace appximation

The third approach is the Laplace approximation we will show here.

To approximate $p(a\_{N+1}\|\mathbf{t}\_N)$, we first show that

$$
\begin{align*}
p(a_{N+1}|\mathbf{t}_N) &= \int p(a_{N+1}, \mathbf{a}_N | \mathbf{t}_N) \,d \mathbf{a}_N \\
&= \frac{1}{p(\mathbf{t}_N)} \int p(a_{N+1}, \mathbf{a}_N) p(\mathbf{t}_N | a_{N+1}, \mathbf{a}_N ) \,d \mathbf{a}_N \\
&= \frac{1}{p(\mathbf{t}_N)} \int p(a_{N+1} | \mathbf{a}_N) p(\mathbf{a}_N) p(\mathbf{t}_N | \mathbf{a}_N ) \,d \mathbf{a}_N \\
&= \int p(a_{N+1} | \mathbf{a}_N) p(\mathbf{a}_N | \mathbf{t}_N) \,d \mathbf{a}_N \tag{6.77} \\
\end{align*}
$$

where we have used $p(\mathbf{t}\_N \| a\_{N+1}, \mathbf{a}\_N ) = p(\mathbf{t}\_N \| \mathbf{a}\_N )$.

The conditional distribution $p(a\_{N+1} \| \mathbf{a}\_N)$ is derived in Section 6.4.2, which
is given by

$$
p(a_{N+1} | \mathbf{a}_N ) = \mathcal{N}
(a_{N+1} | \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{t}_N, c - \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{k})
$$

To evaluate the integral (6.77), we want to find an Gaussian approximation for $p(\mathbf{a}\_N \| \mathbf{t}\_N)$, before which we find the mode.

By Bayes' theorem the log posterior gives

$$
\ln p(\mathbf{a}_N | \mathbf{t}_N) = \ln p(\mathbf{a}_N) + \ln p(\mathbf{t}_N | \mathbf{a}_N)
+ \textrm{const}
$$

where 'const' denote terms independent of $\mathbf{a}\_N$.

Given the likelihood

$$
p(\mathbf{t}_N|\mathbf{a}_N) = \prod_{n=1}^N \sigma(a_n)^{t_n} (1- \sigma(a_n))^{1- t_n}
= \prod_n \exp (a_n t_n) \sigma(-a_n)
$$

we can write

$$
\begin{align*}
\Psi (\mathbf{a}_N) &= \ln p(\mathbf{a}_N) + \ln p(\mathbf{t}_N | \mathbf{a}_N) \\
&= -\frac{1}{2} \mathbf{a}_N^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{a}_N
- \frac{N}{2} \ln 2 \pi - \frac{1}{2} \ln |\mathbf{C}_N| \\
&\quad + \mathbf{t}^{\mathsf{T}} \mathbf{a}_N - \sum_n \ln (1 + \exp (a_n)) \\
\end{align*}
$$

Taking derivative of the poterior is then equivalent to taking derivative of $\Psi (\mathbf{a}\_N)$. The gradient

$$
\nabla \Psi (\mathbf{a}_N) = \mathbf{t}_N - \boldsymbol{\sigma}_N - \mathbf{C}_N^{-1} \mathbf{a}_N
\tag{6.81}
$$

where $\boldsymbol{\sigma}\_N = (\sigma(a\_1), \dots, \sigma(a\_N))^{\mathsf{T}}$.
The mode is solved with an optimization algorithm.

Taking the second derivative of $\Phi(\mathbf{a}\_N)$ gives

$$
\nabla \nabla \Psi (\mathbf{a}_N) = - \mathbf{W}_N - \mathbf{C}_N^{-1}
$$

where $\mathbf{W}\_N$ is a diagonal matrix with elements $W\_{nn} = \sigma(a\_n) (1-\sigma(a\_n))$.
Since $W\_{nn} > 0$, $\mathbf{W}\_N$ is postive definite.
Since $\mathbf{C}\_N$ is also constructed to be postive definite, $\mathbf{C}\_N^{-1}$ and $\mathbf{W}\_N + \mathbf{C}\_N^{-1}$ is postive definite.
Therefore the log posterior $\ln p(\mathbf{a}\_N \| \mathbf{t}\_N)$ is strictly concave and have a single unique mode.

We shall use the Newton-Raphson method to calculate the mode. The iteration is given by

$$
\mathbf{a}_N^{\mathrm{new}} = \mathbf{C}_N (\mathbf{I} + \mathbf{W}_N \mathbf{C}_N)^{-1}
\left( \mathbf{t}_N - \boldsymbol{\sigma}_N + \mathbf{W}_N \mathbf{a}_N \right)
$$

Denoting the mode by $\mathbf{a}\_N^\ast$, the gradient takes zero at $\mathbf{a}\_N^\ast$. From (6.81) we have

$$
\mathbf{a}_N^\ast = \mathbf{C}_N (\mathbf{t}_N - \boldsymbol{\sigma}_N)
\tag{6.84}
$$

The Gaussian approximation to $p(\mathbf{a}\_N \| \mathbf{t}\_N)$ is thus given by

$$
q(\mathbf{a}_N) = \mathcal{N} (\mathbf{a}_N | \mathbf{a}_N^\ast, \mathbf{H}^{-1})
$$

where

$$
\mathbf{H} = - \nabla \nabla \Psi (\mathbf{a}_N^\ast) = \mathbf{W}_N + \mathbf{C}_N^{-1}
$$

Now we can evaluate (6.77) using results in Section 2.3.3, with following change of variables

$$
\begin{align*}
\mathbf{x} &\rightarrow \mathbf{a}_N,
&\boldsymbol{\mu} &\rightarrow \mathbf{a}_N^\ast, 
&\boldsymbol{\Lambda}^{-1} &\rightarrow \mathbf{H}^{-1} \\
\mathbf{y} &\rightarrow a_{N+1},
& \mathbf{A} &\rightarrow \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1},
& \mathbf{b} &\rightarrow \mathbf{0}, 
& \mathbf{L}^{-1} &\rightarrow c - \mathbf{k}^{\mathsf{T}} \mathbf{C}_N^{-1} \mathbf{k}\\
\end{align*}
$$

which gives the a Gaussian distribution with the mean and variance given by

$$
\begin{align*}
\operatorname{E} [a_{N+1}|\mathbf{t}_N] &= \mathbf{k}^{\mathsf{T}} (\mathbf{t}_N - \boldsymbol{\sigma}_N) \\
\operatorname{var} [a_{N+1}|\mathbf{t}_N] &= c - \mathbf{k}^{\mathsf{T}} (\mathbf{W}_N^{-1} + \mathbf{C}_N)^{-1} \mathbf{k} \\
\end{align*}
$$

As with the Bayesian logistic regression in Section 4.5, if the decision boundary is chosen corresponding to $p(t\_{N+1}\|\mathbf{t}\_N) = 0.5$, the variance can be ignored.

To determine the parameter $\boldsymbol{\theta}$ in the covariance (kernel) function,
one approach is to maximize the likelihood function $p(\mathbf{t}\_N\|\boldsymbol{\theta})$.
If required, a prior can be added.
The likelihood function is given by

$$
p(\mathbf{t}_N|\boldsymbol{\theta}) = \int p(\mathbf{t}_N|\mathbf{a}_N) p(\mathbf{a}_N|\boldsymbol{\theta}) \,d \mathbf{a}_N
$$

The integral is analytically intractable, we could use Laplace approximation again. Using the result (4.135) we have
$$
\begin{align*}
\ln p(\mathbf{t}_N|\boldsymbol{\theta})
&\simeq \ln p (\mathbf{t}_N|\mathbf{a}_N^\ast) + \ln p(\mathbf{a}_N^\ast | \boldsymbol{\theta})
+ \frac{N}{2} \ln 2 \pi - \frac{1}{2} \ln | \mathbf{H} | \\
&= \Psi (\mathbf{a}_N^\ast) - \frac{1}{2} \ln | \mathbf{W}_N + \mathbf{C}_N^{-1} |
+ \frac{N}{2} \ln 2 \pi
\tag{6.90}
\end{align*}
$$

Variations in $\boldsymbol{\theta}$ will cause variations in $\mathbf{C}\_N$ as well as $\mathbf{a}\_N^\ast$,
so the derivative with respect to $\boldsymbol{\theta}$ can be written in the form

$$
\frac{\partial \ln p(\mathbf{t}_N|\boldsymbol{\theta})}{\partial \theta_j}
= \frac{\partial \ln p(\mathbf{t}_N|\boldsymbol{\theta})}{\partial \mathbf{C}_N}
\frac{\partial \mathbf{C}_N}{\partial \theta_j}
+ \frac{\partial \ln p(\mathbf{t}_N|\boldsymbol{\theta})}{\partial \mathbf{a}_N^\ast}
\frac{\partial \mathbf{a}_N^\ast}{\partial \theta_j}
$$

The first term on the r.h.s gives

$$
\begin{align*}
\frac{\partial \ln p(\mathbf{t}_N|\boldsymbol{\theta})}{\partial \mathbf{C}_N}
\frac{\partial \mathbf{C}_N}{\partial \theta_j}
&= \frac{1}{2} \mathbf{a}_N^{\ast \mathsf{T}} \mathbf{C}_N^{-1} \frac{\partial \mathbf{C}_N}{\partial \theta_j} \mathbf{C}_N^{-1} \mathbf{a}_N^\ast \\
&\quad - \frac{1}{2} \operatorname{Tr} \left\{ (\mathbf{I} + \mathbf{C}_N \mathbf{W}_N)^{-1}
\mathbf{W}_N \frac{\partial \mathbf{C}_N}{\partial \theta_j} \right\}
\end{align*}
$$

The second term is the derivative with respect to $\mathbf{a}\_N^\ast$. Since
$\nabla \Psi (\mathbf{a}\_N^\ast) = \mathbf{0}$, the derivative of the first term in (6.90)
will vanish. Thus we have

$$
\begin{align*}
\partial \ln p(\mathbf{t}_N|\boldsymbol{\theta})
&= - \frac{1}{2} \partial \ln | \mathbf{W}_N + \mathbf{C}_N^{-1} | \\
&= \operatorname{Tr} \left\{ (\mathbf{W}_N + \mathbf{C}_N^{-1})^{-1} \partial (\mathbf{W}_N + \mathbf{C}_N^{-1}) \right\} \\
&= \operatorname{Tr} \left\{ (\mathbf{I} + \mathbf{C}_N \mathbf{W}_N)^{-1}
\mathbf{C}_N \partial (\mathbf{W}_N) \right\} \\
&= \operatorname{Tr} \left\{ (\mathbf{I} + \mathbf{C}_N \mathbf{W}_N)^{-1} \mathbf{C}_N 
\operatorname{diag} \left[ \sigma_n (1 - \sigma_n) (1 - 2 \sigma_n) \partial a_n^\ast \right]
\right\} \\
&= \sum_n \left[ (\mathbf{I} + \mathbf{C}_N \mathbf{W}_N)^{-1} \mathbf{C}_N \right]_{nn} 
\sigma_n (1 - \sigma_n) (1 - 2 \sigma_n) \partial a_n^\ast \\
\end{align*}
$$

where $a\_n^\ast$ is the n-th component of $\mathbf{a}\_N^\ast$ and we defined $\sigma\_n = \sigma (a\_n^\ast)$. Then we obtain the derivative

$$
\frac{\partial \ln p(\mathbf{t}_N|\boldsymbol{\theta})}{\partial \mathbf{a}_N^\ast}
\frac{\partial \mathbf{a}_N^\ast}{\partial \theta_j}
= \sum_n \left[ (\mathbf{I} + \mathbf{C}_N \mathbf{W}_N)^{-1} \mathbf{C}_N \right]_{nn} 
\sigma_n (1 - \sigma_n) (1 - 2 \sigma_n) \frac{\partial a_n^\ast }{\partial \theta_j}
$$

Evaluation of $\partial \mathbf{C}\_N / \partial \theta\_j$ involves evaluation of derivatives of the kernel function, which is straightforward.

The expression for $\partial a\_n^\ast / \partial \theta\_j$ can be obtained from (6.84), by
taking derivative with repsect to $\theta\_j$ and then rearraging terms, which finnaly gives

$$
\frac{\partial a_n^\ast }{\partial \theta_j}
= (\mathbf{I} + \mathbf{C}_N \mathbf{W}_N)^{-1} \frac{\partial \mathbf{C}_N}{\partial \theta_j} (\mathbf{t}_N - \boldsymbol{\sigma}_N)
$$

TODO: extension to multiclass

### Connection to nerual networks

For a broad class of priors on $\mathbf{w}$, the distributions of functions generated by neural network will tend to a Gaussian in the limit $M \rightarrow \infty$.