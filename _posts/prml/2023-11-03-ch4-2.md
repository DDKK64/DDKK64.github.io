---
layout: posts
title: 'PRML Ch4 - Linear Models for Classification - Part II'
---
- [Probabilistic discriminative models](#probabilistic-discriminative-models)
  - [Fixed basis functions](#fixed-basis-functions)
  - [Logistic regression](#logistic-regression)
  - [Iterative reweighted least squares](#iterative-reweighted-least-squares)
  - [Multiclass logistic regression](#multiclass-logistic-regression)
  - [Probit pregression](#probit-pregression)
  - [Canonical link functions](#canonical-link-functions)
- [The Laplace approximation](#the-laplace-approximation)
  - [Model comparison and BIC](#model-comparison-and-bic)
- [Bayesian logistic regression](#bayesian-logistic-regression)
  - [Laplace approximation](#laplace-approximation)
  - [Predictive distribution](#predictive-distribution)

## Probabilistic discriminative models

The discriminative approach models the posterior $p(C\_k\|\mathbf{x})$ directly in some generalized linear form and then uses maximum likelihood to determine the model.

Compared to the generative approach, it has the advantages of fewer adaptive parameters.

### Fixed basis functions

It is common to introduce a vector of basis functions $\boldsymbol{\phi} (\mathbf{x})$ to transform the input $\mathbf{x}$ into the feature space, 
in the hope that the decision boundaries become linear in the feature space.

However, such transformation cannot remove overlap among classes.

The modeling methods in the space of $\mathbf{x}$ discussed in previous sections, are equally applied in the feature vector space $\boldsymbol{\phi}$.

### Logistic regression

Consider a binary classification problem. We model the posterior distributions as

$$
\begin{align*}
p(C_1|\boldsymbol{\phi}) &= y (\boldsymbol{\phi}) = \sigma (\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) \tag{4.87} \\
p(C_2|\boldsymbol{\phi}) &= 1 - p(C_1|\boldsymbol{\phi}) \\
\end{align*}
$$

where $\boldsymbol{\phi}$ is the feature vector and $\sigma$ is the logistic sigmoid function.
In statistics, this model is called the **logistic regression**. Despite the name, it's for classification problem not for regression.

For $M$-dimensional $\boldsymbol{\phi}$, if we model the class conditional distribution with Gaussians,
the number of parameters in the posterior will be quadratic in $M$, even when we use shared covariance matrix.
In logistic regression, only $M$ parameters needs to estimate, which shows significant advantage when $M$ is large.

We now use maximum likelihood to estimate $\mathbf{w}$. 
Given a data set $\{\boldsymbol{\phi}\_n, t\_n\}$, where $N=1, \dots, N$ and $\boldsymbol{\phi}\_n \equiv \boldsymbol{\phi} (\mathbf{x}\_n)$,
the likelihood function takes the form

$$
p(\mathbf{t}|\mathbf{w}) = \prod_{n=1}^N y_n^{t_n} (1-y_n)^{1-t_n} \tag{4.89}
$$

where $\mathbf{t} = (t\_1, \dots, t\_N)^{\mathsf{T}}$ and $y\_n = p(C\_k\|\boldsymbol{\phi}\_n) = \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n)$.

Taking the negtive logrithm, we obtain the **cross-entropy** error function 

$$
E(\mathbf{w}) = -\ln p(\mathbf{t}|\mathbf{w})
= - \sum_{n=1}^N \left\{ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right\} \tag{4.90}
$$

Using

$$
\frac{d \sigma}{d a} = \sigma \cdot (1 - \sigma) \tag{4.88}
$$

we obtain the derivative

$$
\frac{\partial y_n}{\partial \mathbf{w}} = y_n (1 - y_n) \boldsymbol{\phi}_n
$$

The gradient of the error function with respect to $\mathbf{w}$ is then given by

$$
\nabla E(\mathbf{w}) = \sum_{n=1}^N (y_n - t_n) \boldsymbol{\phi}_n
$$

with which we can perform gradient-based optimization.

**Excercise 4.14**

However, this error function will cause severe over-fitting for linearly separable data sets. 
For such data sets, we will first find a $\mathbf{w}$ such that $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n > 0$ for $n \in C\_1$ and $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n < 0$ for $n \in C\_2$,
and from (4.90) we see $E(\mathbf{w}) \ge 0$. To achieve the minimum of $0$ we want $y\_n \rightarrow t\_n$,
which will drive the magnitude of $\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n$ to infinity and thus $y\_n \rightarrow 0 \text{ or } 1$.

The singularity can be avoided by MAP approach or equivalently introducing a regularization term.

### Iterative reweighted least squares

Faster convergence can be achived by exploiting the second derivative of the error function.

Consider the iteration

$$
\mathbf{w}^{(\mathrm{\tau+1})} = \mathbf{w}^{(\mathrm{\tau})} + \boldsymbol{\epsilon}
$$

in which we choose proper value of $\boldsymbol{\epsilon}$ to optmize the error function $E(\mathbf{w})$.

Expanding the error function by Taylor theorem

$$
E(\mathbf{w} + \boldsymbol{\epsilon}) \simeq E(\mathbf{w})
+ \boldsymbol{\epsilon}^{\mathsf{T}} \nabla E(\mathbf{w})
+ \frac{1}{2} \boldsymbol{\epsilon}^{\mathsf{T}} \mathbf{H} \boldsymbol{\epsilon}
$$

If $\mathbf{H}$ is postive definite, $E(\mathbf{w}+\boldsymbol{\epsilon})$ will be a strictly convex function of $\boldsymbol{\epsilon}$.
The minimum can be obtained by setting derivative with respect to $\boldsymbol{\epsilon}$ to zero, which gives

$$
\begin{align*}
\frac{\partial}{\partial \boldsymbol{\epsilon}} E(\mathbf{w} + \boldsymbol{\epsilon}) 
= \nabla E(\mathbf{w}) + \mathbf{H} \boldsymbol{\epsilon} = 0
\end{align*}
$$

which gives rise to the Newton-Raphson method

$$
\mathbf{w}^{(\mathrm{\tau+1})} = \mathbf{w}^{(\mathrm{\tau})} - \mathbf{H}^{-1} \nabla E(\mathbf{w})
$$

\
To begin with, we try to apply this method on the linear regression model with the sum-of-squares error function defined by (3.12).
The gradient and the Hessian are given by

$$
\begin{align*}
\nabla E(\mathbf{w}) &= \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{w} - \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
\mathbf{H} &= \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \\
\end{align*}
$$

It can be shown that $\mathbf{H}$ is postive semi-definite,
the sum-of-squares error function is convex.
If $\boldsymbol{\Phi}$ further has linearly independent columns, $\mathbf{H}$ will be postive definite,
and we can apply the Newton-Raphson to obtain the iteration

$$
\begin{align*}
\mathbf{w}^{(\mathrm{\tau+1})} &= \mathbf{w}^{(\mathrm{\tau})}
- (\boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1}
\left\{ \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \mathbf{w}^{(\mathrm{\tau})} - \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \right\} \\
&= (\boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t} \\
\end{align*}
$$

where $\mathbf{w}^{\mathrm{\tau}}$ is canceld. The result is the exact solution for least-sqaures.

\
For logistic regression model with two classes, we optimize the cross-entropy error function defined by (4.90). The gradient and the Hessian gives

$$
\begin{align*}
\nabla E(\mathbf{w}) &= \sum_{n=1}^N (y_n - t_n) \boldsymbol{\phi}_n
= \boldsymbol{\Phi}^{\mathsf{T}} (\mathbf{y} - \mathbf{t}) \\
\mathbf{H} &= \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi}
\end{align*}
$$

where $\mathbf{R}$ is an $N \times N$ diagonal matrix with elements

$$
R_{nn} = y_n (1 - y_n)
$$

**Excercise 4.15**

Let $\mathbf{u} \ne \mathbf{0}$ be a vector in $\mathbb{R}^D$. Then we have

$$
\mathbf{u}^{\mathsf{T}} \mathbf{H} \mathbf{u}
= (\boldsymbol{\Phi} \mathbf{u})^{\mathsf{T}} \mathbf{R} (\boldsymbol{\Phi} \mathbf{u})
$$

Since $\mathbf{R}$ is diagonal and $0 < R\_{nn} < 1$, we see $\mathbf{R}$ is postive definite.
Provided that columns of $\boldsymbol{\Phi}$ are linearly independent,
we have $\boldsymbol{\Phi} \mathbf{u} \ne \mathbf{0}$ and hence $\mathbf{H}$ is postive definite.
The error function is strictly convex and we can conduct Newton-Raphson iteration

$$
\begin{align*}
\mathbf{w}^{(\mathrm{\tau+1})} &= \mathbf{w}^{(\mathrm{\tau})}
- (\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf{T}} (\mathbf{y} - \mathbf{t}) \\
&= (\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi})^{-1} 
\left\{ \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi} \mathbf{w}^{(\mathrm{\tau})} - \boldsymbol{\Phi}^{\mathsf{T}} (\mathbf{y} - \mathbf{t}) \right\} \\
&= (\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \boldsymbol{\Phi})^{-1} 
\boldsymbol{\Phi}^{\mathsf{T}} \mathbf{R} \mathbf{z}
\end{align*}
$$

where

$$
\mathbf{z} = \boldsymbol{\Phi} \mathbf{w}^{(\mathrm{\tau})} - \mathbf{R}^{-1} (\mathbf{y} - \mathbf{t})
$$

This is known as the **iterative reweighted least squares** (or IRLS) for logistic regression.

$\mathbf{R}$ can be interpreted as variances since the mean and variance of the target $t$ in the logistic model is given by

$$
\begin{align*}
\operatorname{E} [t] &= p(C_1|\mathbf{x}) = y \\
\operatorname{var} [t] &= \operatorname{E} [t^2] - \operatorname{E} [t]
= y (1 - y)
\end{align*} 
$$

### Multiclass logistic regression

For multiclass case, the posterior distributions are defined in the form of softmax function, which are given by

$$
p(C_k|\boldsymbol{\phi}) = y_k (\boldsymbol{\phi}) 
= \frac{\exp(a_k)}{\sum_j \exp(a_j)}
$$

where activations $a\_k$ are defined by

$$
a_k = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}
$$

We now use the maximum likelihood to estimate the parameters. Suppose the target vector $\mathbf{t}$ applies 1-of-K coding scheme.
The likelihood function is given by

$$
p(\mathbf{T}|\mathbf{w}_1, \dots, \mathbf{w}_K)
= \prod_{n=1}^N \prod_{k=1}^K p(C_k|\boldsymbol{\phi}_n)^{t_{nk}}
= \prod_{n=1}^N \prod_{k=1}^K y_{nk}^{t_{nk}}
$$

where we defined $y\_{nk} = p(C\_k\|\boldsymbol{\phi}\_n)$, and $\mathbf{T}$ is an $N \times K$ matrix $\mathbf{T} = (\mathbf{t}\_1, \dots, \mathbf{t}\_N)^{\mathsf{T}}$.

Taking the negtive logarithm, we obtain the cross-entropy error function

$$
E(\mathbf{w}_1, \dots, \mathbf{w}_K) = - \ln p(\mathbf{T}|\mathbf{w}_1, \dots, \mathbf{w}_K)
= - \sum_{n=1}^N \sum_{k=1}^K t_{nk} \ln y_{nk}
$$

**Excercise 4.18**

By using the derivative

$$
\frac{\partial y_k}{\partial a_j} = y_k (I_{kj} - y_j)
$$

the radient with repsect to $\mathbf{w}\_j$ gives

$$
\begin{align*}
\nabla_{\mathbf{w}_j} E(\mathbf{w}_1, \dots, \mathbf{w}_K)
&= - \sum_n \sum_k t_{nk} \frac{1}{y_{nk}} y_{nk} (I_{kj} - y_{nj}) \boldsymbol{\phi}_n \\
&= -\sum_n \sum_k I_{kj} t_{nk} \boldsymbol{\phi}_n
+ \sum_n \sum_k t_{nk} y_{nj} \boldsymbol{\phi}_n \\
&= -\sum_n t_{nj} \boldsymbol{\phi}_n + \sum_n y_{nj} \boldsymbol{\phi}_n \\
&= \sum_n (y_{nj} - t_{nj}) \boldsymbol{\phi}_n \tag{4.109} \\
\end{align*}
$$

with which we can perform gradient-based optimization.

Defining a full parameter vector $\widetilde{\mathbf{w}}$ which is obtained by vertically stacking $\mathbf{w}\_k$

$$
\widetilde{\mathbf{w}} = \begin{pmatrix}
\mathbf{w}_1 \\
\vdots \\
\mathbf{w}_K \\
\end{pmatrix} 
$$

The full gradient gives

$$
\nabla_{\widetilde{\mathbf{w}}} E = \begin{pmatrix}
\nabla_{\mathbf{w}_1} E \\
\vdots \\
\nabla_{\mathbf{w}_K} E \\
\end{pmatrix} 
$$

Taking the gradient of (4.109) again with respect to $\mathbf{w}\_k$, we obtain

$$
\mathbf{H}_{jk} = \nabla_{\mathbf{w}_k} \nabla_{\mathbf{w}_j} E(\mathbf{w}_1, \dots, \mathbf{w}_K)
= - \sum_{n=1}^N y_{nk} (I_{kj} - y_{nj}) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \tag{4.110}
$$

which is the block $(j, k)$ of the full Hessian matrix $\nabla\_{\widetilde{\mathbf{w}}} \nabla\_{\widetilde{\mathbf{w}}} E$. The full Hessian now comprises of $K \times K$ blocks each of which has a size of $M \times M$.

**Excercise 4.20**

To show that the Hessian is postive semi-definite. Consider a non-zero vector $\mathbf{u}$ of size $MK$, which is concatenated vertically by vectors $\mathbf{u}\_i$ of size $M$.

Then we have

$$
\begin{align*}
\mathbf{u}^{\mathsf{T}} \mathbf{H} \mathbf{u}
&= \sum_{i=1}^K \sum_{j=1}^K \mathbf{u}_i^{\mathsf{T}} \mathbf{H}_{ij} \mathbf{u}_j \\
&= \sum_{i=1}^K \sum_{j=1}^K \mathbf{u}_i^{\mathsf{T}}
\left( \sum_{n=1}^N y_{nj} (I_{ij} - y_{ni}) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \right) \mathbf{u}_j \\
&= \sum_i \sum_j \sum_n \mathbf{u}_i^{\mathsf{T}} y_{nj} I_{ij} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_j 
- \sum_i \sum_j \sum_n \mathbf{u}_i^{\mathsf{T}} y_{nj} y_{ni} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_j \\
&= \sum_n \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_i
- \sum_n \left( \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \right) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \left( \sum_i y_{ni} \mathbf{u}_i \right) \\
&= \sum_n \left\{ \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}_i 
- \left( \sum_i \mathbf{u}_i^{\mathsf{T}} y_{ni} \right) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \left( \sum_i y_{ni} \mathbf{u}_i \right) \right\} \\
\end{align*}
$$

in which $\mathbf{H}\_{ij}$ is block $(i, j)$ of the Hessian matrix given by (4.110).

Defining a function

$$
f(\mathbf{u}) = \mathbf{u}^{\mathsf{T}} \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u}
$$

we have

$$
f(\mathbf{u}) = \left\| \boldsymbol{\phi}_n^{\mathsf{T}} \mathbf{u} \right\|^2 \ge 0
$$

Hence $\boldsymbol{\phi} \boldsymbol{\phi}^{\mathsf{T}}$ is postive semi-definite 
and $f(\mathbf{u})$ is convex.

Using Jesnsen's inequality we have:

$$
f \left( \frac{\sum_i y_{ni} \mathbf{u}_i}{\sum_i y_{ni}} \right)
\le \sum_i y_{ni} f(\mathbf{u}_i)
$$

Here we used $\sum\_i y\_{ni} = 1$ and the equality holds if and only if $\mathbf{u}\_1 = \cdots = \mathbf{u}\_K$.

Therefore, we have

$$
\mathbf{u}^{\mathsf{T}} \mathbf{H} \mathbf{u} \ge \mathbf{0}
$$

and $\mathbf{H}$ is postive semi-definite.

### Probit pregression

Althogh a broad range of distributions can be expressed as a sgimoid (or softmax) transformation on a linear combination of input variables,
there's still some distributions for which we cannot apply this approach, e.g. mixture distributions.

Consider a two-class problem and a generalized linear model defined by

$$
p(t=1|a) = f(a)
$$

where $a=\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}$ and $f(\cdot)$ is the activition function.

For each input $\boldsymbol{\phi}\_n$, we evaluate $a\_n = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n$ and select the target value according to

$$
t_n = 
\begin{cases}
1 & \text{ if } a_n \ge \theta \\
0 & \text{ otherwise} \\
\end{cases}
$$

in which $\theta$ is a noisy threshold drawn from the distribution $p(\theta)$.

Then the activation function is given by the cumulative distribution function

$$
f(a) = \int_{- \infty}^a p(\theta) \,d\theta
$$

When $p(\theta) = \mathcal{N} (\theta \| 0, 1)$, we obtain the cumulative distribution

$$
\Phi (a) = \int_{- \infty}^a \mathcal{N} (\theta|0, 1) \,d\theta
$$

Its inverse $\Phi^{-1}$ is called the **probit** function.

Many numerical programs provide methods to calculate the **erf** function or **error** function defined by

$$
\mathrm{erf} (a) = \frac{2}{\sqrt{\pi}} \int_0^a \exp \left( - \theta^2 \right) \,d\theta
$$

It is related to the cumulative distribution function by

$$
\Phi (a) = \frac{1}{2} \left\{ 1 + \mathrm{erf} \left( \frac{a}{\sqrt{2}} \right) \right\}
$$

The generalized linear model based on the cumulative distribution activation is called the **probit regression**.

The probit model is significantly more sensitive to outiliers than the logistic sigmoid,
since the tail of probit model decays in $\exp (-x^2)$.

\
Note that, both logistic sigmoid and the probit models assume that the target value is correctly labeled.
To deal with mislabelling, a probability $\epsilon$ is introduced to indicate the chance of mislabelling.
For a two-class classification problem, the target distribution is then given by

$$
\begin{align*}
p(t|\mathbf{x}) &= (1 - \epsilon) \sigma(\mathbf{x}) + \epsilon (1-\sigma(\mathbf{x})) \\
&= \epsilon + (1 - 2 \epsilon) \sigma(\mathbf{x})
\end{align*}
$$

where $\sigma$ is the activation function. $\epsilon$ can be set in advance or estimated from data.

### Canonical link functions

Among the linear models dicussed in chapter 3 and chapter 4, many of them have a derivative with respect to $\mathbf{w}$ which takes the form of the error given by $y\_n - t\_n$ multiplied with the feature $\boldsymbol{\phi}\_n$.
Indeed, this is a general result related to the exponential family.
The corresponding choice of activation function is called the **canonical link function**.

Consider a distribution over target $t$, with the restricted form of expoenential family, which is given by

$$
p(t|\eta, s) = \frac{1}{s} h \left( \frac{t}{s} \right) g(\eta) \exp \left( \frac{\eta t}{s} \right)
$$

Using (2.226), with following change of variables

$$
g(\boldsymbol{\eta}) \rightarrow g(\eta),
\quad h(\mathbf{x}) \rightarrow \frac{1}{s} h(\frac{t}{s}),
\quad \boldsymbol{\eta} \rightarrow \eta,
\quad \mathbf{u}(\mathbf{x}) \rightarrow \frac{t}{s}
$$

we have

$$
\operatorname{E} [t|\eta] = - s \frac{d}{d \eta} \ln g(\eta)
$$


Define a generalized linear model on the conditional mean

$$
y = \operatorname{E} [t|\eta] = f(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi})
$$

where $f(\cdot)$ is an **activation function**. Its inverse $f^{-1}(\cdot)$ is known as the **link function** in statistics.
Note that, assuming $s$ is constant, $\eta$ and $y$ are also linked by some function $\eta = \psi (y)$.

Treating the likelihood function as a function of $\eta$, we have

$$
\ln p(\mathbf{t}|\eta,s) = \sum_{n=1}^N \ln p(t_n|\eta_n, s)
= \sum_{n=1}^N \left\{ \ln g(\eta_n) + \frac{\eta_n t_n}{s} \right\} + \mathrm{const}
$$

where $\text{const}$ concludes terms independent of $\eta$ and all observations share a common scale parameter $s$.

Taking derivative with repspect to $\mathbf{w}$, we have

$$
\begin{align*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t}|\eta,s) 
&= \sum_{n=1}^N \left\{ \frac{d}{d \eta_n} \ln g(\eta_n) + \frac{t_n}{s} \right\}
\frac{d \eta_n}{d a_n} \nabla a_n \\
&= \sum_n \frac{1}{s} (t_n - y_n) \frac{d \eta_n}{d a_n} \boldsymbol{\phi}_n \\
\end{align*}
$$

where $a\_n = \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n$ and $y\_n = f(a\_n)$.

If we choose a particular link function $f^{-1}$ such that

$$
f^{-1} = \psi
$$

we have

$$
\begin{align*}
\eta_n = \psi (y_n) = f^{-1} (y_n) = f^{-1} (f(a_n)) = a_n
\end{align*}
$$

The gradient of the error function then takes the form

$$
\begin{align*}
\nabla E(\mathbf{w}) &= - \nabla_{\mathbf{w}} \ln p(\mathbf{t}|\eta,s) \\
&= \frac{1}{s} \sum_{n=1}^N (y_n - t_n) \boldsymbol{\phi}_n
\end{align*}
$$

For the Gaussian $s = \beta^{-1}$; for logistic model $s=1$.

## The Laplace approximation

To handle the case when the integral over parameters cannot be easily solved, some approximation method is required.

Consider a univariate distribution given by

$$
p(z) = \frac{1}{Z} f(z)
$$

where $Z = \int f(z) \,dz$. The goal of the Laplace method is to find a Gaussian approximation $q(z)$ centered on the mode of $p(z)$.

Let $z\_0$ be a stationary point of $p(z)$, so that $f^\prime (z\_0) = 0$.

Expanding $\ln f(z)$ at $z\_0$ using Taylor's theorem gives

$$
\begin{align*}
\ln f(z) &\simeq \ln f(z_0) + \frac{f^\prime (z_0)}{f(z_0)} (z - z_0)
+ \frac{1}{2} \left( \frac{d^2}{d z^2} \ln f(z) \bigg|_{z = z_0} \right) (z-z_0)^2 \\
&= \ln f(z_0) - \frac{1}{2} A (z-z_0)^2 \\
\end{align*}
$$

where we defined

$$
A = - \frac{d^2}{d z^2} \ln f(z) \bigg|_{z = z_0} 
$$

Taking the exponential of both sides we have

$$
f(z) \simeq f(z_0) \exp \left\{ - \frac{A}{2} (z-z_0)^2 \right\}
$$

Normalizing $f(z)$ we obtain the distribution

$$
q(z) = \left( \frac{A}{2 \pi} \right)^{1/2} \exp \left\{ - \frac{A}{2} (z - z_0)^2 \right\}
$$

Note that a well defined $q(z)$ requires $A>0$, that is, the second derivative at $z\_0$ is negative and hence $z\_0$ must be a local maximum.

For a random vector $\mathbf{z}$ with dimensionality $M$ and a stationary point $\mathbf{z}\_0$.
Using similar approach, we obtain

$$
f(\mathbf{z}) \simeq f(\mathbf{z}_0) \exp\left\{ - \frac{1}{2} (\mathbf{z} - \mathbf{z}_0)^{\mathsf{T}} \mathbf{A} (\mathbf{z} - \mathbf{z}_0) \right\}
$$

where

$$
\mathbf{A} = -\nabla \nabla \ln f(\mathbf{z}) \big|_{\mathbf{z} = \mathbf{z}_0} 
= - \mathbf{H}(\ln f(\mathbf{z})) \big|_{\mathbf{z} = \mathbf{z}_0} \tag{4.132}
$$

Normalizing over $\mathbf{z}$ gives

$$
q(\mathbf{z}) = \mathcal{N} (\mathbf{z} | \mathbf{z}_0, \mathbf{A}^{-1})
$$

$q(\mathbf{z})$ is well defined only if $\mathbf{A}$ is postive definite, which implies $\mathbf{z}\_0$ is a local maximum.

In practice, a mode can be found numerically. For multimodal distributions, different modes have different approximations.

The justification for the Gaussian approximation is that, as a result of central limit theorem,
in the limit of infinite data, the posterior distribution converges to some Gaussian distribution.
Therefore the Laplace approxmation workds well on large data sets.

Note that, the Laplace appximation only uses local information at specific point, which will fail to capture global information.

### Model comparison and BIC

Using (4.133), we obtain an approximation to the normalization constant $Z$

$$
\begin{align*}
Z &= \int f(\mathbf{z}) \,d \mathbf{z} \\
&\simeq f(\mathbf{z}_0) \int \exp\left\{ - \frac{1}{2} (\mathbf{z} - \mathbf{z}_0)^{\mathsf{T}} \mathbf{A} (\mathbf{z} - \mathbf{z}_0) \right\} \,d\mathbf{z} \\
&= f(\mathbf{z}_0) \frac{(2 \pi)^{M/2}}{|\mathbf{A}|^{1/2}}
\tag{4.135}
\end{align*}
$$

By Bayes' theorem, the model evidence is given by

$$
p(D|M_i) = \int p(D|\boldsymbol{\theta}, M_i) p (\boldsymbol{\theta}, M_i) \,d \boldsymbol{\theta}
$$

To keep notations uncluttered, the conditioning on $M\_i$ will be omitted.

Identifying $f(\boldsymbol{\theta}) =  p(D\|\boldsymbol{\theta}) p (\boldsymbol{\theta})$ and 
$Z = p(D)$, we see that the mode of $f(\boldsymbol{\theta})$ correponds to the maximum posterior solution $\boldsymbol{\theta}\_{\mathrm{MAP}}$.
Using (4.135), we have

$$
\ln p(D) \simeq \ln p(D|\boldsymbol{\theta}_{\mathrm{MAP}}) + \ln p(\boldsymbol{\theta}_{\mathrm{MAP}}) + \frac{M}{2} \ln 2 \pi 
- \frac{1}{2} \ln |\mathbf{A}|
\tag{4.137}
$$

where

$$
\mathbf{A} = - \nabla \nabla \ln p(D|\boldsymbol{\theta}_{\mathrm{MAP}}) p(\boldsymbol{\theta}_{\mathrm{MAP}}) = - \nabla \nabla \ln p(\boldsymbol{\theta}_{\mathrm{MAP}}|D)
\tag{4.138}
$$

Here we used $\partial p(D) / \partial \boldsymbol{\theta} = \mathbf{0}$.

The first term on the r.h.s of (4.137) is the log likelihood function evaluated with the optimized parameters,
the remaining terms are called **Occam factor** which panelizes model complexity.

**Excercise 4.23**

Consider a prior distribution over parameters given by

$$
p(\boldsymbol{\theta}) = \mathcal{N} (\boldsymbol{\theta} | \mathbf{m}, \mathbf{V}_0)
$$

which is followed by

$$
\ln p(\boldsymbol{\theta}) = - \frac{M}{2} \ln 2\pi - \frac{1}{2} \ln |\mathbf{V}_0|
- \frac{1}{2} (\boldsymbol{\theta} - \mathbf{m})^{\mathsf{T}} \mathbf{V}_0^{-1} (\boldsymbol{\theta} - \mathbf{m})
$$

By (4.138), we have

$$
\mathbf{A} = - \nabla \nabla \ln p(D|\boldsymbol{\theta}_{\mathrm{MAP}})
- \nabla \nabla \ln p(\boldsymbol{\theta}_{\mathrm{MAP}})
= \mathbf{H} + \mathbf{V}_0^{-1}
$$

where we defined $\mathbf{H} = - \nabla \nabla \ln p(D\|\boldsymbol{\theta}\_{\mathrm{MAP}})$

On the assumption that the prior is broad, $\|\mathbf{V}\_0^{-1}\|$ will be small, so that
$\mathbf{A} \simeq \mathbf{H}$ and the third term on the r.h.s of $\ln p(\boldsymbol{\theta})$ can be ignored.

Applying (4.137), we obtain

$$
\ln p(D) \simeq \ln p(D|\boldsymbol{\theta}_{\mathrm{MAP}}) - \frac{1}{2} \ln |\mathbf{H}|
+ \textrm{const}
$$

where $\textrm{const}$ concludes terms idependent of $\boldsymbol{\theta}$.

Further assume that data set size $N$ is sufficiently large, so that

$$
\mathbf{H} = N \frac{1}{N} \sum_n \mathbf{H}_n 
\simeq N \operatorname{E} [\mathbf{H}_n | \boldsymbol{\theta}_{\mathrm{MAP}}]
$$

where $\mathbf{H}\_n$ is the Hessian for a single observation.

Then we have

$$
\begin{align*}
\ln |\mathbf{H}|
&\simeq \ln N^M \big| \operatorname{E} [\mathbf{H}_n | \boldsymbol{\theta}_{\mathrm{MAP}}] \big| \\
&= M \ln N + \ln \left| \operatorname{E} \left[ \mathbf{H}_n \big| \boldsymbol{\theta}_{\mathrm{MAP}} \right] \right| \\
\end{align*}
$$

If that $\left\| \operatorname{E} \left[ \mathbf{H}\_n \big\| \boldsymbol{\theta}\_{\mathrm{MAP}} \right] \right\|$ is bounded,
then we can ignore it compared to $M \ln N$.

Combining these terms, we obtain a rough approximation to (4.137) given by

$$
\mathrm{BIC} \simeq \ln p(D|\boldsymbol{\theta}_{\mathrm{MAP}}) - \frac{1}{2} M \ln N
$$

which is known as the **Baysian Information Criterion** or the **Schwarz criterion**.

Compared to AIC, BIC panelizes model complexity more severely.

Note that, the assumption that the Hessian is bounded is often invalid since parameters may  not be well-determined and the corresponding determinant will diverge.

## Bayesian logistic regression

Exact Bayesian inference is intractable for logistic regression, since the evaluation of the posterior and the predictive distribution both involves integration of the product of logistic sigmoid functions and the prior probability density. 

### Laplace approximation

Suppose the prior distribution of parameter $\mathbf{w}$ follows the Gaussian distribution

$$
p(\mathbf{w}) = \mathcal{N} (\mathbf{w} | \mathbf{m}_0, \mathbf{S}_0)
$$

By

$$
p(\mathbf{w}|\mathbf{t}) \propto p(\mathbf{t}|\mathbf{w}) p (\mathbf{w})
$$

we write the log posterior as

$$
\ln p(\mathbf{w}|\mathbf{t}) = -\frac{1}{2} (\mathbf{w} - \mathbf{m})^{\mathsf{T}} \mathbf{S}_0^{-1} (\mathbf{w} - \mathbf{m}_0)
+ \sum_{n=1}^N \left\{ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right\}
+ \text{const}
$$

in which $y\_n = \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}\_n)$
and $\text{const}$ concludes terms independent of $\mathbf{w}$.

To conduct Laplace appximation on the posterior distribution, we first find the maximum posterior solution $\mathbf{w}\_{\mathrm{MAP}}$ by numerical methods, which will be the mean of the Gaussian approximation.

Then from (4.132), the covariance matrix of the approximation is

$$
\mathbf{S}_N^{-1} = \mathbf{S}_0^{-1} + \sum_{n=1}^N y_n (1-y_n) \boldsymbol{\phi}_n \boldsymbol{\phi}_n^{\mathsf{T}}
$$

Thus the Gaussian approximation to the posterior distribution is given by

$$
q(\mathbf{w}) = \mathcal{N} (\mathbf{w}|\mathbf{w}_{\mathrm{MAP}}, \mathbf{S}_N)
\tag{4.144}
$$

### Predictive distribution

With the posterior distribution, we can obtain the prediction distribution by integrating out the $\mathbf{w}$. Using (4.144) we have 

$$
p(C_1|\boldsymbol{\phi}, \mathbf{t}) 
= \int p(C_1 | \boldsymbol{\phi}, \mathbf{w}) p(\mathbf{w}|\mathbf{t}) \,d\mathbf{w}
\simeq \int \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \, d\mathbf{w}
$$

The predictive distribution for class $C\_2$ is then given by $p(C\_2\|\boldsymbol{\phi}, \mathbf{t})=1 - p(C\_1\|\boldsymbol{\phi}, \mathbf{t})$.

To evaluate the integral, we first define

$$
\sigma (\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi})
= \int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) \sigma (a) \,da
$$

where $\delta(\cdot)$ is the Dirac delta function. Then the integral becomes

$$
\begin{align*}
\int \sigma(\mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \, d\mathbf{w}
&= \int q(\mathbf{w}) \,d\mathbf{w}
\int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) \sigma (a) \,da \\
&= \int \sigma(a) \,da \int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \,d\mathbf{w} \\
&= \int \sigma(a) p(a) \,da \\
\end{align*}
$$

where we defined

$$
p(a) = \int \delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}) q(\mathbf{w}) \,d\mathbf{w}
$$

**Excercise 4.24**

To evaluate $p(a)$, without loss of generality, we define $\delta(\cdot)$ as a Gaussian distribution in the limit form such that

$$
\delta(a - \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi})
= \lim_{\beta \rightarrow \infty} \mathcal{N} (a | \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi}, \beta^{-1})
$$

$p(a)$ is now a marginalized distribution of two Gaussians. Using the results from section 2.2.3, we have

$$
p(a) = \mathcal{N} (a| \mu_a, \sigma_a^2)
$$

where

$$
\begin{align*}
\mu_a &= \operatorname{E} [a] = \boldsymbol{\phi}^{\mathsf{T}} \mathbf{w}_{\mathrm{MAP}} \\
\sigma_a^2 &= \operatorname{var} [a] = \boldsymbol{\phi}^{\mathsf{T}} \mathbf{S}_n \boldsymbol{\phi} \\
\end{align*}
$$

Still, the integration of the product of a Gaussian and a logistic sigmoid cannot be evaluated analytically. 
A good approximation for $\sigma(a)$ is $\Phi (\lambda a)$,
in which the value $\lambda = \sqrt{\pi/8}$ is determined so that both functions have the same slope at the origin.
The integral is then approximated by

$$
\int \sigma(a) p(a) \,da = 
\int \sigma(a) \mathcal{N} (a|\mu_a, \sigma_a^2) \,da
\simeq \int \Phi(\lambda a) \mathcal{N} (a|\mu_a, \sigma_a^2) \,da
$$

**Excercise 4.26**

To evalute the integral, we define two random varianbles such that

$$
\begin{align*}
X &\sim \mathcal{N} (0, 1) \\
Y &\sim \mathcal{N} (\mu, \sigma^2) \\
\end{align*}
$$

Identify

$$
\Phi (\lambda a) = P(X \le \lambda Y | Y=a )
$$

and

$$
\begin{align*}
P(X - \lambda Y \le 0) = P(X \le \lambda Y)
&= \int P(X \le \lambda Y | Y=a) \mathcal{N} (a|\mu, \sigma^2) \,da \\
&= \int \Phi(\lambda a) \mathcal{N} (a|\mu, \sigma^2) \,da \\
\end{align*}
$$

Since

$$
X - \lambda Y \sim \mathcal{N} (- \lambda \mu, 1 + \lambda^2 \sigma^2)
$$

we have

$$
P(X - \lambda Y \le 0) = \Phi \left( \frac{\lambda \mu}{(1 + \lambda^2 \sigma^2)^{1/2}} \right)
\simeq \sigma \left( k(\sigma^2) \mu \right)
$$

where we used the approximation $\sigma(a) \simeq \Phi(\lambda a)$ again and defined

$$
k(\sigma^2) = (1 + \pi \sigma^2 /8)^{-1/2}
$$

Finally, the approximated predictive distribution is given by

$$
p(C_1|\boldsymbol{\phi}, \mathbf{t}) \simeq \sigma \left( k(\sigma_a^2) \mu_a \right)
$$

Note that the decision boundary corresponding to $p(C\_1 \| \boldsymbol{\phi}, \mathbf{t}) = 0.5$ is given by $\mu\_a = 0$,
which gives the same decision boundary as using the $\mathbf{w}\_{\mathrm{MAP}}$ directly.
So if the decision criterion is the misclassification rate, the marginalization over $\mathbf{w}$ is redundant.
