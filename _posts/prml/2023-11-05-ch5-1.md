---
layout: posts
title: 'PRML - Neural networks - Part I'
---
- [Neural networks](#neural-networks)
  - [Feed-forward network functions](#feed-forward-network-functions)
    - [weight-space symmetries](#weight-space-symmetries)
  - [Network training](#network-training)
    - [Parameter optimization](#parameter-optimization)
    - [Local quadratic approximation](#local-quadratic-approximation)
    - [Use of gradient information](#use-of-gradient-information)
    - [Gradient descent optimization](#gradient-descent-optimization)
  - [Error backpropagation](#error-backpropagation)
    - [Evaluation of error-function derivatives](#evaluation-of-error-function-derivatives)
    - [A simple example](#a-simple-example)
    - [Efficiency of backpropagation](#efficiency-of-backpropagation)
    - [The Jacobian matrix](#the-jacobian-matrix)
  - [The Hessian matrix](#the-hessian-matrix)
    - [Diagonal approximation](#diagonal-approximation)
    - [Outer product approximation](#outer-product-approximation)
    - [TODO: more methods](#todo-more-methods)

# Neural networks

This chapter gives an introduction to the neural networks.

In previous chapters, a class of models with fixed basis functions have been discussed.
However, we stil have not discussed how to choose the basis functions.

The neural network introduces adaptive basis functions.

## Feed-forward network functions

We begin with a simple two-layer neural network.

First we construct $M$ linear combinations of $D$ input variables

$$
a_j = \sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}^{(1)}
$$

where $j = 1, \dots, M$. The superscript $(1)$ indicates the layer of parameters in the network.
$w\_{ji}^{(1)}$ is called the weights and $w\_{j0}^{(1)}$ is the bais.
$a\_j$ are called the **activations**. Using a nonlinear **activation function** $h(\cdot)$ such as logistic sigmoid, we transform each $a\_j$ to give

$$
z_j = h(a_j)
$$

which are called **hidden units**.

Linearly combining $z\_j$ gives the **output unit activations**

$$
a_k = \sum_{j=1}^M w_{kj}^{(2)} z_j + w_{k0}^{(2)}
$$

where $k=1,\dots,K$ and $K$ is the number of output variables.

Finally, the output unit activations are transformed into the network outputs $y\_k$, with another appropriate activation function.

The choice of activation function is determined by the nature of the data and the assumed distribution of target variables.
For regression problems, the identity $y\_k = a\_k$ is typically the choice.
For binary classification problem, the logistic sigmoid is used, and for multi-class classification, we can use the softmax function.

This process can be interpreted as a forward propagation of infomation from the input to the output.

![forward-propagation](/assets/images/prml/5.2.png)

Indeed, if all activation functions for hidden units are taken to be linear, a equivalent network without hidden units can always be found.
Since the superposition of multiple linear transformations is still a linear transformation.

If the number of hidden units is less than the number of input or output units, information may be lost.

There are different terminologies of counting the layers of a network:

- By the number of layers of units. In this case we have 3.
- By the number of layers of hidden units. Then we have 1.
- By the number of layers of weights. Used in this book, it counts to 2.

An optional **skip-layer** connects the inputs directly to the outputs, with each connection associated with an adaptive parameter.
Sigmoidal hidden units can mimic such layer by setting intput weights to very small value, so that the activation function presents approximate linearity.

The **universal approximation** property states that if the neural network is flexible enough,
with suitable parameters, any continuous function can be approximated in any precision.

### weight-space symmetries

In feed-forward architecture, multiple distinct choices of weights and biases can lead to
equivalent mapping from inputs to outputs.

Consider interchange of the hidden units along with associated weights of each unit.
The resulting new weight vectors gives the same mapping.
In this way, we obtain $M!$ equivelent weight values.

## Network training

Starting from univariate regression problems, we assume that target $t$ follows a Gaussian distribution so that

$$
p(t|\mathbf{x}, \mathbf{w}, \beta) = \mathcal{N} (t | y(\mathbf{x}, \mathbf{w}), \beta^{-1})
$$

in which $y(\mathbf{x}, \mathbf{w})$ is the network output and $\beta$ is the precision of the noise.

Given a data set $\mathbf{X} = \{\mathbf{x}\_1, \dots, \mathbf{x}\_N \}$ and corresponding targets $\mathbf{t} = \{t\_1, \dots, t\_N\}$,
the likelihood function is given by

$$
p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta) = \prod_{n=1}^N p(t_n | \mathbf{x}_n, \mathbf{w}, \beta^{-1})
$$

Taking the negtive logarithm gives

$$
- \ln p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta) 
=  \frac{\beta}{2} \sum_{n=1}^N \left[ y(\mathbf{x}_n, \mathbf{w}) - t_n \right]^2 - \frac{N}{2} \ln \beta
+ \frac{N}{2} \ln 2 \pi
$$

Maximizing the likelihood function with respect to $\mathbf{w}$ is equivalent to minimizing the sum-of-sqaures error given by

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left[ y(\mathbf{x}_n, \mathbf{w}) -t_n \right]^2
$$

$\beta$ is then estimated with

$$
\frac{1}{\beta_{\mathrm{ML}}} = \frac{1}{N} \sum_{n=1}^N \left[ y(\mathbf{x}_n, \mathbf{w}_{\mathrm{ML}}) -t_n \right]^2
$$

\
In the case of multivariate target $\mathbf{t}$, with a shared noise precision $\beta$, the distribution is given by

$$
p(\mathbf{t}|\mathbf{x}, \mathbf{w}, \beta) = \mathcal{N} (\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{w}), \beta^{-1}\mathbf{I})
$$

The corresponding error function takes the same form as the univaraite case

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\| \mathbf{y}(\mathbf{x}_n, \mathbf{w})- \mathbf{t}_n \right\|^2
$$

And the estimator for $\beta$

$$
\frac{1}{\beta_{\mathrm{ML}}} = \frac{1}{NK} \sum_{n=1}^N \left\| \mathbf{y}(\mathbf{x}_n, \mathbf{w})- \mathbf{t}_n \right\|^2
$$

**Excercise 5.3**

If the covariance matrix is arbitrary, then the error function is given by

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left[ \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n
 \right]^{\mathsf{T}} 
\boldsymbol{\Sigma}^{-1} \left[  \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n \right]
$$

And the estimator for $\boldsymbol{\Sigma}$ gives

$$
\boldsymbol{\Sigma}_{\mathrm{ML}} = \frac{1}{N} \sum_{n=1}^N \left[ \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n \right] \left[ \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n \right]^{\mathsf{T}}
$$

\
Consider a binary classification problem, where the target variable $t = 1$ for class $C\_1$ and $t=0$ for $C\_2$.

We choose the logistic sigmoid function as the output activation function,
so that $0 \le y(\mathbf{x}, \mathbf{w}) \le 1$ can be interpreted as $p(C\_1\|\mathbf{x})$.

The target distribution then takes the form

$$
p(t|\mathbf{x}, \mathbf{w}) = y(\mathbf{x}, \mathbf{w})^t \left[ 1 - y(\mathbf{x}, \mathbf{w}) \right]^{1-t}
$$

Taking negtive logarithm of the likelihood function, we obtain the cross-entropy error function

$$
E(\mathbf{w}) = - \sum_{n=1}^N \left[ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right]
$$

where $y\_n \equiv y(\mathbf{x}\_n, \mathbf{w})$.

**Excercise 5.4**

We can take the possibility of mislabelling targets into account by involving a $\epsilon$ to indicate the probability of mislabelling.
Using the total probability rule, the target distribution of $C\_1$ is given by

$$
\begin{align*}
p(t=1 | \mathbf{x}, \mathbf{w}, \epsilon)
&= (1-\epsilon) y(\mathbf{x}, \mathbf{w}) + \epsilon \left[ 1 - y(\mathbf{x}, \mathbf{w}) \right] \\
&= \epsilon + (1 - 2 \epsilon) y(\mathbf{x}, \mathbf{w})
\end{align*}
$$

Therefore the target distribution can be collectively expressed by

$$
p(t|\mathbf{x}, \mathbf{w}, \epsilon)
= \left[ \epsilon + (1 - 2 \epsilon) y(\mathbf{x}, \mathbf{w}) \right]^t
\left[ 1 - \epsilon + (2 \epsilon - 1) y(\mathbf{x}, \mathbf{w}) \right]^{1-t}
$$

Corresponding cross-entropy function gives

$$
\begin{align*}
E(\mathbf{w}) 
&= - \sum_{n=1}^N \big\{ t_n \ln \left[ \epsilon + (1 - 2 \epsilon) y(\mathbf{x}_n, \mathbf{w}) \right] \\
&\qquad \qquad + (1 - t_n) \ln \left[ 1 - \epsilon + (2 \epsilon - 1) y(\mathbf{x}_n, \mathbf{w}) \right] \big\}
\end{align*}
$$

\
Now consider a multiclass classification problem with 1-of-$K$ coding scheme. We take the softmax as the output activation function, so that

$$
p(t_k=1|\mathbf{x}) = y_k(\mathbf{x}, \mathbf{w}) = \frac{\exp (a_k(\mathbf{x}, \mathbf{w}))}{\sum_j \exp (a_j(\mathbf{x}, \mathbf{w}))}
$$

which satisfies $0 \le y\_k \le 1$.

The error function then gives

$$
E(\mathbf{w}) = - \sum_{n=1}^N \sum_{k=1}^K t_{nk} \ln y_k(\mathbf{x}_n, \mathbf{w})
$$

**Excersize 5.6**: see derivation of (4.109)

### Parameter optimization

Because $E(\mathbf{w})$ is a smooth continuous function of $\mathbf{w}$, the minimum should occur at some stationary point.
However, there will be many stationary points, each of which could be a local extrema, a saddle point or a global minimum. 
In practice, it may not be necessary to find the global minimum, but it is necessary to check multiple local minima to get a sufficiently good one.

### Local quadratic approximation

Consider the Tylor expansion of $E(\mathbf{w})$ at a point $\widehat{\mathbf{w}}$

$$
E(\mathbf{w}) \simeq E(\widehat{\mathbf{w}}) + (\mathbf{w} - \widehat{\mathbf{w}})^{\mathsf{T}} \mathbf{b} + (\mathbf{w} - \widehat{\mathbf{w}})^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \widehat{\mathbf{w}})
\tag{5.28}
$$

where $\mathbf{b}$ is the gradient defined by

$$
\mathbf{b} \equiv \nabla E |_{\mathbf{w} = \widehat{\mathbf{w}}} 
$$

and $\mathbf{H}$ is the Hessian matrix defined by

$$
H_{ij} = \frac{\partial E}{\partial w_i \partial w_j} \bigg|_{\mathbf{w} = \widehat{\mathbf{w}}}
$$

Taking derivative of (5.28) with respect to $\mathbf{w}$, we obtain an approximation to the gradient

$$
E(\mathbf{w}) \simeq E(\mathbf{w}^\ast) + (\mathbf{w} - \mathbf{w}^\ast)^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast)
$$

**Excercise 5.10**

Let $\{\mathbf{u}\_i\}$ be the orthonomal eigenvectors of $\mathbf{H}$, such that

$$
\mathbf{H} \mathbf{u}_i = \lambda_i \mathbf{u}_i
$$

For any non-zero vector $\mathbf{v}$ in $\mathbb{R}^D$, we have

$$
\mathbf{v} = \sum_i \alpha_i \mathbf{u}_i = P \mathbf{a}
$$

where we defined

$$
\begin{align*}
P &= (\mathbf{u}_1, \dots, \mathbf{u}_D) \\
\mathbf{a} &= (\alpha_1, \dots, \alpha_D) \\
\end{align*}
$$

Then we have

$$
\mathbf{v}^{\mathsf{T}} \mathbf{H} \mathbf{v}
= \mathbf{a}^{\mathsf{T}} P^{\mathsf{T}} \mathbf{H} P \mathbf{a}
= \mathbf{a}^{\mathsf{T}} \operatorname{diag} (\lambda_1, \dots, \lambda_D) \mathbf{a}
= \sum_i \lambda_i \alpha_i^2
$$

Since $\{ \alpha\_i \}$ is defined by arbitrary non-zero $\mathbf{v}$, we see that $\mathbf{H}$ is postive definite if and only if all $\lambda\_i$ is postive.

Suppose $\mathbf{w}^\ast$ is a stationary point of $E(\mathbf{w})$, at which the gradient is zero. Then we have

$$
\nabla E (\mathbf{w}) \simeq \mathbf{b} + \mathbf{H} (\mathbf{w} - \widehat{\mathbf{w}})
$$

**Excercise 5.12**

And when $\mathbf{H}$ is postive definite, in the neighbor of $\mathbf{w}^\ast$ we have

$$
E(\mathbf{w}) - E(\mathbf{w}^\ast) = (\mathbf{w} - \mathbf{w}^\ast)^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast) > 0
$$

which means $\mathbf{w}^\ast$ is a minimum.

For the univariate case, the stationary point will be a minimum if 

$$
\frac{\partial^2 E}{\partial w^2} > 0
$$

### Use of gradient information

This section tries to illustrate that the use of gradient can speed up optimization.

### Gradient descent optimization

Batch methods: use the whole data set at once. E.g. gradient descent (aka steepest descent).

Better batch methods: conjugate gradients, quasi-Newton methods. Ensures monotonicly decreasing error.

On-line methods: sequential gradient descent or stochastic gradient descent. Cycling through each data point in sequence or selecting points at random with replacement.

## Error backpropagation

Error backpropagation (a.k.a backprop): an efficient technique to evaluate the garadient of error functions, 

### Evaluation of error-function derivatives

In a general feed-forward network, each activation is computed with

$$
a_j = \sum_i w_{ji} z_i \tag{5.48}
$$

Here $z\_i$ can be an input or an activation of unit i which sends a connection to unit $j$.
The activation is then is transformed into $z\_j$ by a nonlinear function $h(\cdot)$ so that $z\_j = h(a\_j)$.
This process is often called **forward propagation**.

Let $E\_n$ denote the error associated with pattern n.
Consider the derivative of $E\_n$ with respective to $w\_{ji}$, which can be written as

$$
\frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j} \frac{\partial a_j}{\partial w_{ji}}
$$

in which $w\_{ji}$ is the weight of connection from unit $i$ to unit $j$.

Define

$$
\delta_j \equiv \frac{\partial E_n}{\partial a_j}
$$

which is often called the **error**.

By

$$
\frac{\partial a_j}{\partial w_{ji}} = z_i
$$

we can write

$$
\frac{\partial E_n}{\partial w_{ji}} = \delta_j z_i \tag{5.53}
$$

For output units $k$, provided that the output-unit activation function is a canonical link, we have

$$
\delta_k = y_k - t_k
$$

This is the reason why $\delta\_k$ is called error.

For hidden units $j$, we can write

$$
\delta_j \equiv \frac{\partial E_n}{\partial a_j}
= \sum_k \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_j}
$$

where $a\_k$ are activations of the units that unit $j$ sends connections to. Note that unit $k$ here could be another hidden unit or the output unit.

By

$$
\begin{align*}
\frac{\partial E_n}{\partial a_k} &= \delta_k,
&\frac{\partial a_k}{\partial a_j} &= w_{kj} h^\prime(a_j)
\end{align*}
$$

the **backpropagation** formula is then given by

$$
\delta_j = h^\prime (a_j) \sum_k w_{kj} \delta_k \tag{5.56}
$$

![backprop](/assets/images/prml/5.7.png)

The backpropagation procedure is shown as follows:

1. Run forward propagation to find activations of all units.
2. Evaluate $\delta\_k$ for output units.
3. Backpropagate to evaluate $\delta\_j$
4. Calculate the recursively derivatives using (5.53)

In practice, the evaluation with backpropagation is a dynamic programming problem, which can be done either in a top-down fashion or in a bottom-up fashion.

The derivative of total error can be summed up by

$$
\frac{\partial E}{\partial w_{ji}} = \sum_n \frac{\partial E_n}{\partial w_{ji}}
$$

### A simple example

Consider a two-layer network for regression.

The error function associated with pattern $n$ gives

$$
E_n = \frac{1}{2} \sum_{k=1}^K (y_k - t_k)^2
$$


The output units use the identity activation function $y\_k = a\_k$ and the hidden units uses

$$
h(a) = \tanh (a) = \frac{e^{2a} - 1}{e^{2a} + 1}
$$

$h$ has a property that

$$
h^\prime (a) = 1 - h(a)^2
$$

Then the forward propagation is shown by

$$
\begin{align*}
a_j &= \sum_{i=0}^D w_{ji}^{(1)} x_i \\
z_j &= \tanh(a_j) \\
y_k &= \sum_{j=0}^{M} w_{kj}^{(2)} z_j \\
\end{align*}
$$

For output unit $k$

$$
\delta_k = y_k - t_k
$$

For hidden unit $j$

$$
\delta_j = (1 - z_j^2) \sum_{k=1}^K w_{kj}^{(2)} \delta_k
$$

Finally, the derivatives with respect to the parameters are obtained by

$$
\frac{\partial E_n}{\partial w_{ji}^{(1)}} = \delta_j x_i, \quad
\frac{\partial E_n}{\partial w_{kj}^{(2)}} = \delta_k z_j
$$

### Efficiency of backpropagation

Let $W$ be the number of weights and baises in the network.

Both forward propagation and backpropagation perform a multiplication and an addition on each parameter, which requries $O(W)$ operations.
In a non-sparse network, the number of weights is much greater than the number of units so that the cost of activation functions is relatively small.
The overall cost is asymptotically $O(W)$.

An alternative approach to evaluate the derivatives is the finite difference

$$
\frac{\partial E_n}{\partial w_{ji}} 
= \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji})}{\epsilon} + O(\epsilon)
$$

By making $\epsilon$ sufficiently small, we get a good approximation of the derivatives.

A more accurate method is the symmetric **central differences** which takes the form

$$
\frac{\partial E_n}{\partial w_{ji}} 
= \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji} - \epsilon)}{2 \epsilon} + O(\epsilon^2)
$$

**Excercise 5.14**

This method cancels the first order error $O(\epsilon)$, which can be seen by taking difference of the Taylor expansions

$$
\begin{align*}
E_n(w_{ji} + \epsilon) &= E_n(w_{ji}) + \frac{\partial E_n}{\partial w_{ji}} \epsilon + O(\epsilon^2) \\
E_n(w_{ji} - \epsilon) &= E_n(w_{ji}) - \frac{\partial E_n}{\partial w_{ji}} \epsilon + O(\epsilon^2) \\
\end{align*}
$$

With finite difference, evaluation of the derivative to a single parameter requires $O(W)$ operations, and hence evaluation of total derivative costs $O(W^2)$ operations.

Although numerical differentiation has high complexity, it is useful to verify the correctness of implementations of other methods.

### The Jacobian matrix

Consider the Jacobian matrix whose elements

$$
J_{ki} = \frac{\partial y_k}{\partial x_i}
$$

where $y\_k$ are outputs and $x\_i$ are inputs.

It is a measure of local sensitivity of outputs to changes in each input variables. This can be expressed as

$$
\Delta y_k \simeq \sum_i \frac{\partial y_k}{\partial x_i} \Delta x_i
$$

where $\|\Delta x\_i\|$ are sufficiently small.

The evaluation of the Jacobian matrix

$$
\begin{align*}
J_{ki} = \frac{\partial y_k}{\partial x_i}
= \sum_j \frac{\partial y_k}{\partial a_j} \frac{\partial a_j}{\partial x_i}
= \sum_j w_{ji} \frac{\partial y_k}{\partial a_j}
\end{align*}
$$

where units $j$ are those that the input unit $x\_i$ sends connections to.

To evaluate the $\partial y\_k / \partial a\_j$, we use backpropagation to give

$$
\frac{\partial y_k}{\partial a_j}
= \sum_l \frac{\partial y_k}{\partial a_l} \frac{\partial a_l}{\partial a_j}
= h^\prime (a_j) \sum_l w_{lj} \frac{\partial y_k}{\partial a_l}
$$

where units $l$ are those that the unit $j$ sends connections to.

Repeat the backpropagation procedure recursively until $a\_l$ are output activations. Then for sigmoidal output activation function we have

$$
\frac{\partial y_k}{\partial a_l} = I_{kl} \sigma^\prime (a_l)
$$

and for softmax activation function we have

$$
\frac{\partial y_k}{\partial a_l} = y_k (I_{kl} - y_l)
$$

To evaluate the Jacobian matrix, we first perform forward propagation to calculate all the activations.
Then recursively we evaluate $J\_{ki}$ starting from ${\partial y\_k}/{\partial a\_l}$ until ${\partial a\_j}/{\partial x\_i}$.

**Exercise 5.15**

An alternative forward propagation algorithm can also be used. Consider a two-layer network, the Jocobian can be written as

$$
\begin{align*}
J_{ki} &= \sum_{j, l} \frac{\partial y_k}{\partial a_l} \frac{\partial a_l}{\partial a_j} \frac{\partial a_j}{\partial x_i} \\
\end{align*}
$$

where units $j$ are hidden, and units $l$ are the output units.
In a forward manner, we first evaluate $\partial a\_j / \partial x\_i$, follwed by $\partial a\_l / \partial a\_j$, until the outpupt unit $\partial y\_k / \partial a\_l$.
This can be generalized to networks with more layers.

Formally, it can be written as the following iteration formula:

$$
\frac{\partial a_k}{\partial x_i} = \beta_k = \sum_{j} w_{kj} \alpha_j,
\quad \frac{\partial z_k}{\partial x_i} = \alpha_k = h^\prime (a_k) \beta_k
$$

where units $j$ sends connections to unit $k$. The iteration starts from the first layer of hidden units j with

$$
\beta_j = w_{ji}
$$

And it ends at

$$
\frac{\partial y_k}{\partial x_i} = \alpha_k
$$

where units $k$ corresponds to the output units.

\
Note that, the Jacobian can also be evaluated using the central difference

$$
\frac{\partial y_k}{\partial x_i} = \frac{y_k(x_i+\epsilon) - y_k(x_i-\epsilon)}{2 \epsilon} + O(\epsilon^2)
$$

## The Hessian matrix

Hessian matrix is typically used in a second-order optimization algorithm for faster convergence.

This section introduces several methods for evaluating the Hessian matrix.

### Diagonal approximation

The inverse of a diagnal marix is trivial to solve. The diagonal approximation simply drops off-diagonal elements of Hessian so that it becomes diagonal.

Consider the addtive error function $E = \sum\_n E\_n$ where $E\_n$ is the error associated with pattern $n$.

Following (5.48) and (5.47), the diagonal elements of the Hessian for pattern $n$ can be written

$$
\frac{\partial^2 E_n}{\partial w_{ji}^2} = \frac{\partial^2 E_n}{\partial a_j^2} z_i^2
$$

To evaluate ${\partial^2 E\_n}/{\partial a\_j^2}$, use the backpropagation formula (5.56) by taking derivative with repect to $a\_j$ again

$$
\frac{\partial^2 E_n}{\partial a_{j}^2}
= h^{\prime\prime} (a_j) \sum_k w_{kj} \frac{\partial E_n}{\partial a_k}
+ h^\prime (a_j) \sum_k w_{kj} \frac{\partial}{\partial a_j} \left( \frac{\partial E_n}{\partial a_{k}} \right)
$$

where units $j$ sends connections to units $k$.

> The topology of network: $z\_i \to z\_j=h(a\_j) \to y\_k=h(a\_k) \to E\_n$

Note that

$$
\begin{align*}
\frac{\partial^2 E_n}{\partial a_{k} \partial a_j}
&= \sum_{k^\prime} \frac{\partial^2 E_n}{\partial a_{k} \partial a_{k^\prime}}
\frac{\partial a_{k^\prime}}{\partial a_j} \\
&= \sum_{k^\prime} \frac{\partial^2 E_n}{\partial a_{k} \partial a_{k^\prime}}
h^\prime(a_j) w_{k^\prime j}
\end{align*}
$$

in which $k$ and $k^\prime$ are units of the same set.

Back-substituting, we have

$$
\frac{\partial^2 E_n}{\partial a_{j}^2}
= h^{\prime\prime} (a_j) \sum_k w_{kj} \frac{\partial E_n}{\partial a_k}
+ h^\prime (a_j)^2 \sum_k \sum_{k^\prime} w_{kj} w_{k^\prime j} \frac{\partial^2 E_n}{\partial a_k \partial a_{k^\prime}}
$$

By further neglecting the off-diagonal elements, we obtain

$$
\frac{\partial^2 E_n}{\partial a_{j}^2}
= h^{\prime\prime} (a_j) \sum_k w_{kj} \frac{\partial E_n}{\partial a_k}
+ h^\prime (a_j)^2 \sum_k w_{kj}^2 \frac{\partial^2 E_n}{\partial a_k^2}
\tag{5.81}
$$

Evaluation for a single diagonal element costs $O(W)$ and a full Hessian $O(W^2)$.

In practice, the Hessian is typically non-diagonal.

### Outer product approximation

Consider a regression problem with identity output activation function, whose error function takes the form

$$
E = \frac{1}{2} \sum_{n=1}^N ( y_n - t_n)^2
$$

The first order derivative gives

$$
\nabla E = \sum_{n} (y_{n} - t_{n}) \nabla y_{n}
$$

The second order derivative gives

$$
\mathbf{H} = \sum_{n} \nabla y_{n} \nabla y_{n}^{\mathsf{T}} + \sum_{n} (y_{n} - t_{n}) \nabla \nabla y_n
\tag{5.83}
$$

If the output of a trained network $y\_n$ is very close to $t\_n$, the second term will be small and can be neglected.

More generally, consider the squared loss function given by

$$
E = \frac{1}{2} \iint [ y(\mathbf{x}, \mathbf{w}) - t]^2 p(\mathbf{x}, t) \,d\mathbf{x} dt
$$

Taking derivative with respect to $w\_r$ and $w\_s$ in order, we have

$$
\frac{\partial E^2}{\partial w_r \partial w_s} 
= \iint \frac{\partial y}{\partial w_r} \frac{\partial y}{\partial w_s} p(\mathbf{x}) \,d\mathbf{x}
+ \iint \frac{\partial^2 y}{\partial w_r \partial w_s} \left\{ y(\mathbf{x}, \mathbf{w}) - \operatorname{E}_t [t|\mathbf{x}] \right\} p(\mathbf{x}) \,d\mathbf{x}
$$

In section 1.5.5, we see the optimal function that minizes the sum-of-squares loss is conditional mean of the target.
In this case the second term will vanish and we obtain

$$
\frac{\partial E^2}{\partial w_r \partial w_s} = \operatorname{E}_\mathbf{x} \left[ \frac{\partial y}{\partial w_r} \frac{\partial y}{\partial w_s} \right]
$$

from which we see when $y(\mathbf{x}, \mathbf{w})$ is trained close enough to the conditional mean of the target,
and $N$ is sufficiently large we obtain the **Levenberg-Marquardt** approximation or **outer product** approximation 

$$
\mathbf{H} \simeq \sum_{n=1}^N \mathbf{b}_n \mathbf{b}_n^{\mathsf{T}}
\tag{5.84}
$$

where $\mathbf{b}\_n \equiv \nabla y\_n = \nabla a\_n$. 

Using backpropagation, evaluation of the first derivatives costs $O(W)$. The overall cost of the Hessian thus is $(W^2)$.

Note that, this approximation is only valid when a network has been trained well enough.

**Excercise 5.16**

For regression problems with multiple target variables, the error function can be written

$$
E = \frac{1}{2} \sum_{n=1}^N \| \mathbf{y}_n - \mathbf{t}_n \|^2 
= \frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K ( \mathbf{y}_{nk} - \mathbf{t}_{nk} )^2 
$$

The Hessian takes the form

$$
\mathbf{H} = \sum_{n,k} \nabla y_{nk} \nabla y_{nk}^{\mathsf{T}} + \sum_{n,k} (y_{nk} - t_{nk}) \nabla \nabla y_n
$$

Thereby the approximation is given by

$$
\mathbf{H} \simeq \sum_{n,k} \mathbf{b}_{nk} \mathbf{b}_{nk}^{\mathsf{T}}
$$

where $\mathbf{b}\_{nk} \equiv \nabla y\_{nk}$

For a binary classification problem, the cross-entropy error function is given

$$
E(\mathbf{w}) = - \sum_{n=1}^N \left[ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right]
$$

With the logstic sigmoid as the output unit activation function, the first order derivative gives

$$
\nabla E = \sum_n (y_n - t_n) \nabla a_n
$$

The Hessian gives

$$
\mathbf{H} = \sum_n y_n (1-y_n) \nabla a_n (\nabla a_n)^{\mathsf{T}}
+ \sum_n (y_n - t_n) \nabla \nabla a_n
$$

The approximation takes the form

$$
\mathbf{H} \simeq \sum_n y_n (1-y_n) \mathbf{b}_n \mathbf{b}_n^{\mathsf{T}}
$$

where $\mathbf{b}\_n \equiv \nabla a\_n$

In the case of multiclass problem, we have a similar result given by

$$
\mathbf{H} \simeq \sum_{n,k} y_{nk} (1-y_{nk}) \mathbf{b}_{nk} \mathbf{b}_{nk}^{\mathsf{T}}
$$

### TODO: more methods
