---
layout: posts
title: 'PRML Ch2 - Linear Models for Regression - Part I'
---
- [Ch3 Linear Models for Regression](#ch3-linear-models-for-regression)
  - [3.1. Linear basis function models](#31-linear-basis-function-models)
    - [3.1.1. Maximum likelihood and least squares](#311-maximum-likelihood-and-least-squares)
    - [3.1.2. Geometry of least sqaures](#312-geometry-of-least-sqaures)
    - [3.1.3. Sequential learning](#313-sequential-learning)
    - [3.1.4. Regularized least squares](#314-regularized-least-squares)
    - [3.1.5. Multiple outputs](#315-multiple-outputs)
  - [3.2. The bais-variance decomposition](#32-the-bais-variance-decomposition)
  - [3.3. Bayesian linear regression](#33-bayesian-linear-regression)
    - [3.3.1. Parameter distribution](#331-parameter-distribution)
    - [3.3.2. Predictive distribution](#332-predictive-distribution)
    - [3.3.3. Equivalent kernel](#333-equivalent-kernel)

# Ch3 Linear Models for Regression

This chapter discusses a class of regression models that are linear functions of the parameters.

## 3.1. Linear basis function models


A linear regression model takes the form

$$
y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{j=1}^{M-1} w_j \phi_j(\mathbf{x})
$$

where $\mathbf{x} = (x\_1, \dots, x\_{M-1})^{\mathsf {T}}$, $\mathbf{w}$ is a vector of parameters defined by $\mathbf{w} = (w\_0, w\_1, \dots, w\_{M-1})^{\mathsf {T}}$,
$\phi\_j (\mathbf{x})$ is a nonlinear **basis function** and $w\_0$ is the **bias** parameter.
Functions of such form are called linear models.

$\phi\_j (\cdot)$ is required to be nonlinear since if not, the linearity in it can always
be expressed by $\mathbf{w}$.

By adding a dummy basis function $\phi\_0 (\mathbf{x}) = 1$, we can write

$$
y(\mathbf{x}, \mathbf{w}) = \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x})
= \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi}(\mathbf{x})
$$

where $\boldsymbol{\phi} = (\phi\_0, \dots, \phi\_{M-1})^{\mathsf {T}}$. 

There are many possible choices for the basis functions:

The polynomial basis functions given by 

$$
\phi_j = x^j
$$

A variant of polynomial basis is the spline functions.

The Gaussian basis functions

$$
\phi_j (x) = \exp \left\{ - \frac{(x - \mu_j)^2}{2 s^2} \right\}
$$

The sigmoidal basis functions

$$
\phi_j (x) = \sigma \left( \frac{x - \mu_j}{s} \right)
$$

where $\sigma(x)$ is the logistic sigmoid function defined by 

$$
\sigma(a) = \frac{1}{1 + e^{-a}}
$$

Another equivalent basis function to the sigmoidal is the $\tanh$ basis given by

$$
\tanh (a) = 2 \sigma(2a) - 1 = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

The Fourier basis functions which can be seen as ...

### 3.1.1. Maximum likelihood and least squares

Assume that the target variable $t$ is obtained from a deterministic function $y(\mathbf{x}, \mathbf{w})$ with an additive Gaussian noise denoted by $\epsilon$, so that

$$
t = y(\mathbf{x}, \mathbf{w}) + \epsilon
$$

where $\epsilon$ is distributed according to a zero-mean Gaussian distribution with precision $\beta$. Thus we have

$$
p(t|\mathbf{x}, \mathbf{w}, \beta) = \mathcal{N} (t|y(\mathbf{x}, \mathbf{w}), \beta^{-1})
$$

Recall that, for a squared loss function, the optimal prediction based on new input $\mathbf{x}$ is given by $\operatorname{E}[t\|\mathbf{x}]$. In this case

$$
\operatorname{E}[t|\mathbf{x}] = y(\mathbf{x}, \mathbf{w})
$$

Given a data set of inputs $\mathbf{X} = \{\mathbf{x}\_1, \dots, \mathbf{x}\_N\}$
and the corresponding targets $\mathbf{t} = (t\_1, \dots, t\_N)^{\mathsf {T}}$,
the likelihood function is

$$
p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta)
= \prod_{n=1}^N \mathcal{N} (t_n|\mathbf{w}^{\mathsf {T}} \boldsymbol{\phi}(\mathbf{x}_n), \beta^{-1})
$$

Taking the logarithm

$$
\ln p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta)
= \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2\pi) - \beta E_D(\mathbf{w})
$$

where $E\_D(\mathbf{w})$ is the sum-of-squres error defined by

$$
E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left[ t_n - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^2
$$

Therefore, maximizing the likelihood function with respect to $\mathbf{w}$ is equivalent to minimizing the sum-of-squres error.

By matrix partitioning

$$
\begin{align*}
\sum_{n=1}^N \left[ t_n - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^2
&= \| \begin{pmatrix}
t_1 - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_1) \\
\vdots \\
t_N - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_N) \\
\end{pmatrix} \|^2 \\
&= \| \mathbf{t} - \begin{pmatrix}
\boldsymbol{\phi} (\mathbf{x}_1) \\
\vdots \\
\boldsymbol{\phi} (\mathbf{x}_N) \\
\end{pmatrix} \mathbf{w} \|^2 \\
&= \| \mathbf{t} - \boldsymbol{\Phi} \mathbf{w} \|^2
\end{align*} 
$$

Thus we have

$$
E_D(\mathbf{w}) = \frac{1}{2} \| \mathbf{t} - \boldsymbol{\Phi} \mathbf{w} \|^2
$$

Here we defined $N \times M$ matrix $\boldsymbol{\Phi}$, which is also called the **design matrix**, whose elements are given by $\boldsymbol{\Phi}\_{ij} = \phi\_{j-1} (\mathbf{x}\_i)$, so that

$$
\boldsymbol{\Phi} = 
\begin{pmatrix}
\phi_0 (\mathbf{x}_1) & \phi_1 (\mathbf{x}_1) & \cdots & \phi_{M-1} (\mathbf{x}_1) \\
\phi_0 (\mathbf{x}_2) & \phi_1 (\mathbf{x}_2) & \cdots & \phi_{M-1} (\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0 (\mathbf{x}_N) & \phi_1 (\mathbf{x}_N) & \cdots & \phi_{M-1} (\mathbf{x}_N) \\
\end{pmatrix}
$$

Taking gradient of the log likelihood function with respect to $\mathbf{w}$, we have

$$
\nabla_{\mathbf{w}} \ln p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta)
= \beta \boldsymbol{\Phi}^{\mathsf{T}} (\mathbf{t} - \boldsymbol{\Phi} \mathbf{w})
$$

Setting the gradient to 0 gives

$$
\mathbf{w}_{\textrm{ML}} = (\boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf {T}} \mathbf{t}
$$

which is known as the **normal equations** for the least squares problem. The matrix

$$
\boldsymbol{\Phi}^{\dagger} = (\boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf{T}}
$$

is known as the **Moore-Penrose pseudo-inverse** of $\boldsymbol{\Phi}$.
It is a generalization of matrix inverse to nonsquare matrices. Indeed,
if $\boldsymbol{\Phi}$ is square and invertible, we have $\boldsymbol{\Phi}^{\dagger} = \boldsymbol{\Phi}^{-1}$.

In practice, numerical difficulties arise when $\boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi}$ is nearly singular, which may result in large maginitude of parameters. 
A particular case is when two columns of $\boldsymbol{\Phi}$ are almost linearly dependent.

A technique to address the issue is the singular value decomposition (SVD).
Let $A = \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi}$. Then the pseudo-inverses matrix of $A$ is given by $A^+ = V\_r D^{-1} U\_r^{\mathsf{T}}$, which satisfies $A^+A = I$.
Alternatively, using a regularization term can also avoid the singularity.

Setting the log likelihood function with respect to $\beta$ to zero gives

$$
\frac{1}{\beta_{\textrm{ML}}} = \frac{1}{N} \sum_{n=1}^N \left[ t_n - \mathbf{w}_{\textrm{ML}}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^2
$$

We see that $\beta$ is derived from the residual variance.

### 3.1.2. Geometry of least sqaures

Consider $\mathbf{t}$ as a target vector in an N-dimensional space.
Define $\boldsymbol{\varphi}\_j = \left( \phi\_j (\mathbf{x}\_1), \dots, \phi\_j (\mathbf{x}\_N) \right)^{\mathsf {T}}, j=0, \dots, M-1$,
which is the column $j+1$ of $\boldsymbol{\Phi}$.

Let $S$ be a subspace that is spaned by $\{\boldsymbol{\varphi}\_j\}$.

Defining a vector $\mathbf{y} = (y(\mathbf{x}\_1, \mathbf{w}), \dots, y(\mathbf{x}\_N, \mathbf{w}))^{\mathsf {T}}$, 
we see $\mathbf{y}$ is a linear combination of the columns of $\boldsymbol{\Phi}$ such that

$$
\mathbf{y} = \boldsymbol{\Phi} \mathbf{w}
$$

and $\mathbf{w}$ is the coordinate of $\mathbf{y}$ in $S$.

**Excercise 3.2**

Minimizing the sum-of-squares error is equivalent to find a $\mathbf{y}$ that lies closest to $\mathbf{t}$. 
Hence $\mathbf{y}$ should be the orthogonal projection of $\mathbf{t}$ onto $S$,
which satifies $(\mathbf{y} - \mathbf{t}) \perp \{\boldsymbol{\varphi}\_j\}, \quad j=0, \dots, M-1$

Therefore, we have

$$
\boldsymbol{\Phi}^{\mathsf {T}} (\mathbf{y} - \mathbf{t})
= \boldsymbol{\Phi}^{\mathsf {T}} (\boldsymbol{\Phi} \mathbf{w} - \mathbf{t})
= \mathbf{0}
$$

which is exactly the normal equation.

If $\boldsymbol{\Phi}$ has $M$ independent columns, 
using the QR decompostition we can write $\boldsymbol{\Phi}$ as

$$
\boldsymbol{\Phi} = QR
$$

where $Q$ is $N \times M$ matrix whose columns form a orthonormal set and $R$ is a invertible matrix.  Then we have

$$
\boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi} 
= R^{\mathsf {T}} Q^{\mathsf {T}} Q R
= I_M
$$

which shows that $\boldsymbol{\Phi}^{\mathsf {T}}\boldsymbol{\Phi}$ is invertible. Therefore the solution is given by $\mathbf{w}\_{\mathrm{ML}}$.

### 3.1.3. Sequential learning

Sequential algorithms, a.k.a on-line algorithms, process a single data points or a small batch of data points at one time and thus is applicable for large data sets and real-time applications.

A technique for sequential learning is the **stochastic gradient descent** (a.k.a. sequential gradient descent).

If an error function takes the form of a sum of errors such that $E = \sum\_n E\_n$.
After observing each pattern $n$, we can update the parameter using

$$
\mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} - \eta \nabla E_n
$$

where $\eta$ is the learning rate parameter, which can be interpreted as the step along the negtive gradient.
Valueo of $\eta$ needs to be chosen carefully to ensure the algorithm is convergent.

In the case of linear regression, the algorithm gives

$$
\mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} + \eta \left( t_n - \mathbf{w}^{(\tau)} \boldsymbol{\phi}(\mathbf{x}_n) \right) \boldsymbol{\phi}(\mathbf{x}_n)
$$

This is known as the **least-mean-squares** or **LMS** algorithm.

### 3.1.4. Regularized least squares

To suppress over-fitting, we can add a regularization term so that the total error gives

$$
E_D (\mathbf{w}) + \lambda E_W(\mathbf{w})
$$

where $\lambda$ is the regularization coefficient.

A commonly used regularizers is the  **L2 regularizer** given by

$$
E_W(\mathbf{w}) = \frac{1}{2} \mathbf{w}^{\mathsf {T}} \mathbf{w}
$$

It is called **weight decay**, since it drives parameter values to small magnitude. In terms of statistics, this is a process of **parameter shrinkage**.

With L2 regularizer, the total error function is written as

$$
\frac{1}{2} \sum_{n=1}^N \left[ t_n - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^2
+ \frac{\lambda}{2} \mathbf{w}^{\mathsf {T}} \mathbf{w}
$$

Setting the gradient with respect to $\mathbf{w}$ to zero, we obtain

$$
\mathbf{w} = (\lambda I + \boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf {T}} \mathbf{t}
$$

In general, regularizers take the form

$$
\sum_{j=0}^{M-1} |w_j|^q
$$

When $q=1$, it is known as the **L1** regularizer or **lasso** regularizer. It has a property that when $\lambda$ is large, some of the parameters tends to 0,
giving a **sparse** model in which some basis functions have no effect.

**Excercise 3.5**

To see this, we first show that minimizing the regularized error given by

$$
\frac{1}{2} \sum_{n=1}^N \left[ t_n - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^2
+ \frac{\lambda}{2} \sum_{j=0}^{M-1} |w_j|^q
$$

is equivalent to minimizing the unregularized sum-of-sqaures error $E\_D(\mathbf{w})$ subject to the constraint

$$
\sum_{j=0}^{M-1} |w_j|^q \le \eta
$$

where $\eta$ is a carefully chosen constant.

Introduce a Lagrange multiplier $\mu$ with the Lagragian function given by

$$
L(\mathbf{w}, \mu) = 
\frac{1}{2} \sum_{n=1}^N \left[ t_n - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^2
- \mu \left( \eta - \sum_{j=0}^{M-1} |w_j|^q \right)
$$

The KKT conditions are

$$
\begin{align*}
\mu &\ge 0 \\
\eta - \sum_{j=0}^{M-1} |w_j|^q &\ge 0 \\
\mu \left( \eta - \sum_{j=0}^{M-1} |w_j|^q \right) &= 0 \\
\end{align*}
$$

Setting the derivative with respect to $\mathbf{w}$ to zero, we have

$$
\frac{\partial}{\partial \mathbf{w}} 
\left\{  
\frac{1}{2} \sum_{n=1}^N \left[ t_n - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^2
\right\}
+ \mu \frac{\partial}{\partial \mathbf{w}} 
\left( \sum_{j=0}^{M-1} |w_j|^q \right)
= 0
$$

Setting $\mu = \lambda/2$, this derivative takes the same form as that of the unregularized error.
Denote the solution by $w\_j^\ast (\mu)$, which is a function of $\mu$.

When $\lambda = 0$, the error function becomes unregularized,
so $\eta$ is required to be large enough to contain the optimal results, so that $\eta \ge \sum\_{j=0}^{M-1} \|w\_j^\ast (\lambda / 2)\|^q$.

When $\lambda > 0$, by the KKT condition, we require $\eta = \sum\_{j=0}^{M-1} \|w\_j^\ast (\lambda / 2)\|^q$.

Therefore we take

$$
\eta = \sum_{j=0}^{M-1} |w_j^\ast \left( \frac{\lambda}{2} \right)|^q
$$

Back to the sparse property of lasso. The figure below illustrates how different values of $q$ affect the optimal solution in the case of two parameters:
Blue curves indicate the contours of the error function, and the center dot represents the optimal solution without constraints.
Colored regions are imposed by the inequality constraints. On the left is the L2 regularizer and on the right the lasso.

![](/assets/images/prml/3.4.jpeg)

Large $\lambda$ generally suppress the matgnitude of parameters, leanding to small $\eta$ and small constraint region. 
When $\lambda$ is sufficiently large, the global optimal solution will lie outside the constraint region. 
Compared to the quadratic regularizer, the lasso is less smooth and more likely to drive some of its components to zero.

Thereby we see that besides the number of basis functions, regularization can also effectively control the model complecity.

Moreover, when the data have large offsets, it is preferable not to regularize the bias parameter, so that the regularizer takes the form

$$
\sum_{j=1}^{M-1} |w_j|^q
$$

As we will see in Bayesian framework, this corresponds to a noninformatice prior on the bias parameter.

### 3.1.5. Multiple outputs

Consider a target vector $\mathbf{t}$, with $K > 1$ components.

A simple way to construct a linear regression model of $\mathbf{t}$ is by constructing independent models of each variable, with independent sets of parameters and basis functions.

A more common way is to share the same set of basis functions, so that

$$
\mathbf{y} (\mathbf{x}, \mathbf{w}) = \mathbf{W}^{\mathsf {T}} \boldsymbol{\phi}(\mathbf{x})
$$

Here $\mathbf{W} = (\mathbf{w}\_1 \cdots \mathbf{w}\_K)$ is a $M \times K$ matrix of parameters,
and $\boldsymbol{\phi} (\mathbf{x})$ is the vector of basis functions with $\phi\_0(\mathbf{x}) = 1$.

Suppose that the conditional distribution of $\mathbf{t}$ follows a Gassuain distribution, so that

$$
p(\mathbf{t} | \mathbf{x}, \mathbf{W}, \boldsymbol{\Sigma})
= \mathcal{N} (\mathbf{t} | \mathbf{W}^{\mathsf {T}} \boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\Sigma})
$$

The log likelihood function is given by

$$
\begin{align*}
\ln p(\mathbf{T} | \mathbf{X}, \mathbf{w}, \boldsymbol{\Sigma}) 
&= \sum_{n=1}^N \ln \mathcal{N} (\mathbf{t} | \mathbf{W}^{\mathsf {T}} \boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\Sigma}) \\
&= - \frac{NK}{2} \ln (2 \pi) - \frac{N}{2} \ln |\boldsymbol{\Sigma}| - \frac{1}{2} \sum_n \left[ \mathbf{t}_n -\mathbf{W}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right]^{\mathsf {T}}
\boldsymbol{\Sigma}^{-1} \left[ \mathbf{t}_n -\mathbf{W}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right] \\
\end{align*}
$$

where $\mathbf{T} = (\mathbf{t}\_1 \cdots \mathbf{t}\_N)^{\mathsf {T}}$ is a $N \times K$ matrix whose i-th row is $\mathbf{t}\_i^{\mathsf {T}}$.

**Excercise 3.6**

Setting derivative with respect to $\mathbf{W}$ to zero, we have

$$
- \frac{1}{2} \sum_n (\boldsymbol{\Sigma}^{-1} + (\boldsymbol{\Sigma}^{-1})^{\mathsf {T}}) 
\left( \mathbf{W}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) - \mathbf{t}_n \right) \boldsymbol{\phi} (\mathbf{x}_n)^{\mathsf {T}} = 0
$$

Solving the equation we obtain

$$
\mathbf{W}_{\textrm{ML}} = (\boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf {T}} \mathbf{T}
$$

which is equivalent to

$$
\mathbf{w}_k = (\boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\mathsf {T}} \mathbf{T}_{:,k}
$$

where $\mathbf{w}\_k$ are the parameters associated with target $t\_k$ and $\mathbf{T}\_{:,k}$ is the k-th column of $\mathbf{T}$ corresponding to the observations of $t\_k$.
Hence for multiple variables using the same set of basis function, the pseudo-inverse $\boldsymbol{\Phi}^\dagger$ only needs to compute once.

Refering to the results of Section 2.3.4, setting derivative with respect to $\boldsymbol{\Sigma}$ to zero gives

$$
\boldsymbol{\Sigma}_{\textrm{ML}} 
= \frac{1}{N} \sum_{n=1}^N (\mathbf{t}_n - \mathbf{W}_{\textrm{ML}}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n)) 
(\mathbf{t}_n - \mathbf{W}_{\textrm{ML}}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n))^{\mathsf {T}}
$$

## 3.2. The bais-variance decomposition

From a frequentist perspective, model complecity can be interpreted as the **bais-variance trade-off**.

Recall in Section 1.5.5, with the sqaured loss function, the optimal prediction is given by the conditional mean $\operatorname{E} [t\|\mathbf{x}]$,
in which $t$ is the target and $\mathbf{x}$ is the input. 
Here we denote $\operatorname{E} [t\|\mathbf{x}]$ by a function $h(\mathbf{x})$.

From Section 1.5.5, the expected squared loss is given by

$$
\operatorname{E} [L] 
= \int \left[ y(\mathbf{x}) - h(\mathbf{x}) \right]^2 p(\mathbf{x}) \,d\mathbf{x}
+ \iint \left[ h(\mathbf{x}) - t \right]^2 p(\mathbf{x}) \,d\mathbf{x}
$$

The first term is what we try to minimize by choosing an appropriate $y(\mathbf{x})$.
In principle, given infinite number of observations, we could make $y(\mathbf{x})$ converges to the true $h(\mathbf{x})$. 
However, data sets in real life only contain finite number of data points.

The second term is independent of $y(\mathbf{x})$, which arises from the intrinsic noise in the data and is the minimum achivable expected loss.

Suppose we model $h(\mathbf{x})$ with a function $y(\mathbf{x}, \mathbf{w})$ governed by the parameter $\mathbf{w}$. 

From a frequentist view, we are making a point estimation on $\mathbf{w}$ from a data set $D$.
Suppose there are a large number of data sets of size $N$, each of which is independently drawn from the distribution $p(t, \mathbf{x})$.
Based on each data set $D$, we obtain a prediction function $y(\mathbf{x}, D)$. The predictive performance/uncertainty of the model is then measured by taking average over all data sets.

> An **ensemble** basically means a large number of identical copies.

Now consider the squared difference

$$
\left\{ y(\mathbf{x}, D) - h(\mathbf{x}) \right\}^2
$$

By adding and subtracting a term $\operatorname{E}\_D [y(\mathbf{x}, D)]$, we have

$$
\left\{ y(\mathbf{x}, D) - \operatorname{E}_D [y(\mathbf{x}, D)] + \operatorname{E}_D [y(\mathbf{x}, D)] - h(\mathbf{x}) \right\}^2 \\
= \left\{ y(\mathbf{x}, D) - \operatorname{E}_D [y(\mathbf{x}, D)] \right\}^2
+ \left\{ \operatorname{E}_D [y(\mathbf{x}, D)] - h(\mathbf{x}) \right\}^2 \\
+ 2 \left\{ y(\mathbf{x}, D) - \operatorname{E}_D [y(\mathbf{x}, D)] \right\}
\left\{ \operatorname{E}_D [y(\mathbf{x}, D)] - h(\mathbf{x}) \right\}^2
$$

Taking the expectation over $D$, we obtain

$$
\operatorname{E}_D \left[ \left\{ y(\mathbf{x}, D) - h(\mathbf{x}) \right\}^2 \right] \\
= \left\{ \operatorname{E}_D [y(\mathbf{x}, D)] - h(\mathbf{x}) \right\}^2
+ \operatorname{E}_D \left[ \left\{ y(\mathbf{x}, D) - \operatorname{E}_D [y(\mathbf{x}, D)] \right\}^2 \right]
$$

The first term on the r.h.s is called the **squared bias**. It represents the gap between the average prediction over all data sets and the true regression function.

The second term is called the **variance**. It measures sensitivity of a model to different choices of data set.

Back-substituting in to the expected squared loss, we have

$$
\text{expected loss} = (\text{bias})^2 + \text{variance} + \text{noise}
$$

where

$$
\begin{align*}
(\text{bias})^2 &= \int \left\{ \operatorname{E}_D [y(\mathbf{x}, D)] - h(\mathbf{x}) \right\}^2 p(\mathbf{x}) \,d\mathbf{x} \\
\text{variance} &= \int \operatorname{E}_D \left[ \left\{ y(\mathbf{x}, D) - \operatorname{E}_D [y(\mathbf{x}, D)] \right\}^2 \right] p(\mathbf{x}) \,d\mathbf{x} \\
\text{noise} &= \iint \left[ h(\mathbf{x}) - t \right]^2 \,d\mathbf{x}d\mathbf{t} \\
\end{align*}
$$

Our goal is to minimize the expected loss. However, there's a trade-off between the bias and the variance. High flexibility typically means

- High variance
- Low bias
- Finly tuned to noise

Low flexibility typically means

- Low variance
- High bias
- Restricted ability to learn the true model.

Bias-variance decomposition is quite limited in practice, since it is based on an ensemble of data sets which is often impractical.

## 3.3. Bayesian linear regression

In Bayesian approach, model complexity can be determined using training data alone.

### 3.3.1. Parameter distribution

Suppose the noise precision $\beta$ is a known constant. 

Then the exponent of the likelihood function

$$
p(\mathbf{t} | \mathbf{X}, \mathbf{w})
= \prod_{n=1}^N \mathcal{N} (t_n | \mathbf{w}^{\mathsf {T}}\boldsymbol{\phi}(\mathbf{x}_n), \beta^{-1})
$$

is a quadratic form of $\mathbf{w}$, the conjugate prior is given by a Gaussian distribution

$$
p(\mathbf{w}) = \mathcal{N} (\mathbf{w} | \mathbf{m}_0, \mathbf{S}_0)
$$

**Excercise 3.7**

To obtain the posterior distribution, we inspect the exponent

$$
- \frac{1}{2} \mathbf{w}^{\mathsf {T}} \left[ \beta \sum_n \boldsymbol{\phi} (\mathbf{x}_n) \boldsymbol{\phi} (\mathbf{x}_n)^{\mathsf {T}} + \mathbf{S}_0^{-1} \right] \mathbf{w}
+ \mathbf{w}^{\mathsf {T}} \left[ \beta \sum_n \boldsymbol{\phi} (\mathbf{x}_n)t_n + \mathbf{S}_0^{-1} \mathbf{m}_0 \right]
+ \text{const}
$$

The posterior distribution is then given by

$$
p(\mathbf{w}|\mathbf{t}) = \mathcal{N} (\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N)
$$

where

$$
\begin{align*}
\mathbf{S}_N^{-1} &= \mathbf{S}_0^{-1} + \beta \boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi} \\
\mathbf{m}_N &= \mathbf{S}_N ( \mathbf{S}_0^{-1} \mathbf{m}_0 + \beta \boldsymbol{\Phi}^{\mathsf {T}} \mathbf{t} ) \\
\end{align*}
$$

Note that

- The maximum posterior solution corresponds to the mode so that $\mathbf{w}\_{\mathrm{MAP}} = \mathbf{m}\_N$
- If $\mathbf{S}\_0 = \alpha^{-1} I$, in the limit $\alpha \rightarrow 0$, the mean $\mathbf{m}\_N$ converges to the maximum likelihood solution $\mathbf{w}\_{\mathrm{ML}}$. The prior becomes noninformative.
- If $N=0$, the posterior degrades to the prior.

**Excercise 3.8**

To illustrate the sequential nature of Bayesian learning, we append a new point $(\mathbf{x}\_{N+1}, \mathbf{t}\_{N+1})$ to the data set, and treating the posterior of $N$ points as the prior for the new point, so that

$$
\begin{align*}
p(\mathbf{w}|\mathbf{t}_N) &= \mathcal{N} (\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N) \\
p(\mathbf{w}|t_{N+1}) &\propto p(\mathbf{w}|\mathbf{t}_N) p(t_{N+1} | \mathbf{w})
\end{align*}
$$

We obtain the new posterior distribution

$$
p(\mathbf{w}|t_{N+1}) = \mathcal{N} (\mathbf{w} | \mathbf{m}_{N+1}, \mathbf{S}_{N+1})
$$

where

$$
\begin{align*}
\mathbf{S}_{N+1}^{-1} &= \mathbf{S}_N^{-1} + \beta \boldsymbol{\phi} (\mathbf{x}_{N+1}) \boldsymbol{\phi} (\mathbf{x}_{N+1})^{\mathsf{T}} \\
\mathbf{m}_N &= \mathbf{S}_{N+1} \left[ \mathbf{S}_N^{-1} \mathbf{m}_N + \beta \boldsymbol{\phi} (\mathbf{x}_{N+1}) t_{N+1} \right] \\
\end{align*}
$$

which basically has the same form as the original posterior.

\
Consider a isotropic Gaussian prior given by

$$
p(\mathbf{w} | \alpha) = \mathcal{N} (\mathbf{w} | \mathbf{0}, \alpha^{-1}I)
$$

The corresponding posterior gives

$$
\begin{align*}
\mathbf{m}_N &= \beta \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf {T}} \mathbf{t} 
\tag {3.53} \\
\mathbf{S}_N^{-1} &= \alpha I + \beta \boldsymbol{\Phi}^{\mathsf {T}} \boldsymbol{\Phi} 
\tag {3.54}\\
\end{align*}
$$

Taking the logarithm, we have

$$
\ln p(\mathbf{w} | \mathbf{t}) = -\frac{\beta}{2} \sum_n \left\{ t_n - \mathbf{w}^{\mathsf {T}} \boldsymbol{\phi} (\mathbf{x}_n) \right\}^2 
- \frac{\alpha}{2} \mathbf{w}^{\mathsf {T}} \mathbf{w} + \text{const}
$$

in which $\text{const}$ concludes terms independent of $\mathbf{w}$.

Therefore, maximizing this posterior with respect to $\mathbf{w}$ is equivalent to minimizing the sum-of-squares error function with a quadratic regularizer, with regularization coefficient $\lambda = \alpha / \beta$.

It can be shown that in the limit $N \rightarrow \infty$, the posterior will become a Dirac delta function centered on the true patameter values.

A prior can also be chosen to correspond to a specific regularizer. A $q$-th order regularizer has the prior

$$
p(\mathbf{w}|\alpha) = \left[ \frac{q}{2} \left( \frac{\alpha}{2} \right)^{1/q} \frac{1}{\Gamma(1/q)} \right]^M
\exp \left( - \frac{\alpha}{2} \sum_j |w_j|^q \right)
$$

When $q=2$, it is the Gaussian prior.

### 3.3.2. Predictive distribution

To make predictions of $t$ for new value of $\mathbf{x}$, we evaluate the **predictive distribution** defined by

$$
p(t | \mathbf{t}, \mathbf{x}, \alpha, \beta) 
= \int p(t|\mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w}|\mathbf{t}, \alpha, \beta) \,d\mathbf{w}
$$

where $\mathbf{t} (t\_1, \dots, t\_N)^{\mathsf{T}}$.

Here the target distribution and posterior distribution are taken as

$$
\begin{align*}
p(t|\mathbf{x}, \mathbf{w}, \beta) &= \mathcal{N} (t | \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}), \beta^{-1} ) \\
p(\mathbf{w}|\mathbf{t}, \alpha, \beta) &= \mathcal{N} (\mathbf{w} | \mathbf{m}_N, \mathbf{S}_N) \\
\end{align*}
$$

**Excercise 3.10**

Using the result from the section 2.2.3, with changes of variables

$$
\begin{align*}
A &\rightarrow \boldsymbol{\phi} (\mathbf{x})^{\mathsf{T}}, &\mathbf{x} &\rightarrow \mathbf{w} , & b &\rightarrow \mathbf{0} \\
\mathbf{L}^{-1} &\rightarrow \beta^{-1}, & \boldsymbol{\mu} &\rightarrow \mathbf{m}_N, & \boldsymbol{\Lambda} &\rightarrow \mathbf{S}_N^{-1}
\end{align*}
$$

we obtain the predictive distribution given by

$$
p(t|\mathbf{x}, \mathbf{t}, \alpha, \beta) 
= \mathcal{N} (t|\mathbf{m}_N^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}), \sigma_N^2(\mathbf{x}))
\tag{3.58}
$$

where

$$
\sigma_N^2(\mathbf{x}) = \beta^{-1} + \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x})
$$

The first term represents the noise and the second term represents the uncertainty in $\mathbf{w}$. 

**Excercise 3.11**

It can be shown that $\sigma^2\_{N+1}(\mathbf{x}) \le \sigma^2\_N (\mathbf{x})$ by following steps.

Making subtraction

$$
\begin{align*}
\sigma^2_{N+1}(\mathbf{x}) - \sigma^2_N (\mathbf{x}) 
&= \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} (\mathbf{S}_{N+1} - \mathbf{S}_N) \boldsymbol{\phi}(\mathbf{x}) \\
\end{align*}
$$

Expanding $\mathbf{S}\_{N+1}$ and then using Woodbury identity

$$
\begin{align*}
\mathbf{S}_{N+1} - \mathbf{S}_N
&= \left[ \mathbf{S}_N^{-1} + \beta \boldsymbol{\phi} (\mathbf{x}_{N+1}) \boldsymbol{\phi} (\mathbf{x}_{N+1})^{\mathsf {T}} \right]^{-1} - \mathbf{S}_N \\
&= \mathbf{S}_N - \frac{\beta \mathbf{S}_N \boldsymbol{\phi} (\mathbf{x}_{N+1}) \boldsymbol{\phi}(\mathbf{x}_{N+1})^{\mathsf{T}} \mathbf{S}_N}{1 + \beta \boldsymbol{\phi}(\mathbf{x}_{N+1})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}_{N+1})} - \mathbf{S}_N \\
\end{align*}
$$

Back-sustituting

$$
\begin{align*}
\sigma^2_{N+1}(\mathbf{x}) - \sigma^2_N (\mathbf{x}) 
&= - \frac{\beta \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi} (\mathbf{x}_{N+1}) \boldsymbol{\phi}(\mathbf{x}_{N+1})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x})}
{1 + \beta \boldsymbol{\phi}(\mathbf{x}_{N+1})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}_{N+1})} \\
&= - \frac{\beta \left\| \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}_{N+1}) \right\| ^2}
{1 + \beta \boldsymbol{\phi}(\mathbf{x}_{N+1})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}_{N+1})} \le 0 \\
\end{align*}
$$

In the limit $N \rightarrow \infty$, $\sigma^2\_{N}$ converges to 0 and the target distribution becomes sharply peaked.

Note that, when using localized basis functions such as Gaussians, regions far from the centers will go to zero. 
However, the model can be very confident about these far regions, which is generally unexpected.
A Gaussian process can be adopted to avoid this problem.

**Excercise  3.12**

Now, suppose $\mathbf{w}$ and $\beta$ are both unknown. We take the Gaussian-gamma distribution as the prior such that

$$
p(\mathbf{w}, \beta) = \mathcal{N} (\mathbf{w} | \mathbf{m}_0, \beta^{-1}\mathbf{S}_0)
\mathrm{Gam}(\beta | a_0, b_0)
$$

Multiplying the prior with the likelihood function

$$
\begin{align*}
p(\mathbf{w}, \beta | \mathbf{t})
&= p(\mathbf{w}, \beta) p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta) \\
&\propto
\mathcal{N} (\mathbf{w} | \mathbf{m}_0, \beta^{-1}\mathbf{S}_0)
\mathrm{Gam}(\beta | a_0, b_0)
\prod_{n=1}^N \mathcal{N} (t_n | \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}_n), \beta^{-1} ) \\
\end{align*}
$$

By inspecting the product, we obtain the posterior distribution

$$
p(\mathbf{w}, \beta | \mathbf{t}) = \mathcal{N}(\mathbf{w}|\mathbf{m}_N, \beta^{-1}\mathbf{S}_N)
\mathrm{Gam} (\beta | a_N, b_N)
$$

in which

$$
\begin{align*}
\mathbf{S}_N^{-1} &= \mathbf{S}_0^{-1} + \boldsymbol{\Phi}^{\mathsf{T}} \boldsymbol{\Phi} \\
\mathbf{m}_N &= \mathbf{S}_N (\mathbf{S}_0^{-1} \mathbf{m_0} + \beta \boldsymbol{\phi}^{\mathsf{T}} \mathbf{t}) \\
a_n &= a_0 + \frac{N}{2} - 1 \\
b_n &= b_0 + \frac{1}{2} \mathbf{m}^{\mathsf{T}} \mathbf{S}_0^{-1} \mathbf{m} + \frac{1}{2} \mathbf{t}^{\mathsf{T}} \mathbf{t}
\end{align*}
$$

**Excercise 3.13**

Then the target distribution is obtained from

$$
p(t|\mathbf{x}, \mathbf{t}) = \iint \mathcal{N} (t | \mathbf{w}^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}), \beta^{-1} )
\cdot
\mathcal{N}(\mathbf{w}|\mathbf{m}_N, \beta^{-1}\mathbf{S}_N)
\mathrm{Gam} (\beta | a_N, b_N) \,d\beta d\mathbf{w}
$$

First we integrate out $\mathbf{w}$, by making following change of variables and using the results in section 2.3.3 again

$$
\begin{align*}
A &\rightarrow \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} , &\mathbf{x} &\rightarrow \mathbf{w} , & b &\rightarrow \mathbf{0} \\
\mathbf{L}^{-1} &\rightarrow \beta^{-1}, & \boldsymbol{\mu} &\rightarrow \mathbf{m}_N, & \boldsymbol{\Lambda}^{-1} &\rightarrow \beta^{-1} \mathbf{S}_N
\end{align*}
$$

then we obtain

$$
p(t|\mathbf{x}, \mathbf{t}) = 
\mathcal{N} \left( t|\mathbf{m}_N^{\mathsf{T}} \boldsymbol{\phi}(\mathbf{x}), 
\beta^{-1} + \beta^{-1} \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}) \right)
\mathrm{Gam} (\beta | a_N, b_N)
$$

Using the technique in section 2.3.7 to integrate out $\beta$, we obtain a Student's t-distribution given by

$$
p(t|\mathbf{x}, \mathbf{t})  = \mathrm{St} (t|\mu, \lambda, \nu)
$$

in which

$$
\begin{align*}
\mu &= \mathbf{m}_N^{\mathsf{T}} \boldsymbol{\phi} (\mathbf{x}) \\
\lambda &= \frac{a_N}{b_N (1 + \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}))} \\
\nu &= 2 a_n \\
\end{align*}
$$

### 3.3.3. Equivalent kernel

Based on (3.58) and (3.53), the predictive mean is given by

$$
y(\mathbf{x}, \mathbf{m}_N) = \mathbf{m}_N \boldsymbol{\phi}(\mathbf{x}) 
= \beta \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\Phi}^{\mathsf{T}} \mathbf{t}
= \sum_{n=1}^N \beta \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}_n) t_n
$$

which takes the form of a linear combination of target variables $\{t\_n\}$. So we can write

$$
y(\mathbf{x}, \mathbf{m}_N) = \sum_{n=1}^N k(\mathbf{x}, \mathbf{x}_n) t_n
$$

where

$$
k(\mathbf{x}, \mathbf{x}^\prime) = \beta \boldsymbol{\phi}(\mathbf{x})^{\mathsf{T}} \mathbf{S}_N \boldsymbol{\phi}(\mathbf{x}^\prime)
$$

is known as the **smoother matrix** or the **equivalent kernel**. Regression functions that make predictions by linearly combining the target values are knowns as the **linear smoothers**.
