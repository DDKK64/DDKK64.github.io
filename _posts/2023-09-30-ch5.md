- [Neural networks](#neural-networks)
  - [Feed-forward network functions](#feed-forward-network-functions)
    - [weight-space symmetries](#weight-space-symmetries)
  - [Network training](#network-training)
    - [Parameter optimization](#parameter-optimization)
    - [Local quadratic approximation](#local-quadratic-approximation)
    - [Use of gradient information](#use-of-gradient-information)
    - [Gradient descent optimization](#gradient-descent-optimization)
  - [Error backpropagation](#error-backpropagation)
    - [Evaluation of derivatives](#evaluation-of-derivatives)
    - [A simple example](#a-simple-example)
    - [Efficiency of backpropagation](#efficiency-of-backpropagation)
    - [The Jacobian matrix](#the-jacobian-matrix)
  - [The Hessian matrix](#the-hessian-matrix)
    - [Diagonal approximation](#diagonal-approximation)
    - [Outer product approximation](#outer-product-approximation)
    - [Inverse Hessian](#inverse-hessian)
    - [Finite differences](#finite-differences)
    - [Exact evaluation of the Hessian](#exact-evaluation-of-the-hessian)
  - [Regularization in neural networks](#regularization-in-neural-networks)
    - [Consistent Gaussian priors](#consistent-gaussian-priors)
    - [Early stopping](#early-stopping)
    - [Invariances](#invariances)
    - [Tangent propagation](#tangent-propagation)

# Neural networks

## Feed-forward network functions

First we construct $M$ linear combinations of $D$ input variables

$$
a_j = \sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}^{(1)}
$$

where $j = 1, \dots, M$. The superscript $(1)$ indicates the layer of parameters in the network.
$w\_{ji}^{(1)}$ is called weights and $w\_{j0}^{(1)}$ is the bais.
$a\_j$ are called the **activations**. Using a nonlinear activation function $h(\cdot)$ we transform each $a\_j$ to give

$$
z_j = h(a_j)
$$

These are called **hidden units**.

Linearly combining $z\_j$ gives **output unit activations** given by

$$
a_k = \sum_{j=1}^M w_{kj}^{(2)} z_j + w_{k0}^{(2)}
$$

where $k=1,\dots,K$ and $K$ is the number of output variables.

Finnaly, the output unit activations are transformed into the network outputs $y\_k$, with an appropriate activation function.

The choice of activation function is determined by the nature of the data and the assumed distribution of target variables.
For regression problems, the identity function can be chosen so that $y\_k = a\_k$.
For binary classification problem, the logistic sigmoid can be used so that $y\_k = \sigma(a\_k)$. And for multi-class classification, we can use the softmax function.

If all activation functions for hidden units are taken to be linear, a equivalent network without hidden units can always be found.
Since superposition of multiple linear transformations is still a linear transformation.

If the number of hidden units is less than the number of input or output units, information may be lost.

There are different term for counting the layers of a network:

- By the number of layers of units
- By the number of layers of hidden units
- By the number of layers of weights

An optional **skip-layer** connects the inputs directly to the outputs.
Sigmoidal hidden units can mimic such layer by setting intput weights to very small value, so that the activation function present linearity.

### weight-space symmetries



## Network training

Starting by univariate regression problems, we assume that target $t$ follows a Gaussian distribution so that

$$
p(t|\mathbf{x}, \mathbf{w}, \beta) = \mathcal{N} (t | y(\mathbf{x}, \mathbf{w}), \beta^{-1})
$$

in which $y(\mathbf{x}, \mathbf{w})$ is the network output and $\beta$ is the precision of the noise.

Given a data set $\mathbf{X} = \{\mathbf{x}\_1, \dots, \mathbf{x}\_N \}$ and corresponding targets $\mathbf{t} = \{t\_1, \dots, t\_N\}$,
the likelihood function is given by

$$
p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta) = \prod_{n=1}^N p(t_n | \mathbf{x}_n, \mathbf{w}, \beta)
$$

Taking the negtive logrithm gives

$$
- \ln p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta) 
=  \frac{\beta}{2} \left[ y(\mathbf{x}_n, \mathbf{w}) - t_n \right]^2 - \frac{N}{2} \ln \beta
+ \frac{N}{2} \ln 2 \pi
$$

Then we see maximizing the likelihood function with respect to $\mathbf{w}$ is equivalent to minimizing
the sum-of-sqaures error function given by

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left[ y(\mathbf{x}_n, \mathbf{w}) -t_n \right]^2
$$

The value of $\beta$ is estimated by

$$
\frac{1}{\beta_{\mathrm{ML}}} = \frac{1}{N} \sum_{n=1}^N \left[ y(\mathbf{x}_n, \mathbf{w}_{\mathrm{ML}}) -t_n \right]^2
$$

\
In the case of multivariate target $\mathbf{t}$, with shared noise precision $\beta$, the distribution is given by

$$
p(\mathbf{t}|\mathbf{x}, \mathbf{w}) = \mathcal{N} (\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{w}), \beta^{-1}\mathbf{I})
$$

The corresponding error function is given by

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\| \mathbf{y}(\mathbf{x}_n, \mathbf{w})- \mathbf{t}_n \right\|^2
$$

And the estimator for $\beta$ gives

$$
\frac{1}{\beta_{\mathrm{ML}}} = \frac{1}{NK} \sum_{n=1}^N \left\| \mathbf{y}(\mathbf{x}_n, \mathbf{w})- \mathbf{t}_n \right\|^2
$$

If the covariance matrix is arbitrary, then the error function is defined as

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left[ \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n
 \right]^{\mathsf{T}} 
\boldsymbol{\Sigma}^{-1} \left[  \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n \right]
$$

And the estimator for $\boldsymbol{\Sigma}$ gives

$$
\boldsymbol{\Sigma}_{\mathrm{ML}} = \frac{1}{N} \sum_{n=1}^N \left[ \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n \right] \left[ \mathbf{y}(\mathbf{x}_n, \mathbf{w}) - \mathbf{t}_n \right]^{\mathsf{T}}
$$

Consider a binary classification problem, where the target variable $t = 1$ for class $C\_1$ and $t=0$ for $C\_2$.

We choose the logistic sigmoid function as the output activation function,
so that $0 \le y(\mathbf{x}, \mathbf{w}) \le 1$ and we interpret $y$ as $p(C\_1\|\mathbf{x})$.

The target distribution takes the form

$$
p(t|\mathbf{x}, \mathbf{w}) = y(\mathbf{x}, \mathbf{w})^t \left[ 1 - y(\mathbf{x}, \mathbf{w}) \right]^{1-t}
$$

By taking the negtive logarithm of the likelihood function, we obtain the cross-entropy error function

$$
E(\mathbf{w}) = - \sum_{n=1}^N \left[ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right]
$$

where $y\_n \equiv y(\mathbf{x}\_n, \mathbf{w})$.

We can take mislabelling targets into account by involving a $\epsilon$ to indicate the probability of mislabelling.
Using the total probability, the probability of $C\_1$ is given by

$$
\begin{align*}
p(t=1 | \mathbf{x}, \mathbf{w}, \epsilon)
&= (1-\epsilon) y(\mathbf{x}, \mathbf{w}) + \epsilon \left[ 1 - y(\mathbf{x}, \mathbf{w}) \right] \\
&= \epsilon + (1 - 2 \epsilon) y(\mathbf{x}, \mathbf{w})
\end{align*}
$$

Therefore the target distribution can be collectively expressed by

$$
p(t|\mathbf{x}, \mathbf{w}, \epsilon)
= \left[ \epsilon + (1 - 2 \epsilon) y(\mathbf{x}, \mathbf{w}) \right]^t
\left[ 1 - \epsilon + (2 \epsilon - 1) y(\mathbf{x}, \mathbf{w}) \right]^{1-t}
$$

Corresponding cross-entropy function gives

$$
\begin{align*}
E(\mathbf{w}) 
&= - \sum_{n=1}^N \left\{ t_n \ln \left[ \epsilon + (1 - 2 \epsilon) y(\mathbf{x}_n, \mathbf{w}) \right]
+ (1 - t_n) \ln \left[ 1 - \epsilon + (2 \epsilon - 1) y(\mathbf{x}_n, \mathbf{w}) \right] \right\}
\end{align*}
$$

Now consider a statndard multiclass classification problem with 1-of$K$ coding scheme. The output $y\_k$ is interpreted as $p(t\_k=1\|\mathbf{x})$ and takes the softmax function as the output activation function, so that

$$
y_k(\mathbf{x}, \mathbf{w}) = \frac{\exp (a_k(\mathbf{x}, \mathbf{w}))}{\sum_j \exp (a_j(\mathbf{x}, \mathbf{w}))}
$$

which satisfies $0 \le y\_k \le 1$.


The error function then gives

$$
E(\mathbf{w}) = - \sum_{n=1}^N \sum_{k=1}^K t_{nk} \ln y_k(\mathbf{x}_n, \mathbf{w})
$$

> Excersize 5.6: refer to Multiclass logistic regression

### Parameter optimization

Because $E(\mathbf{w})$ is a smooth continuous function of $\mathbf{w}$, the minimum should occur at some stationary point.
However, there will be many stationary points, each of which could be a local minima, saddle point or global minimum. 
In practice, it may not be necessary to find the global minimum, but it is necessary to check multiple local minima to get sufficiently good one.

### Local quadratic approximation

Consider the Tylor expansion of $E(\mathbf{w})$ at the point $\widehat{\mathbf{w}}$, which is given by

$$
E(\mathbf{w}) \simeq E(\widehat{\mathbf{w}}) + (\mathbf{w} - \widehat{\mathbf{w}}) \mathbf{b}^{\mathsf{T}} + (\mathbf{w} - \widehat{\mathbf{w}})^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \widehat{\mathbf{w}})
$$

where $\mathbf{b}$ is the gradient

$$
\mathbf{b} \equiv \nabla E |_{\mathbf{w} = \widehat{\mathbf{w}}} 
$$

and $\mathbf{H}$ is the Hessian matrix

$$
(\mathbf{H})_{ij} \equiv \frac{\partial E}{\partial w_i \partial w_j} \bigg|_{\mathbf{w} = \widehat{\mathbf{w}}}
$$

Taking derivative of both sides, we obtain an approximation to the gradient

$$
\nabla E \simeq \mathbf{b} + \mathbf{H} (\mathbf{w} - \widehat{\mathbf{w}})
$$

Suppose a $\mathbf{w}^\ast$ is a stationary point of $E(\mathbf{w})$, at which the gradient is zero. Then we have

$$
E(\mathbf{w}) \simeq E(\mathbf{w}^\ast) + (\mathbf{w} - \mathbf{w}^\ast)^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast)
$$

Let $\{\mathbf{u}\_i\}$ be the orthonomal eigenvectors of $\mathbf{H}$, such that

$$
\mathbf{H} \mathbf{u}_i = \lambda_i \mathbf{u}_i
$$

For any non-zero vector $\mathbf{v}$ in $\mathbb{R}^D$, we have

$$
\mathbf{v} = \sum_i \alpha_i \mathbf{u}_i = P \mathbf{a}
$$

where we defined

$$
\begin{align*}
P &= (\mathbf{u}_1, \dots, \mathbf{u}_D) \\
\mathbf{a} &= (\alpha_1, \dots, \alpha_D)^{\mathsf{T}} \\
\end{align*}
$$

Then we have

$$
\mathbf{v}^{\mathsf{T}} \mathbf{H} \mathbf{v}
= \mathbf{a}^{\mathsf{T}} P^{\mathsf{T}} \mathbf{H} P \mathbf{a}
= \mathbf{a}^{\mathsf{T}} \operatorname{diag} (\lambda_1, \dots, \lambda_D) \mathbf{a}
= \sum_i \lambda_i \alpha_i^2
$$

Since $\alpha\_i$ is defined by the arbitrary non-zero $\mathbf{v}$, we see that $\mathbf{H}$
is postive definite if and only if all $\lambda\_i$ is postive.

And when $\mathbf{H}$ is postive definite, in the neighbor of $\mathbf{w}^\ast$ we have

$$
E(\mathbf{w}) - E(\mathbf{w}^\ast) = (\mathbf{w} - \mathbf{w}^\ast)^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast) > 0
$$

which means $\mathbf{w}^\ast$ is a minima. For the univariate case, the stationary point will be a minimum if 

$$
\frac{\partial^2 E}{\partial w^2} > 0
$$

Note that, for constant error value, the contour forms a ellipse, which centered on $\mathbf{w}^\ast$ with the length of axes determined by eigenvalues and 

### Use of gradient information

### Gradient descent optimization

Batch methods: use the whole data set at once. E.g. gradient descent (or steepest descent).

Better batch methods: conjugate gradients, quasi-Newton methods

On-line methods: sequential gradient descent or stochastic gradient descent.

## Error backpropagation

An efficient technique to evaluate the garadient of error functions, error backpropagation (a.k.a backprop).

### Evaluation of derivatives

In a general feed-forward network, each unit is computed with

$$
a_j = \sum_i w_{ji} z_i \tag{5.48}
$$

where $z\_i$ can be input, or activation of a unit that sends a connection to unit $j$.

Then the sum is transformed into activation $z\_j$ using a nonlinear function $h(\cdot)$ so that $z\_j = h(a\_j)$.

This process is often called forward propagation.

Let $E\_n$ denote the error associated with pattern n.
Consider the derivative of $E\_n$ with respective to $w\_{ji}$, which can be written as

$$
\frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j} \frac{\partial a_j}{\partial w_{ji}}
$$

in which $w\_{ji}$ is a weight on input to unit $j$.

Define

$$
\delta_j \equiv \frac{\partial E_n}{\partial a_j}
$$

which is often called the error.

By

$$
\frac{\partial a_j}{\partial w_{ji}} = z_i
$$

we can write

$$
\frac{\partial E_n}{\partial w_{ji}} = \delta_j z_i \tag{5.47}
$$

For output units $k$, provided that the output-unit activation function is a canonical link, we have

$$
\delta_k = y_k - t_k
$$

For hidden units $j$, we can write

$$
\delta_j \equiv \frac{\partial E_n}{\partial a_j}
= \sum_k \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_j}
$$

where $a\_k$ are weighted sums of unit $k$ to which unit $j$ sends connection. Note that unit $k$ here could be another hidden unit or the output unit.

By

$$
\begin{align*}
\frac{\partial E_n}{\partial a_k} &= \delta_k \\
\frac{\partial a_k}{\partial a_j} &= w_{kj} h^\prime(a_j)
\end{align*}
$$

the **backpropagation** formula is then given by

$$
\delta_j = h^\prime (a_j) \sum_k w_{kj} \delta_k \tag{5.56}
$$

The backpropagation procedure is shown as follows:

1. Run forward propagation to find activations of all units.
2. Evaluate $\delta\_k$ for output units.
3. Backpropagate to evaluate $\delta\_j$
4. Calculate the derivatives.

Assuming that each unit have the same activation function, the derivative of total error can be summed up by

$$
\frac{\partial E}{\partial w_{ji}} = \sum_n \frac{\partial E_n}{w_{ji}}
$$

### A simple example

Consider a two-layer network.

The error function associated with pattern $n$ gives

$$
E_n = \frac{1}{2} \sum_{k=1}^K (y_k - t_k)^2
$$


The output units use the identity activation function and the activation function for hidden units is given by

$$
h(a) = \tanh (a) = \frac{e^{2a} - 1}{e^{2a} + 1}
$$

Note that, the derivative of $h$ has a property that

$$
h^\prime (a) = 1 - h(a)^2
$$

Then the forward propagation is shown by

$$
\begin{align*}
a_j &= \sum_{i=0}^D w_{ji}^{(1)} x_i \\
z_j &= \tanh(a_j) \\
y_k &= \sum_{j=0}^{M} w_{kj}^{(2)} z_j \\
\end{align*}
$$

For output units $k$

$$
\delta_k = y_k - t_k
$$

For hidden units $j$

$$
\delta_j = (1 - z_j^2) \sum_{k=1}^K w_{kj}^{(2)} \delta_k
$$

Finnaly, the derivatives with respect to the parameters is obtained by

$$
\frac{\partial E_n}{\partial w_{ji}^{(1)}} = \delta_j x_i, \quad
\frac{\partial E_n}{\partial w_{kj}^{(2)}} = \delta_k z_j
$$

### Efficiency of backpropagation

Suppose we have a total of $W$ weights and baises in the network.

The forward propagation process conducts a multiplication and an addition on each parameter, which requries $O(W)$ operations.
In a non-sparse network, the number of weights is much greater than the number of units so that the cost of activation function is relatively small.
Therefore, if $W$ is sufficiently large, then the number of required operations is $O(W)$.
A single evaluation for one pattern requries $O(W)$ oprations.

An alternative approach to evaluate the derivatives is the finite difference, which is shown by

$$
\frac{\partial E_n}{\partial w_{ji}} 
= \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji})}{\epsilon} + O(\epsilon)
$$

By making $\epsilon$ small enough, we get a good approximation of the derivatives.

A more accurate method is the symmetric **central differences** which takes the form

$$
\frac{\partial E_n}{\partial w_{ji}} 
= \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji} - \epsilon)}{2 \epsilon} + O(\epsilon^2)
$$

This method eliminates the first order error $O(\epsilon)$, which can be derived by taking difference of the Taylor expansions

$$
\begin{align*}
E_n(w_{ji} + \epsilon) &= E_n(w_{ji}) + \frac{\partial E_n}{\partial w_{ji}} \epsilon + O(\epsilon^2) \\
E_n(w_{ji} - \epsilon) &= E_n(w_{ji}) - \frac{\partial E_n}{\partial w_{ji}} \epsilon + O(\epsilon^2) \\
\end{align*}
$$

Evaluation of a single parameter $w\_{ji}$ requires $O(W)$ operations, and hence evaluation of the derivative costs $O(W^2)$ operations.

Although numerical differentiation has high complexity, it is useful to verify the correctness of implementations of other optimized methods.

### The Jacobian matrix

Consider the Jacobian matrix given by the derivatives of outputs with repsect to inputs such that

$$
J_{ki} = \frac{\partial y_k}{\partial x_i}
$$

It can be a measure of local sensitivity of outputs to changes in each input variables. This can be expressed as

$$
\Delta y_k \simeq \sum_i \frac{\partial y_k}{\partial x_i} \Delta x_i
$$

where $\|\Delta x\_i\|$ are sufficiently small.

The evaluation of the Jacobian matrix

$$
\begin{align*}
J_{ki} = \frac{\partial y_k}{\partial x_i}
= \sum_j \frac{\partial y_k}{\partial a_j} \frac{\partial a_j}{\partial x_i}
= \sum_j w_{ji} \frac{\partial y_k}{\partial a_j}
\end{align*}
$$

where units $j$ are those that the input unit $x\_i$ sends connections to.

To evaluate the $\partial y\_k / \partial a\_j$, we use backpropagation to give

$$
\frac{\partial y_k}{\partial a_j}
= \sum_l \frac{\partial y_k}{\partial a_l} \frac{\partial a_l}{\partial a_j}
= h^\prime (a_j) \sum_l w_{lj} \frac{\partial y_k}{\partial a_l}
$$

where units $l$ are those that the unit $j$ sends connections to.

Repeat the backpropagation procedure recursively until $y\_k$ are the output units. Then for sigmoidal output activation function we have

$$
\frac{\partial y_k}{\partial a_l} = I_{kl} \sigma^\prime (a_l)
$$

and for softmax activation function we have

$$
\frac{\partial y_k}{\partial a_l} = y_k (I_{kl} - y_l)
$$

To evaluate the Jacobian matrix, we first run forward propagation to calculate all the activations.
Then recursively we evaluate $J\_{ki}$ starting from ${\partial y\_k}/{\partial a\_l}$ until ${\partial a\_j}/{\partial x\_i}$.

An alternative forward propagation algorithm can also be used. Consider a two-layer network, the Jocobian can be written as

$$
\begin{align*}
J_{ki} &= \sum_{j, l} \frac{\partial y_k}{\partial a_l} \frac{\partial a_l}{\partial a_j} \frac{\partial a_j}{\partial x_i} \\
\end{align*}
$$

where units $j$ are in the hidden layer, and units $l$ are in the output layer.

In a forward manner, we first evaluate $\partial a\_j / \partial x\_i$, follwed by $\partial a\_l / \partial a\_j$, until the outpupt unit $\partial y\_k / \partial a\_l$.
This can be generalized to multi-layer network.

Note that, the Jacobian can also be evaluated using the central difference

$$
\frac{\partial y_k}{\partial x_i} = \frac{y_k(x_i+\epsilon) - y_k(x_i-\epsilon)}{2 \epsilon} + O(\epsilon^2)
$$

## The Hessian matrix



### Diagonal approximation

The inverse of a diagnal marix is trivial to solve. The diagonal approximation simply ignores off-diagonal elements of Hessian so that it becomes diagonal.

Consider the addtive error function such that $E = \sum\_n E\_n$ where $E\_n$ are errors associated with pattern $n$.

Following (5.48) and (5.47), the diagonal elements of the Hessian for pattern $n$ can be written

$$
\frac{\partial^2 E_n}{\partial w_{ji}^2} = \frac{\partial^2 E_n}{\partial a_j^2} z_i^2
$$

To evaluate ${\partial^2 E\_n}/{\partial a\_j^2}$, use the backpropagation formula (5.56) by taking the derivative with repect to $a\_j$ again

$$
\frac{\partial^2 E_n}{\partial a_{j}^2}
= h^{\prime\prime} (a_j) \sum_k w_{kj} \frac{\partial E_n}{\partial a_k}
+ h^\prime (a_j) \sum_k w_{kj} \frac{\partial}{\partial a_j} \left( \frac{\partial E_n}{\partial a_{k}} \right)
$$

where units $j$ sends connections to units $k$.

> The topology of network: $z\_i \to z\_j=h(a\_j) \to y\_k=h(a\_k) \to E\_n$

Note that

$$
\begin{align*}
\frac{\partial^2 E_n}{\partial a_{k} \partial a_j}
&= \sum_{k^\prime} \frac{\partial^2 E_n}{\partial a_{k} \partial a_{k^\prime}}
\frac{\partial a_{k^\prime}}{\partial a_j} \\
&= \sum_{k^\prime} \frac{\partial^2 E_n}{\partial a_{k} \partial a_{k^\prime}}
h^\prime(a_j) w_{k^\prime j}
\end{align*}
$$

in which $k$ and $k^\prime$ are units in the same layer.

Back-substituting, we have

$$
\frac{\partial^2 E_n}{\partial a_{j}^2}
= h^{\prime\prime} (a_j) \sum_k w_{kj} \frac{\partial E_n}{\partial a_k}
+ h^\prime (a_j)^2 \sum_k \sum_{k^\prime} w_{kj} w_{k^\prime j} \frac{\partial^2 E_n}{\partial a_k \partial a_{k^\prime}}
$$

By further neglecting the off-diagonal elements, we obtain

$$
\frac{\partial^2 E_n}{\partial a_{j}^2}
= h^{\prime\prime} (a_j) \sum_k w_{kj} \frac{\partial E_n}{\partial a_k}
+ h^\prime (a_j)^2 \sum_k w_{kj}^2 \frac{\partial^2 E_n}{\partial a_k^2}
\tag{5.81}
$$

Evaluation for a single diagonal element costs $O(W)$ operations. A full Hessian requries $O(W^2)$ operations.

In practice, the Hessian is typically non-diagonal. This approximation is mainly for computational convenience.

### Outer product approximation

Consider a error function of regression problems which takes the form

$$
E = \frac{1}{2} \sum_{n=1}^N ( y_n - t_n)^2
$$

The first order derivative gives

$$
\nabla E = \sum_{n} (y_{n} - t_{n}) \nabla y_{n}
$$

The second order derivative gives

$$
\mathbf{H} = \sum_{n} \nabla y_{n} \nabla y_{n}^{\mathsf{T}} + \sum_{n} (y_{n} - t_{n}) \nabla \nabla y_n
\tag{5.83}
$$

If the output of a trained network $y\_n$ is very close to $t\_n$, the second term will be small and be neglected.

More generally, consider the squared loss function given by

$$
E = \frac{1}{2} \iint [ y(\mathbf{x}, \mathbf{w}) - t]^2 p(\mathbf{x}, t) \,d\mathbf{x} dt
$$

Taking derivative with respect to $w\_r$ and $w\_s$ in order, we have

$$
\frac{\partial E^2}{\partial w_r \partial w_s} 
= \iint \frac{\partial y}{\partial w_r} \frac{\partial y}{\partial w_s} p(\mathbf{x}) \,d\mathbf{x}
+ \iint \frac{\partial^2 y}{\partial w_r \partial w_s} \left\{ y(\mathbf{x}, \mathbf{w}) - \operatorname{E}_t [t|\mathbf{x}] \right\} p(\mathbf{x}) \,d\mathbf{x}
$$

In section 1.5.5, we know the optimal function that minizes the sum-of-squares loss is conditional mean of the target.
In this case the second term will vanish and we obtain

$$
\frac{\partial E^2}{\partial w_r \partial w_s} = \operatorname{E}_\mathbf{x} \left[ \frac{\partial y}{\partial w_r} \frac{\partial y}{\partial w_s} \right]
$$

from which we see when $y(\mathbf{x}, \mathbf{w})$ is trained close enough to the conditional mean of the target,
and $N$ is sufficiently large we obtain the **Levenberg-Marquardt** approximation or **outer product** approximation 

$$
\mathbf{H} \simeq \sum_n \mathbf{b}_n \mathbf{b}_n^{\mathsf{T}}
\tag{5.84}
$$

where $\mathbf{b}\_n \equiv \nabla y\_n = \nabla a\_n$. 

Using backpropagation, evaluation of the first order derivatives costs $O(W)$. Together with the outer product, evaluation of the Hessian costs $(W^2)$.

Note that, this approximation is only valid when a network has been trained well enough.

For regression problems with multiple target variables, the error function can be written

$$
E = \frac{1}{2} \sum_{n=1}^N \| \mathbf{y}_n - \mathbf{t}_n \|^2 
= \frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K ( \mathbf{y}_{nk} - \mathbf{t}_{nk} )^2 
$$

The Hessian takes the form

$$
\mathbf{H} = \sum_{n,k} \nabla y_{nk} \nabla y_{nk}^{\mathsf{T}} + \sum_{n,k} (y_{nk} - t_{nk}) \nabla \nabla y_n
$$

Thereby the approximation is given by

$$
\mathbf{H} \simeq \sum_{n,k} \mathbf{b}_{nk} \mathbf{b}_{nk}^{\mathsf{T}}
$$

where $\mathbf{b}\_{nk} \equiv \nabla y\_{nk}$

For a binary classification problem, the cross-entropy error function is given

$$
E(\mathbf{w}) = - \sum_{n=1}^N \left[ t_n \ln y_n + (1-t_n) \ln (1-y_n) \right]
$$

With the logstic sigmoid as the output unit activation function, the first order derivative gives

$$
\nabla E = \sum_n (y_n - t_n) \nabla a_n
$$

The Hessian gives

$$
\mathbf{H} = \sum_n y_n (1-y_n) \nabla a_n (\nabla a_n)^{\mathsf{T}}
+ \sum_n (y_n - t_n) \nabla \nabla a_n
$$

The approximation takes the form

$$
\mathbf{H} \simeq \sum_n y_n (1-y_n) \mathbf{b}_n \mathbf{b}_n^{\mathsf{T}}
$$

where $\mathbf{b}\_n \equiv \nabla a\_n$

In the case of multiclass problem, we have a similar result given by

$$
\mathbf{H} \simeq \sum_{n,k} y_{nk} (1-y_{nk}) \mathbf{b}_{nk} \mathbf{b}_{nk}^{\mathsf{T}}
$$

### Inverse Hessian


### Finite differences

### Exact evaluation of the Hessian

## Regularization in neural networks

To avoid over-fitting, it is intuitive to introduce a regularization term. A quadratic regularizer gives

$$
\widetilde{E}(\mathbf{w}) = E(\mathbf{w}) + \frac{\lambda}{2} \mathbf{w}^{\mathsf{T}} \mathbf{w}
\tag{5.112}
$$

This regularizer is also called the weight decay.

### Consistent Gaussian priors

Consider a two layer network with linear output units. The hidden layer takes the form

$$
z_j = h \left( \sum_i w_{ji} x_i + w_{j0} \right)
$$

and the output layer gives

$$
y_k = \sum_j w_{kj} z_j + w_{k0}
$$

Suppose a linear transformation is performed on the input so that

$$
x_i \rightarrow \widetilde{x}_i  = a x_i + b
$$

with the the hidden units given by

$$
z_j = h \left( \sum_i a w_{ji} x_i + \sum_i b w_{ji} + w_{j0} \right)
$$

we can carry the following linear transformations on the parameters so that the output of network remains unchanged

$$
\begin{align*}
w_{ji} &\rightarrow \widetilde{w}_{ji} = \frac{1}{a} w_{ji} \\
w_{j0} &\rightarrow \widetilde{w}_{j0} = w_{j0} - \frac{b}{a} \sum_i w_{ji} 
\end{align*}
$$

If a linear transformation is performed on the output so that

$$
y_k \rightarrow \widetilde{y}_k = c y_k + d
$$

a linear transformation can be applied on the second layer parameters

$$
\begin{align*}
w_{kj} &\rightarrow \widetilde{w}_{kj} = c w_{kj} \\ 
w_{k0} &\rightarrow \widetilde{w}_{k0} = c w_{k_0} + d \\
\end{align*}
$$

If we train a network based on linearly transformed inputs or targets,
consistency requires that resulting networks should only differ by the linear transformtion of weights and biases. The quadratic regularizer in (5.112) does not satisfy such invariance.

A regularizer that is invariant under scaling of weights takes the form

$$
\frac{\lambda_1}{2}\sum_{w \in \mathcal{W}_1} w^2
+ \frac{\lambda_2}{2}\sum_{w \in \mathcal{W}_2} w^2
\tag{5.121}
$$

where $\mathcal{W}\_1$ is the set of weights in the first layer and $\mathcal{W}\_2$ are weights in the second layer, with biases excluded. The invariance can be achieved by transformations

$$
\begin{align*}
\lambda_1 &\rightarrow \widetilde{\lambda}_1 = a^{1/2} \lambda_1 \\
\lambda_2 &\rightarrow \widetilde{\lambda}_2 = c^{-1/2} \lambda_2 \\
\end{align*}
$$

The regularizer corresponds to the prior given by

$$
p(\mathbf{w} | \alpha_1, \alpha_2) \propto \exp \left( - \frac{\alpha_1}{2}\sum_{w \in \mathcal{W}_1} w^2 - \frac{\alpha_2}{2}\sum_{w \in \mathcal{W}_2} w^2 \right)
$$

which is impoper since there's no constrains on the biases ($\int dw\_{j0} = \infty$). 

Introducing a seperate prior on the biases can achive the properness, but will braek the shift invariance.

More general prior can be given by dividing weights into arbitrary groups so that

$$
p(\mathbf{w}) \propto \exp \left( - \frac{1}{2} \sum_k \alpha_k \|\mathbf{w}\|_k^2 \right)
$$

in which

$$
\| \mathbf{w} \|^2 = \sum_{w \in \mathcal{W}_k} w^2
$$

### Early stopping

An alternative way to control the model complexity is the early stopping.

For optimization algorithms that have non-increasing error over the iteration index, traning
can be stopped when the smallest error is obtained on the validation data set.

Consider a error function given by

$$
E = E(\mathbf{w}^\ast) + \frac{1}{2} (\mathbf{w} - \mathbf{w}^\ast)^{\mathsf{T}} \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast)
$$

where $\mathbf{w}^\ast$ is the minima, $\mathbf{H}$ is a postive definite constant Hessian matrix.

> Intuition for the choice of error function??

Takging derivative with respect to $\mathbf{w}$, we obtain

$$
\nabla E = \mathbf{H} (\mathbf{w} - \mathbf{w}^\ast)
$$

Suppose the intial vector $\mathbf{w}^{(0)}$ is the origin, and the optimation algorithm is the gradient descent given by

$$
\mathbf{w}^{(\tau)} = \mathbf{w}^{(\tau-1)} - \rho \nabla E
$$

Substituting $\nabla E$

$$
\begin{align*}
\mathbf{w}^{(\tau)} - \mathbf{w}^\ast 
&= (I - \rho \mathbf{H}) (\mathbf{w}^{(\tau-1)} - \mathbf{w}^\ast) \\
&= (I - \rho \mathbf{H})^{\tau} (\mathbf{w}^{(0)} - \mathbf{w}^\ast)\\
&= - (I - \rho \mathbf{H})^{\tau} \mathbf{w}^\ast \\
\end{align*}
$$

Suppose $\{ \mathbf{u}\_j \}$ are orthonormal eigenvectors of $\mathbf{H}$ with corresponding eigenvalues $\{ \eta\_j \}$.
We can write

$$
(I - \rho \mathbf{H})^{\tau} \mathbf{u}_j 
= (I - \rho \mathbf{H})^{\tau - 1} (1 - \rho \eta_j) \mathbf{u}_j 
= (1 - \rho \eta_j)^{\tau} \mathbf{u}_j 
$$

Therefore

$$
w_j^{(\tau)} = \left[ 1 - (1 - \rho \eta_j)^\tau \right] w_j^\ast
$$

where we defined $w\_j = \mathbf{u}\_j^{\mathsf{T}} \mathbf{w}$.

Provided $\|1 - \rho \eta\_j\| < 1$, we have

$$
\lim_{\tau \rightarrow \infty} w_j^{(\tau)} = w_j^\ast
$$

Suppose the training stops after $\tau$ iterations. When $\eta\_j \ll (\rho \tau)^{-1}$, $w\_j^{(\tau)}$ converges to the minima $w\_j^\ast$; when $\eta\_j \gg (\rho \tau)^{-1}$, $w\_j^{(\tau)}$, $w\_j^{(\tau)}$ is close to its initial value 0.

Thereby we see, the quantity $(\rho \tau)^{-1}$ has the effect similar to the regularization coefficient $\lambda$.

### Invariances

In many applications, the predictions should remain unchanged, or stay invariant,
under some transformations on the inputs.
E.g. handwritten digits recognition should be robust against small elastic deformation.

If there are sufficiently large number of ovservations that contains effects of various transformations, the model can learn the invariance from the data.
However, this is not practical for real-life data set.

There are four categories of approches to train an adaptive model with invariaces:

1. Augment the training set by adding new data transformed from the old data.
2. Use regularization to penalize variance of the output to variance of the input.
3. Build invariance into the pre-processing by extracting features that are invariant under specific transformations.
4. Build the invariance into the model iteself.

### Tangent propagation

Tangent propagation uses regularization to build invariance to transformations.

Consider a continuous transformation governed by $\xi$, which transforms D-dimensional vector $\mathbf{x}$ into another vector $\mathbf{s} (\mathbf{x}, \xi)$ such that $\mathbf{s} (\mathbf{x}, 0) = 0$
Then the tagent vector at $\mathbf{x}\_n$ is given by

$$
\boldsymbol{\tau}_n = \frac{\partial \mathbf{s} (\mathbf{x}_n, \xi)}{\partial \xi} \bigg|_{\xi = 0}
$$

Take the transformed vector as the network input, the derivative of $y\_k$ with repsect to $\xi$ gives

$$
\frac{\partial y_k }{\partial \xi} 
= \sum_{i=1}^D \frac{\partial y_k}{\partial x_i} \frac{\partial x_i}{\partial \xi}
= \sum_i J_{ki} \tau_{i}
$$

where $J\_{ki}$ is the element $(k, i)$ in Jacobian matrix.

Defining

$$
\Omega  = \frac{1}{2} \sum_{n, k} \left( \frac{\partial y_{nk}}{\partial \xi} \right) \bigg|_{\xi = 0}
= \frac{1}{2} \sum_{n,k} \left( \sum_{i=1}^D J_{nki} \tau_{ni}  \right)
$$

