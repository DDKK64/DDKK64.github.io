---
layout: posts
title: "PRML - Probability distributions - Part III"
---
- [The exponential family](#the-exponential-family)
  - [Maximum likelihood and sufficient statistics](#maximum-likelihood-and-sufficient-statistics)
  - [Conjugate priors](#conjugate-priors)
  - [Noninformative priors???](#noninformative-priors)
- [Nonparametric methods](#nonparametric-methods)
  - [Histogram methods](#histogram-methods)
  - [Kernel density estimator](#kernel-density-estimator)
  - [Nearest-neighbor methods](#nearest-neighbor-methods)

## The exponential family

The expoenent family of ditributions over $\mathbf{x}$ with parameter $\boldsymbol{\eta}$ takes the form

$$
p(\mathbf{x}|\boldsymbol{\eta}) = h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\}
$$

where $\mathbf{x}$ may be a scalar or vector, discrete or continuous. $\boldsymbol{\eta}$ is called the natural parameter. 
$g(\boldsymbol{\eta})$ ensures the distribution is nomalized so that

$$
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \,d\mathbf{x} = 1
$$

Consider the Bernoulli distribution, by writing in the form

$$
\begin{align*}
p(x|\mu) = \mathrm{Bern}(x|\mu) &= \mu^x (1-\mu)^{1-x} \\
&= (1-\mu) \exp \left\{ \ln \left( \frac{\mu}{1-\mu} \right) x \right\}
\end{align*}
$$

we have

$$
\eta = \ln \left( \frac{\mu}{1-\mu} \right)
$$

where we solve for $\mu$

$$
\mu = \sigma(\eta) = \frac{1}{1 + e^{-\eta}}
$$

where $\sigma$ is the logistic sigmoid function. By using $1- \sigma(\eta) = \sigma(- \eta)$, we have

$$
1 - \mu = \sigma(- \eta)
$$

By comparison we have

$$
\begin{align*}
g(\eta) &= \sigma(-\eta) \\
h(x) &= 1 \\
u(x) &= x \\
\end{align*}
$$

Consider a multinomial distribution of a single observation ($N=1$), which can be written as

$$
p(\mathbf{x}|\boldsymbol{\mu}) = \exp \left\{ \sum_{k=1}^K x_k \ln \mu_k \right\}
$$

where $\mathbf{x} = (x\_1, \dots, x\_K)^{\mathsf {T}}$ and $\boldsymbol{\mu} = (\mu\_1, \dots, \mu\_K)^{\mathsf {T}}$.

Defining $\boldsymbol{\eta} = (\eta\_1, \dots, \eta\_K)^{\mathsf {T}}$, we have $\eta\_k = \ln \mu\_k$.

Other correspondence is given by

$$
\begin{align*}
g(\boldsymbol{\eta}) &= 1 \\
h(\mathbf{x}) &= 1 \\
\mathbf{u} (\mathbf{x}) &= 1 \\
\end{align*}
$$

With constraint $\sum\_k \mu\_k = 1$, we can also express the $\mu\_K$ using the remaining $K-1$ parameters such that

$$
\mu_K = 1 - \sum_{k=1}^{K-1} \mu_k
$$

and the constraints is now given by

$$
0 \le \mu_k \le 1 , \quad \sum_{k=1}^{K-1} \mu_k \le 1
$$

Using similar transformation on $x\_K$, the distribution can be written in the form

$$
\begin{align*}
p(\mathbf{x}|\boldsymbol{\mu})
&= \exp \left\{ \sum_{k=1}^{K-1} x_k \ln \mu_k + \left( 1 - \sum_{k=1}^{K-1} x_k \right) \ln \left( 1- \sum_{k=1}^{K-1} \mu_k \right) \right\} \\
&= \exp \left\{ \sum_{k=1}^{K-1} x_k \ln \left( \frac{\mu_k}{1 - \sum_{j=1}^{K-1} \mu_j} \right) + \ln \left( 1- \sum_{k=1}^{K-1} \mu_k \right) \right\} \\
\end{align*}
$$

where we identify

$$
\eta_k = \ln \left( \frac{\mu_k}{1 - \sum_{j=1}^{K-1} \mu_j} \right)
$$

Remove logarithm and rearranging terms we have

$$
\exp(\eta_k) (1 - \sum_{j=1}^{K-1} \mu_j) = \mu_k
$$

Summing both sides over $k$

$$
\begin{align*}
\sum_{k=1}^{K-1} \exp(\eta_k) (1 - \sum_{j=1}^{K-1} \mu_j) &= \sum_{k=1}^{K-1} \mu_k \\
1- \sum_{k=1}^{K-1} \mu_k &= \frac{1}{1 + \sum_{k=1}^{K-1} \exp(\eta_k)}
\end{align*}
$$

Back-substituting

$$
\mu_k = \frac{\exp (\eta_k)}{1 + \sum_{j=1}^{K-1} \exp(\eta_j)}
$$

which is called the **softmax function** or the **normalized exponential**.

The multinomial distribution now takes the form

$$
p(\mathbf{x}|\boldsymbol{\mu})
= \left( 1 + \sum_{k=1}^{K-1} \exp (\eta_k) \right)^{-1} \exp \left( \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right)
$$

from which we identify

$$
\begin{align*}
\boldsymbol{\eta} &= (\eta_1, \dots, \eta_{K-1})^{\mathsf {T}} \\
g(\boldsymbol{\eta}) &= \left( 1 + \sum_{k=1}^{K-1} \exp (\eta_k) \right)^{-1} \\
\mathbf{u}(\mathbf{x}) &= (x_1, \dots, x_{K-1})^{\mathsf {T}} \\
h(\mathbf{x}) &= 1 \\
\end{align*}
$$

The constraint on $\mathbf{x}$ is given by

$$
x_k \in \{0, 1\}, \quad \sum_{k=1}^{K-1} x_k \le 1
$$

Now consider a univariate Gaussian distribution given by

$$
\begin{align*}
p(x|\mu, \sigma^2)
&= \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left\{ - \frac{1}{2 \sigma^2} (x - \mu)^2 \right\} \\
&= \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left\{ - \frac{1}{2 \sigma^2} x^2 + \frac{\mu}{\sigma^2} x - \frac{1}{2 \sigma^2} \mu^2 \right\} \\
\end{align*}
$$

We identify

$$
\begin{align*}
\boldsymbol{\eta} &= \left( \frac{\mu}{\sigma^2} , -\frac{1}{2 \sigma^2} \right)^{\mathsf {T}} \\
\mathbf{u} (x) &= (x, x^2)^{\mathsf {T}} \\
\end{align*}
$$

Solving for $\mu$ and $\sigma$ we have

$$
\begin{align*}
\sigma^2 &= - \frac{1}{2 \eta_2} \\
\mu &= - \frac{\eta_1}{2 \eta_2} \\
\end{align*}
$$

which is followed by

$$
\begin{align*}
h(x) &= (2 \pi)^{1/2} \\
g(\boldsymbol{\eta}) &= (-2 \eta_2)^{1/2} \exp \left( \frac{\eta_1^2}{4 \eta_2} \right) \\
\end{align*}
$$

Consider a D-dimensional multivariate Gaussian distribution written by

$$
p(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1})
= \frac{1}{(2 \pi)^{D/2}} |\boldsymbol{\Lambda}|^{1/2} \exp \left\{ - \frac{1}{2} \mathbf{x}^{\mathsf{T}} \boldsymbol{\Lambda} \mathbf{x} + \boldsymbol{\mu}^{\mathsf{T}} \boldsymbol{\Lambda} \mathbf{x} - \frac{1}{2} \boldsymbol{\mu}^{\mathsf{T}} \boldsymbol{\Lambda} \boldsymbol{\mu} \right\}
$$

Defining vectors $\boldsymbol{x}, \boldsymbol{\lambda}$ whose elements are given by

$$
\begin{align*}
\boldsymbol{x}_{(i-1)*D+j} &= x_i x_j \\
\boldsymbol{\lambda}_{(i-1)*D+j} &= \Lambda_{ij} \\
\end{align*}
$$

in which $i,j = 1, \dots, D$. By definition, 

Then we obtain following definition

$$
\begin{align*}
\mathbf{u} (\mathbf{x}) &= \begin{pmatrix}
\mathbf{x} \\
\boldsymbol{x} \\
\end{pmatrix} \\
\boldsymbol{\eta} &= \begin{pmatrix}
\boldsymbol{\eta}_1 \\ \boldsymbol{\eta}_2 \\
\end{pmatrix} 
= \begin{pmatrix}
\boldsymbol{\Lambda} \boldsymbol{\mu} \\
-\frac{1}{2} \boldsymbol{\lambda} \\
\end{pmatrix}^{\mathsf{T}} \\
h(\mathbf{x}) &= (2 \pi)^{- 1/2} \\
\end{align*}
$$

Define a simple function $f$ such that

$$
f(\boldsymbol{\eta}_2) = -\frac{1}{2} \boldsymbol{\Lambda}
$$

which simply packs elements of $\boldsymbol{\lambda}$ back into a matrix. Then we have

$$
\begin{align*}
\boldsymbol{\Lambda} &= -2 f(\boldsymbol{\eta}_2) \\
\boldsymbol{\mu} &= \boldsymbol{\Lambda}^{-1} \boldsymbol{\eta}_1 
= - \frac{1}{2} f(\boldsymbol{\eta}_2)^{-1} \boldsymbol{\eta}_1
\end{align*}
$$

Therefore

$$
\begin{align*}
g(\boldsymbol{\eta}) &= |\boldsymbol{\Lambda}|^{1/2} \exp \left\{ - \frac{1}{2} \boldsymbol{\mu}^{\mathsf{T}} \boldsymbol{\Lambda} \boldsymbol{\mu} \right\} \\
&= \left| -2 f(\boldsymbol{\eta}_2) \right|^{1/2} \exp \left\{ \frac{1}{4} \boldsymbol{\eta}_1^{\mathsf{T}} f(\boldsymbol{\eta}_2)^{-1} \boldsymbol{\eta}_1 \right\}
\end{align*}
$$

### Maximum likelihood and sufficient statistics

Taking gradient of following integral with respect to $\boldsymbol{\eta}$

$$
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \,d\mathbf{x} = 1
$$

we have

$$
\nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \,d\mathbf{x}
+ g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) \,d\mathbf{x} = 0
$$

Rearranging terms

$$
- \frac{1}{g(\boldsymbol{\eta})} \nabla g(\boldsymbol{\eta}) 
= g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) \,d\mathbf{x}
= \operatorname{E} [\mathbf{u}(\mathbf{x})]
$$

And we have

$$
- \nabla \ln g(\boldsymbol{\eta}) = \operatorname{E} [\mathbf{u}(\mathbf{x})] \tag{2.226}
$$

Take derivative of (2.226) with respect to $\boldsymbol{\eta}$

$$
\begin{align*}
- \nabla \nabla \ln g(\boldsymbol{\eta})
&= \nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x})^{\mathsf {T}} \,d\mathbf{x} \\
&\quad + g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathsf {T}} \,d\mathbf{x} \\
&= \nabla g(\boldsymbol{\eta}) \frac{1}{g(\boldsymbol{\eta})} \operatorname{E} [\mathbf{u}(\mathbf{x})^{\mathsf {T}}]
+ \operatorname{E}[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathsf {T}}] \\
&= \operatorname{E}[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathsf {T}}]
- \operatorname{E} [\mathbf{u}(\mathbf{x})] \operatorname{E} [\mathbf{u}(\mathbf{x})^{\mathsf {T}}] \\
&= \operatorname{cov} [\mathbf{u}(\mathbf{x})]
\end{align*}
$$

Thus the covariance is given by

$$
- \nabla \nabla \ln g(\boldsymbol{\eta}) = \operatorname{cov} [\mathbf{u}(\mathbf{x})]
$$

Therefore, given a normalized distribution of exponential family, the moment can be derived through simple differentiation.

Consider a set of observations $\mathbf{X} = \{\mathbf{x}\_1, \dots, \mathbf{x}\_N\}$. Then the likelihood fucntion is given by

$$
p(\mathbf{X}|\boldsymbol{\eta})
= \left( \prod_{n=1}^N h(\mathbf{x}_n) \right) g(\boldsymbol{\eta})^N \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) \right\}
$$

Setting derivative of $\ln p(\mathbf{X}\|\boldsymbol{\eta})$ with respect to $\boldsymbol{\eta}$ to zero, we obtain the maximum likelihood estimator

$$
- \nabla \ln g(\boldsymbol{\eta}_{\mathrm{ML}}) = \frac{1}{N} \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n)
$$

in which we say that $\sum\_n \mathbf{u}(\mathbf{x}\_n)$ is the sufficient statistic for the distribution since the estimator for parameters depends only on it.

Moreover, by the law of large number, we have $\lim\_{N \rightarrow \infty} - \nabla \ln g(\boldsymbol{\eta}\_{\mathrm{ML}}) = E[\mathbf{u} (\mathbf{x})]$.
Comparing with (2.226), we see $\boldsymbol{\eta}\_{\mathrm{ML}}$ will converge to the true value $\boldsymbol{\eta}$.

### Conjugate priors

For distributions of exponential family, the conjugate prior can be shosen as

$$
p(\boldsymbol{\eta}|\boldsymbol{\chi}, \nu) = f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^\nu \exp (\nu \boldsymbol{\eta}^{\mathsf {T}} \boldsymbol{\chi})
$$

where $f(\boldsymbol{\chi}, \nu)$ is the normalization coefficient.

> why put a $\nu$ in the exponent??

Indeed, conjugacy can be shown by the posterior

$$
p(\boldsymbol{\eta} | \mathbf{X}, \boldsymbol{\chi}, \nu)
\propto g(\boldsymbol{\eta})^{\nu+N} \exp \left\{ \boldsymbol{\eta}^{\mathsf {T}} \left( \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) + \nu \boldsymbol{\chi} \right) \right\}
$$

which takes the same form as the prior.

Generally, $\nu$ can be interpreted as the number of fictitious observations. And the sufficient statistic for these data is given by $\boldsymbol{\chi}$.

### Noninformative priors???

In many cases, we have little knowledge about the prior distribution of parameters. Therefore, we seek a noninformative prior which have little influence on the posterior.

*Let data speak for themselves.*

Consider a distribution $p(x\|\lambda)$ governed by the parameter $\lambda$.
Intuitively, we want a prior such that each possible value of $\lambda$ has the same probability. 

However, if the domain of $\lambda$ is unbounded, $p(\lambda)$ cannot be normalized since the integral over $\lambda$ diverges.
Such priors are called **improper**. In practice, however, improper priors can be used as long as the posterior is proper.

E.g. a uniform prior on normal distribution ????

Another problem arises from nonlinear transformation of variables. Suppose $p\_\lambda(\lambda)$ is constant and $\lambda = \eta^2$. 
Then the density of $\eta$ given by

$$
p_\eta(\eta) = p_\lambda(\lambda) \left| \frac{d\lambda}{d\eta} \right| 
= p_\lambda(\eta^2) 2 \eta
\propto \eta
$$

is not constant. Note that this problem does not arise when we use maximum likelihood, since the likelihood function is simply a function of $\lambda$.

Here two simple non-informative priors are introduced.

Consider a probability density which takes the form

$$
p(x|\mu) = f(x-\mu)
$$

The parameter $\mu$ is called a **location parameter**. 
Such densities exihibit **translation invariance** since if $x$ is shifted using $\widehat{x} = x + c$, then the distribution over new variables given by

$$
p(\widehat{x}|\widehat{\mu}) = f(\widehat{x} - \widehat{\mu})
$$

where $\widehat{\mu} = \mu - c$, is the same as the original. 

We want a prior to have translation invariance and be non-informative, the prior distribution satisfies

$$
p(\mu-x) = p(\mu)
$$

which implies $p(\mu) = \textrm{Constant}$.

As an example, the conjugate prior of the mean $\mu$ in Gaussian distribution gives $p(\mu\|\mu\_0, \sigma\_0^2) = \mathcal{N} (\mu\|\mu\_0, \sigma\_0^2)$. 
In the limit of $\sigma\_0^2 \rightarrow \infty$, we have $p(\mu\|\mu\_0, \sigma\_0^2) \rightarrow 0$, and from the posterior we can see, the contribution of the prior vanishes. This gives a noninformative prior of the mean.

Consider a density of the form

$$
p(x|\sigma) = \frac{1}{\sigma} f \left( \frac{x}{\sigma} \right)
$$

where $\sigma > 0$ and is called the scale parameter. 

Note that $p(x\|\sigma)$ is normalized as long as the $f$ is normalized. This can be shown by

$$
\int p(x|\sigma) \,dx = \frac{1}{\sigma} \int f(\frac{x}{\sigma}) \,dx
= \int f(\frac{x}{\sigma}) \,d(\frac{x}{\sigma}) = 1
$$

Such form of density exibits scale invariance. If $\widehat{x} = c x$, then we have

$$
p(\widehat{x}|\widehat{\sigma}) = \frac{1}{\widehat{\sigma}} f(\frac{x}{\widehat{\sigma}})
$$

where $\widehat{\sigma} = c \sigma$.

TODO

A example of scale parameter is the standard deviation of Gaussian distribution. The conjugate prior for the standard deviation gives $p(\lambda \| a\_0, b\_0) = \mathrm{Gam}(\lambda \| a\_0, b\_0)$. In the limit $a\_0, b\_0 \rightarrow 0$, we see that $p(\lambda\|a\_0, b\_0) \propto \frac{1}{\lambda}$, and from the posterior, we see that the estimator depends only on the data instead of the prior.

## Nonparametric methods

The distributions discussed previously all have some functional form that is governed by a number of parameters.
This is the parametric approach. However, a problem for this approach is that the choice of the density might be poor.

With nonparametric approach, we make few assumptions about the form of the distribution.

### Histogram methods

Consider a random variable x whose density we wish to estimate.

We first partition the space of $x$ into bins of width $\Delta\_i$, and then count the number $n\_i$ of observations that fall into bin $i$. 
Given the total number of observations $N$, the probability that $x$ falls into bin $i$ is given by

$$
P_i = \frac{n_i}{N}
$$

And the corresponding density is given by

$$
p_i = \frac{n_i}{N \Delta_i}
$$

It is easy to see $\int p(x) \,dx = 1$. The density $p(x)$ is constant within each bin.

Often the width of bins are chosen to be the same, so that $\Delta\_i = \Delta$.

![](/images/2.24.png)

The figure above shows the density estimation based on 50 data points.
When $\Delta$ is small, the histogram tend to be spiky, the model of the distribution is obscure. 
When $\Delta$ is large, the histogram becomes smooth, and the multimodal property of the distribution is not captured.
Some intermidiate value for $\Delta$ results the best.

Histogram method is applicable for sequential data or large data set and is useful for data-virtualization in 1D or 2D space.

One downside of histogram is that the density is discontinous around the eadgs of bins.

Another problem is that if suffers from the curse of dimensionality. A D-deimensional space with each variable devided into M bins has a total of $M^D$ bins.

### Kernel density estimator

Suppose observations are drawn from an unknwon distribution $p(\mathbf{x})$ in a D-dimensional space.

The probability mass within a region $\mathcal{R}$ is given by

$$
P = \int_{\mathcal{R}} p(\mathbf{x}) \,d\mathbf{x}
$$

Then the probability that K points out of a total of N lies inside $\mathcal{R}$ is according to the binomial distribution

$$
\mathrm{Bin}(K|N, P) = \binom{N}{K} P^K (1-P)^{N-K}
$$

by which we see the fraction of points falling in the region has following properties

$$
\begin{align*}
\operatorname{E} [\frac{K}{N}] &= P \\
\operatorname{var} [\frac{K}{N}] &= \frac{P(1-P)}{N} \\
\end{align*}
$$

As $N$ increase, the variance will decrease and $\frac{K}{N}$ converges to $P$.

If the region $\mathcal{R}$ is sufficiently small, the density within the region is approxmately constant and satisfies

$$
P \approx p(\mathbf{x}) V
$$

where V is the volume of $\mathcal{R}$. 

Then the density can be estimated with

$$
p(\mathbf{x}) = \frac{K}{NV}
$$

However, small region is less likely to contain sufficient observations, which consequently requires more data to lower the variance.

We can exploit this estimator in two ways. 
Either by fixing K and determine the value of V, which gives rise to K-nearest-neigbor or
by fixing V and determine the value of K, which gives rise to the kernel approach.
It can be shown that both estimators converge to the true density as $N \rightarrow \infty$ and $V$ shrink suitably.

We begin with the kernel approach.

To count the number of points lying inside a region, we define a function

$$
k(\mathbf{u}) = \begin{cases}
1, & |u_i| \le 1/2, \quad i= 1, \dots, D \\
0, & \text{otherwise}
\end{cases}
$$

which represent a unit cube centered on the origin. $k(\mathbf{u})$ is called a **Parzen** window, which is an example of a kernel function.

The quantity $k((\mathbf{x} - \mathbf{x}\_n)/h)$ will be $1$ if $\mathbf{x}\_n$ lies inside the cube of side $h$ centered on $\mathbf{x}$, 
or symmetrically if $\mathbf{x}$ lies inside the cube centered on $\mathbf{x}\_n$.

The total number of data points lying inside the cube is

$$
K = \sum_{n=1}^N k \left( \frac{\mathbf{x} - \mathbf{x}_n}{h} \right)
$$

The density at $\mathbf{x}$ is then estimated with

$$
p(\mathbf{x}) = \frac{K}{NV} = \frac{1}{N h^D} \sum_{n=1}^N k \left( \frac{\mathbf{x} - \mathbf{x}_n}{h} \right)
$$

where $V = h^D$ is the volume of the hypercube.

However, this kernel estimator also suffers from discontinuity at the boundaries of cubes.
A solution is to choose a smooth kernel function, e.g. the Gassian kernel function.
Estimator using Gaussian kernel takes the form

$$
p(\mathbf{x}) = \frac{1}{N} \sum_{n=1}^N \frac{1}{(2 \pi h^2)^{1/2}} \exp \left\{ - \frac{\left\| \mathbf{x} - \mathbf{x}_n \right\|}{2 h^2} \right\}
$$

where $h$ is the smoothing parameter. With larger $h$, the model becomes smoother; with small $h$, the model becomes spiky.

Note that, the Gaussian estimator takes contributions from all data points into consideration, with further points weighing less. And deviding by $N$ ensures that the density is normalized.

Furthermore, We can choose any kernel function $k(\mathbf{u})$ subject to

$$
\begin{align*}
k(\mathbf{u}) &\ge 0 \\
\int k(\mathbf{u}) \,d\mathbf{u} &= 1 \\
\end{align*}
$$

Kernel methods do not require a seperate training phase but require the proper storage of the data set. 

### Nearest-neighbor methods

One difficulty of kernel methods is that $h$ is fixed. So in regions of high data density, $h$ is relatively large and may lead to local over-smoothing. 
In regions of low data density, h is relatively small and may lead to noisy estimation.

So $h$ may be good to be chosen dependent on the location within data space.

Consider a sphere centered on $\mathbf{x}$ where we want to estimate the density.
By setting $V$ to the volume of the smallest sphere that contains exactly K data points, $p(\mathbf{x})$ is estimated with $\frac{K}{NV}$.
This technique is called K nearest neighbours.

Small $K$ results in noisy estimation.

Note that, the density produced using KNN is not normalized, since the integral over the whole space diverges.

> Intuition is that if $\mathbf{x}$ is far enough from where the data points are located,
> the volume is approximately that of the sphere defined by $\mathbf{x}$ and any data point, which will simplify the problem.
> And presumably this integral will diverge.

Let $V(\mathbf{x})$ be the volume of the smallest sphere containing exactly K points. Then the density estimator takes the form

$$
p(\mathbf{x}) = \frac{K}{N V(\mathbf{x})}
$$

in which $K$ and $N$ are constant.

Now let $S$ be the smallest sphere centered on the origin which contains all $N$ data points.

For any $\mathbf{x}$ outside $S$, a sphere centered on $\mathbf{x}$ with radius $2 \left\\| \mathbf{x} \right\\|$ must also contains all $N$ points. 
If we define $V\_0(\mathbf{x})$ as the volume of such sphere, then we have $V\_0(\mathbf{x}) \ge V(\mathbf{x})$ and $V\_0(\mathbf{x}) \propto \left\\| x \right\\| ^D$

Now we show that the integral $\displaystyle \int\_{\mathbb{R}^D - S} \frac{1}{\left\\| x \right\\| ^D} \,d \mathbf{x}$
diverges. Because if it does, then the integral $\displaystyle \int\_{\mathbb{R}^D - S} \frac{1}{V(\mathbf{x})} \,d \mathbf{x}$
also diverges. Then by the definition of improper integral, we can say that $\displaystyle \int\_{\mathbb{R}^D} \frac{1}{V(\mathbf{x})} \,d \mathbf{x}$
diverges.

Under the [hyperspherical coordinates](https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates), the integral takes the form

$$
\begin{align*}
\int_{\mathbb{R}^D - S} \frac{1}{\left\| x \right\| ^D} \,d \mathbf{x}
&= \int_{\mathbb{R}^D - S} \frac{1}{r^D} r^{D-1}\sin ^{D-2}(\theta _{1})\sin ^{D-3}(\theta _{2})\cdots \sin(\theta _{D-2})\,dr\,d\theta _{1}\,d\theta _{2}\cdots d\theta _{D-1} \\
&= \int_{0}^{2\pi} \sin ^{D-2}(\theta _{1}) \,d\theta_1 
\cdots \int_{0}^{2\pi} \sin(\theta _{D-2}) \,d\theta _{2}
\int_{0}^{2\pi} d\theta _{D-1} 
\int_{R_S}^{\infty} \frac{1}{r} \,dr
\end{align*}
$$

where $R\_S$ is the radius of $S$. We can easily see that this integral diverges since
$\displaystyle  \int\_{R\_S}^{\infty} \frac{1}{r} \,dr$ diverges.

\
A more common problem on which we use KNN is classification.

Consider a data set of size N, with $N\_k$ points in class $C\_k$. 
Suppose we want to classify $\mathbf{x}$, and draw a sphere centered on $\mathbf{x}$ which contains exactly $K$ points regardless of their class.
Let $V$ be the volume of this sphere, and it contains $K\_k$ points of class $C\_k$.

Now the density associated with each class is estimated by

$$
p(\mathbf{x}|C_k) = \frac{K_k}{N_k V}
$$

The unconditional density is given by

$$
p(\mathbf{x}) = \frac{K}{NV}
$$

The class priors are given by

$$
p(C_k) = \frac{N_k}{N}
$$

Using Bayes' theorem, we have the posterior

$$
p(C_k | \mathbf{x}) = \frac{K_k}{K}
$$

Then $\mathbf{x}$ is assigned to the class that has the largest $K\_k/K$. Ties are broken at random.

Small $K$ results in more small regions.

When $K=1$, the method is called the nearest-neighbor. It assigns a point to the same class as the nearest data point.

It is proved that when $N \rightarrow \infty$, the nearest-neighbour ($K=1$) classifier has an error rate no more than twice the error rate of a classfifier that uses the true class distribution.

As with the kernel methods, KNN requires storage of the data set. On large data set, spatial index can be used to lower the cost of search neighbors.
